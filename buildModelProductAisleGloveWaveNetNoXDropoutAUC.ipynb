{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\chapanda\\\\OneDrive - Epsilon\\\\cp\\\\ACG\\\\03_Practise\\\\RnD\\\\crosssell'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import csv\n",
    "from collections import deque\n",
    "from glob import glob\n",
    "import bcolz\n",
    "from sklearn.metrics import log_loss,auc, roc_curve\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0) #,gpu_options=gpu_options\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\'\n",
    "#path_out = u'/userdata/projects/instakart/np/'\n",
    "N = 100\n",
    "debug = False\n",
    "batch_size = 128\n",
    "num_training_steps = 1000000\n",
    "learning_rate = 0.0003\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 500000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 50000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0.2\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "filter_widths=filter_widths+filter_widths\n",
    "skip_channels=64\n",
    "residual_channels=128\n",
    "\n",
    "prediction_dir = path_out + \"predictions\"\n",
    "checkpoint_dir = path_out + \"checkpoints\"\n",
    "\n",
    "if not os.path.isdir(prediction_dir):\n",
    "    os.makedirs(prediction_dir)\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "def tic():\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "def toc():\n",
    "    global start_time\n",
    "    elapsed_time = (time.time() - start_time)\n",
    "    print(\"took me \" + str(round(elapsed_time, 3))+\" seconds to do this..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(tensor, dim=None):\n",
    "    \"\"\"Get tensor shape/dimension as list/int\"\"\"\n",
    "    if dim is None:\n",
    "        return tensor.shape.as_list()\n",
    "    else:\n",
    "        return tensor.shape.as_list()[dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(object):\n",
    "\n",
    "    def __init__(self, path_out, suffix):\n",
    "        \n",
    "        self.filenames = [\n",
    "            'user_id',\n",
    "            'master_aisle_id',\n",
    "            'master_department_id',\n",
    "            'IsInOrder_history',\n",
    "            'NextInOrder_history',\n",
    "            'NumProductsFromDep_history',\n",
    "            'OrderSize_history',\n",
    "            'IndexInOrder_history',\n",
    "            'order_number_history',\n",
    "            'order_hour_of_day_history',\n",
    "            'order_dow_history',\n",
    "            'days_since_prior_order_history',\n",
    "            'history_length',\n",
    "            'ProductID1_history_a',\n",
    "            'ProductID2_history_a',\n",
    "            'ProductID3_history_a',\n",
    "            'ProductID4_history_a',\n",
    "            'ProductID5_history_a',\n",
    "            'ProductID6_history_a',\n",
    "            'ProductID7_history_a',\n",
    "            'ProductID8_history_a',\n",
    "            'ProductID9_history_a',\n",
    "            'ProductID10_history_a',\n",
    "            'ProductID11_history_a',\n",
    "            'ProductID12_history_a',\n",
    "            'ProductID13_history_a',\n",
    "            'ProductID14_history_a',\n",
    "            'ProductID15_history_a',\n",
    "            'ProductID16_history_a',\n",
    "            'ProductID17_history_a',\n",
    "            'ProductID18_history_a',   \n",
    "            'ProductID19_history_a',\n",
    "            'ProductID20_history_a',\n",
    "            'ProductID1_history',\n",
    "            'ProductID2_history',\n",
    "            'ProductID3_history',\n",
    "            'ProductID4_history',\n",
    "            'ProductID5_history',\n",
    "            'ProductID6_history',\n",
    "            'ProductID7_history',\n",
    "            'ProductID8_history',\n",
    "            'ProductID9_history',\n",
    "            'ProductID10_history',\n",
    "            'ProductID11_history',\n",
    "            'ProductID12_history',\n",
    "            'ProductID13_history',\n",
    "            'ProductID14_history',\n",
    "            'ProductID15_history',\n",
    "            'ProductID16_history',\n",
    "            'ProductID17_history',\n",
    "            'ProductID18_history',   \n",
    "            'ProductID19_history',\n",
    "            'ProductID20_history',\n",
    "            'ProductNameEmbedding_history',\n",
    "            'label'\n",
    "        ]\n",
    "        \n",
    "        self.ids = 0\n",
    "        self.start = 0\n",
    "        self.dataFiles = glob(path_out+'user_id*.dat') \n",
    "        self.num_chunks = len(self.dataFiles)\n",
    "        self.chunks_index = 0\n",
    "        \n",
    "        self.start = 0 \n",
    "        self.arrays = [(bcolz.open(self.dataFiles[0].replace('user_id',i))[:]) for i in self.filenames]\n",
    "        self.ids = np.arange(0,len(self.arrays[0]))\n",
    "\n",
    "    def next_batch(self, batch_size, shuffle=True, is_test=False):\n",
    "        while(1):\n",
    "            if (self.arrays[0].shape[0] - self.start) >= batch_size:\n",
    "                batch_ids = self.ids[self.start: (self.start + batch_size)]\n",
    "                self.start += batch_size\n",
    "                yield {globals()[placeholder] : copy.copy(array[batch_ids]) for placeholder, array in zip(self.filenames,self.arrays)} \n",
    "            else:\n",
    "                self.start = 0 \n",
    "                self.chunks_index += 1\n",
    "                if self.chunks_index >= self.num_chunks:\n",
    "                    self.chunks_index = 0\n",
    "                self.arrays = [(bcolz.open(self.dataFiles[self.chunks_index].replace('user_id',i))[:]) for i in self.filenames]\n",
    "                self.ids = np.arange(0,len(self.arrays[0]))\n",
    "                if shuffle:\n",
    "                    random.shuffle(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset(path_out,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_log_loss(y, y_hat, sequence_lengths, max_sequence_length, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Calculates average log loss on variable length sequences.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor of shape [batch size, max_sequence_length, input units].\n",
    "        y_hat: Prediction tensor, same shape as y.\n",
    "        sequence_lengths: Sequence lengths.  Tensor of shape [batch_size].\n",
    "        max_sequence_length: maximum length of padded sequence tensor.\n",
    "\n",
    "    Returns:\n",
    "        Log loss. 0-dimensional tensor.\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.minimum(tf.maximum(y_hat, eps), 1.0 - eps)\n",
    "    log_losses = y*tf.log(y_hat) + (1.0 - y)*tf.log(1.0 - y_hat)\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n",
    "    avg_log_loss = -tf.reduce_sum(log_losses*sequence_mask) / tf.cast(tf.reduce_sum(sequence_lengths), tf.float32)\n",
    "    return avg_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_convolution_layer(inputs, output_units, convolution_width, causal=False, dilation_rate=[1], bias=True,\n",
    "                               activation=None, dropout=None, scope='temporal-convolution-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Convolution over the temporal axis of sequence data.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, input_units].\n",
    "        output_units: Output channels for convolution.\n",
    "        convolution_width: Number of timesteps to use in convolution.\n",
    "        causal: Output at timestep t is a function of inputs at or before timestep t.\n",
    "        dilation_rate:  Dilation rate along temporal axis.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        if causal:\n",
    "            shift = int((convolution_width / 2) + (int(dilation_rate[0] - 1) / 2)) #FIXED THIS\n",
    "            pad = tf.zeros([tf.shape(inputs)[0], shift, inputs.shape.as_list()[2]])\n",
    "            inputs = tf.concat([pad, inputs], axis=1)\n",
    "\n",
    "        W = tf.get_variable(\n",
    "            name='weights',\n",
    "            initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            shape=[convolution_width, shape(inputs, 2), output_units]\n",
    "        )\n",
    "\n",
    "        z = tf.nn.convolution(inputs, W, padding='SAME', dilation_rate=dilation_rate)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                name='biases',\n",
    "                initializer=tf.constant_initializer(),\n",
    "                shape=[output_units]\n",
    "            )\n",
    "            z = z + b\n",
    "        z = activation(z) if activation else z\n",
    "        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "        z = z[:, :-shift, :] if causal else z\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavenet(x, dilations, filter_widths, skip_channels, residual_channels, scope='wavenet', reuse=False):\n",
    "    \"\"\"\n",
    "    A stack of causal dilated convolutions with paramaterized residual and skip connections as described\n",
    "    in the WaveNet paper (with some minor differences).\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape [batch size, max sequence length, input units].\n",
    "        dilations: List of dilations for each layer.  len(dilations) is the number of layers\n",
    "        filter_widths: List of filter widths.  Same length as dilations.\n",
    "        skip_channels: Number of channels to use for skip connections.\n",
    "        residual_channels: Number of channels to use for residual connections.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, len(dilations)*skip_channels].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        # wavenet uses 2x1 conv here\n",
    "        inputs = time_distributed_dense_layer(x, residual_channels, activation=tf.nn.tanh, scope='x-proj')\n",
    "\n",
    "        skip_outputs = []\n",
    "        for i, (dilation, filter_width) in enumerate(zip(dilations, filter_widths)):\n",
    "            dilated_conv = temporal_convolution_layer(\n",
    "                inputs=inputs,\n",
    "                output_units=2*residual_channels,\n",
    "                convolution_width=filter_width,\n",
    "                causal=True,# CHECK THIS OUT\n",
    "                dilation_rate=[dilation],\n",
    "                scope='cnn-{}'.format(i) \n",
    "            )\n",
    "            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "            output_units = skip_channels + residual_channels\n",
    "            outputs = time_distributed_dense_layer(dilated_conv, output_units, scope='cnn-{}-proj'.format(i))\n",
    "            skips, residuals = tf.split(outputs, [skip_channels, residual_channels], axis=2)\n",
    "\n",
    "            inputs += residuals\n",
    "            skip_outputs.append(skips)\n",
    "\n",
    "        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "        return skip_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_layer(inputs, lengths, state_size, keep_prob=1.0, scope='lstm-layer', reuse=False, return_final_state=False):\n",
    "    \"\"\"\n",
    "    LSTM layer.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, ...].\n",
    "        lengths: Tensor of shape [batch size].\n",
    "        state_size: LSTM state size.\n",
    "        keep_prob: 1 - p, where p is the dropout probability.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, state_size] containing the lstm\n",
    "        outputs at each timestep.\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        cell_fw = tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.contrib.rnn.LSTMCell(\n",
    "                state_size,\n",
    "                reuse=reuse\n",
    "            ),\n",
    "            output_keep_prob=keep_prob\n",
    "        )\n",
    "        outputs, output_state = tf.nn.dynamic_rnn(\n",
    "            inputs=inputs,\n",
    "            cell=cell_fw,\n",
    "            sequence_length=lengths,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        if return_final_state:\n",
    "            return outputs, output_state\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_distributed_dense_layer(inputs, output_units, bias=True, activation=None, batch_norm=None,\n",
    "                                 dropout=None, scope='time-distributed-dense-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Applies a shared dense layer to each timestep of a tensor of shape [batch_size, max_seq_len, input_units]\n",
    "    to produce a tensor of shape [batch_size, max_seq_len, output_units].\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, ...].\n",
    "        output_units: Number of output units.\n",
    "        activation: activation function.\n",
    "        dropout: dropout keep prob.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            name='weights',\n",
    "            initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            shape=[shape(inputs, -1), output_units]\n",
    "        )\n",
    "        z = tf.einsum('ijk,kl->ijl', inputs, W)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                name='biases',\n",
    "                initializer=tf.constant_initializer(),\n",
    "                shape=[output_units]\n",
    "            )\n",
    "            z = z + b\n",
    "\n",
    "        if batch_norm is not None:\n",
    "            z = tf.layers.batch_normalization(z, training=batch_norm, reuse=reuse)\n",
    "\n",
    "        z = activation(z) if activation else z\n",
    "        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAverageAUC(validation_predictions,validation_aisle,validation_ground_truths):\n",
    "    \n",
    "    aisles = np.unique(validation_aisle[:,0])\n",
    "    cum_auc = 0.0\n",
    "    cnt = 0\n",
    "\n",
    "    for n in range(len(aisles)):\n",
    "\n",
    "        predics_aisle = validation_predictions[np.where(validation_aisle==aisles[n])]\n",
    "        actual_aisle = validation_ground_truths[np.where(validation_aisle==aisles[n])]\n",
    "        \n",
    "        #print(actual_aisle)\n",
    "        #print(predics_aisle)\n",
    "        fpr, tpr, thresholds = roc_curve(actual_aisle, predics_aisle)\n",
    "        aisle_auc = auc(fpr, tpr)\n",
    "        if np.isnan(aisle_auc)==False:\n",
    "            cum_auc += aisle_auc\n",
    "            cnt += 1\n",
    "        #print(aisle_auc)\n",
    "    return (cum_auc/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_log_loss(y, y_hat, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Calculates log loss between two tensors.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor.\n",
    "        y_hat: Prediction tensor\n",
    "\n",
    "    Returns:\n",
    "        Log loss. 0-dimensional tensor.\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.minimum(tf.maximum(y_hat, eps), 1.0 - eps)\n",
    "    log_loss = -tf.reduce_mean(y*tf.log(y_hat) + (1.0 - y)*tf.log(1.0 - y_hat))\n",
    "    return log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable parameters:\n",
      "[('aisle_embeddings:0', [135, 50]), ('dept_embeddings:0', [22, 10]), ('user_embeddings:0', [207000, 3]), ('product_embeddings:0', [50000, 50]), ('wavenet/x-proj/weights:0', [2457, 128]), ('wavenet/x-proj/biases:0', [128]), ('wavenet/cnn-0/weights:0', [2, 128, 256]), ('wavenet/cnn-0/biases:0', [256]), ('wavenet/cnn-0-proj/weights:0', [128, 192]), ('wavenet/cnn-0-proj/biases:0', [192]), ('wavenet/cnn-1/weights:0', [2, 128, 256]), ('wavenet/cnn-1/biases:0', [256]), ('wavenet/cnn-1-proj/weights:0', [128, 192]), ('wavenet/cnn-1-proj/biases:0', [192]), ('wavenet/cnn-2/weights:0', [2, 128, 256]), ('wavenet/cnn-2/biases:0', [256]), ('wavenet/cnn-2-proj/weights:0', [128, 192]), ('wavenet/cnn-2-proj/biases:0', [192]), ('wavenet/cnn-3/weights:0', [2, 128, 256]), ('wavenet/cnn-3/biases:0', [256]), ('wavenet/cnn-3-proj/weights:0', [128, 192]), ('wavenet/cnn-3-proj/biases:0', [192]), ('wavenet/cnn-4/weights:0', [2, 128, 256]), ('wavenet/cnn-4/biases:0', [256]), ('wavenet/cnn-4-proj/weights:0', [128, 192]), ('wavenet/cnn-4-proj/biases:0', [192]), ('wavenet/cnn-5/weights:0', [2, 128, 256]), ('wavenet/cnn-5/biases:0', [256]), ('wavenet/cnn-5-proj/weights:0', [128, 192]), ('wavenet/cnn-5-proj/biases:0', [192]), ('wavenet/cnn-6/weights:0', [2, 128, 256]), ('wavenet/cnn-6/biases:0', [256]), ('wavenet/cnn-6-proj/weights:0', [128, 192]), ('wavenet/cnn-6-proj/biases:0', [192]), ('wavenet/cnn-7/weights:0', [2, 128, 256]), ('wavenet/cnn-7/biases:0', [256]), ('wavenet/cnn-7-proj/weights:0', [128, 192]), ('wavenet/cnn-7-proj/biases:0', [192]), ('wavenet/cnn-8/weights:0', [2, 128, 256]), ('wavenet/cnn-8/biases:0', [256]), ('wavenet/cnn-8-proj/weights:0', [128, 192]), ('wavenet/cnn-8-proj/biases:0', [192]), ('wavenet/cnn-9/weights:0', [2, 128, 256]), ('wavenet/cnn-9/biases:0', [256]), ('wavenet/cnn-9-proj/weights:0', [128, 192]), ('wavenet/cnn-9-proj/biases:0', [192]), ('wavenet/cnn-10/weights:0', [2, 128, 256]), ('wavenet/cnn-10/biases:0', [256]), ('wavenet/cnn-10-proj/weights:0', [128, 192]), ('wavenet/cnn-10-proj/biases:0', [192]), ('wavenet/cnn-11/weights:0', [2, 128, 256]), ('wavenet/cnn-11/biases:0', [256]), ('wavenet/cnn-11-proj/weights:0', [128, 192]), ('wavenet/cnn-11-proj/biases:0', [192]), ('dense1/weights:0', [768, 100]), ('dense1/biases:0', [100]), ('dense-2/weights:0', [100, 2]), ('dense-2/biases:0', [2])]\n",
      "trainable parameter count:\n",
      "4606416\n",
      "built graph\n"
     ]
    }
   ],
   "source": [
    " with tf.Graph().as_default() as graph:\n",
    "        \n",
    "    user_id = tf.placeholder(tf.int32, [None])\n",
    "    master_aisle_id = tf.placeholder(tf.int32, [None])\n",
    "    master_department_id = tf.placeholder(tf.int32, [None])\n",
    "    history_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    IsInOrder_history = tf.placeholder(tf.int32, [None, N]) #\n",
    "    order_dow_history = tf.placeholder(tf.int32, [None, N])\n",
    "    order_hour_of_day_history = tf.placeholder(tf.int32, [None, N])\n",
    "    days_since_prior_order_history = tf.placeholder(tf.int32, [None, N])\n",
    "    OrderSize_history = tf.placeholder(tf.int32, [None, N]) #\n",
    "    IndexInOrder_history = tf.placeholder(tf.int32, [None, N]) #\n",
    "    order_number_history = tf.placeholder(tf.int32, [None, N])\n",
    "    NumProductsFromDep_history = tf.placeholder(tf.int32, [None, N]) #\n",
    "    NextInOrder_history = tf.placeholder(tf.int32, [None, N])\n",
    "    \n",
    "    label = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    \n",
    "    ProductID1_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID2_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID3_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID4_history_a = tf.placeholder(tf.int32, [None, N])   \n",
    "    ProductID5_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID6_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID7_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID8_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID9_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID11_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID10_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID12_history_a = tf.placeholder(tf.int32, [None, N])   \n",
    "    ProductID13_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID14_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID15_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID16_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID17_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID18_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID19_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID20_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    \n",
    "    ProductNameEmbedding_history = tf.placeholder(tf.float32, [None, N, 50])\n",
    "\n",
    "    ProductID1_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID2_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID3_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID4_history = tf.placeholder(tf.int32, [None, N])   \n",
    "    ProductID5_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID6_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID7_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID8_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID9_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID11_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID10_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID12_history = tf.placeholder(tf.int32, [None, N])   \n",
    "    ProductID13_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID14_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID15_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID16_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID17_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID18_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID19_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID20_history = tf.placeholder(tf.int32, [None, N])\n",
    "  \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    # aisle data\n",
    "    aisle_embeddings = tf.get_variable(\n",
    "        name='aisle_embeddings',\n",
    "        shape=[135, 50],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "  \n",
    "    # department data\n",
    "    dept_embeddings = tf.get_variable(\n",
    "        name='dept_embeddings',\n",
    "        shape=[22, 10],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "  \n",
    "    x_aisle = tf.concat([\n",
    "        tf.nn.embedding_lookup(aisle_embeddings, master_aisle_id),\n",
    "        tf.nn.embedding_lookup(dept_embeddings, master_department_id),\n",
    "    ], axis=1)\n",
    "    x_aisle = tf.tile(tf.expand_dims(x_aisle, 1), (1, N, 1))\n",
    "\n",
    "    # user data\n",
    "    user_embeddings = tf.get_variable(\n",
    "        name='user_embeddings',\n",
    "        shape=[207000, min(lstm_size,3)],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    x_user = tf.nn.embedding_lookup(user_embeddings, user_id)\n",
    "    x_user = tf.tile(tf.expand_dims(x_user, 1), (1, N, 1))\n",
    "\n",
    "    # product data\n",
    "    product_embeddings = tf.get_variable(\n",
    "        name='product_embeddings',\n",
    "        shape=[50000, 50],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "       \n",
    "    x_product_1a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID1_history_a))\n",
    "    x_product_2a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID2_history_a))\n",
    "    x_product_3a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID3_history_a))\n",
    "    x_product_4a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID4_history_a))\n",
    "    x_product_5a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID5_history_a))\n",
    "    x_product_6a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID6_history_a))\n",
    "    x_product_7a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID7_history_a))\n",
    "    x_product_8a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID8_history_a))\n",
    "    x_product_9a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID9_history_a))\n",
    "    x_product_10a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID10_history_a))\n",
    "    x_product_11a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID11_history_a))\n",
    "    x_product_12a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID12_history_a))\n",
    "    x_product_13a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID13_history_a))\n",
    "    x_product_14a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID14_history_a))\n",
    "    x_product_15a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID15_history_a))\n",
    "    x_product_16a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID16_history_a))\n",
    "    x_product_17a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID17_history_a))\n",
    "    x_product_18a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID18_history_a))\n",
    "    x_product_19a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID19_history_a))\n",
    "    x_product_20a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID20_history_a))\n",
    "    \n",
    "    x_product_1 = tf.nn.embedding_lookup(product_embeddings, (ProductID1_history))\n",
    "    x_product_2 = tf.nn.embedding_lookup(product_embeddings, (ProductID2_history))\n",
    "    x_product_3 = tf.nn.embedding_lookup(product_embeddings, (ProductID3_history))\n",
    "    x_product_4 = tf.nn.embedding_lookup(product_embeddings, (ProductID4_history))\n",
    "    x_product_5 = tf.nn.embedding_lookup(product_embeddings, (ProductID5_history))\n",
    "    x_product_6 = tf.nn.embedding_lookup(product_embeddings, (ProductID6_history))\n",
    "    x_product_7 = tf.nn.embedding_lookup(product_embeddings, (ProductID7_history))\n",
    "    x_product_8 = tf.nn.embedding_lookup(product_embeddings, (ProductID8_history))\n",
    "    x_product_9 = tf.nn.embedding_lookup(product_embeddings, (ProductID9_history))\n",
    "    x_product_10 = tf.nn.embedding_lookup(product_embeddings, (ProductID10_history))\n",
    "    x_product_11 = tf.nn.embedding_lookup(product_embeddings, (ProductID11_history))\n",
    "    x_product_12 = tf.nn.embedding_lookup(product_embeddings, (ProductID12_history))\n",
    "    x_product_13 = tf.nn.embedding_lookup(product_embeddings, (ProductID13_history))\n",
    "    x_product_14 = tf.nn.embedding_lookup(product_embeddings, (ProductID14_history))\n",
    "    x_product_15 = tf.nn.embedding_lookup(product_embeddings, (ProductID15_history))\n",
    "    x_product_16 = tf.nn.embedding_lookup(product_embeddings, (ProductID16_history))\n",
    "    x_product_17 = tf.nn.embedding_lookup(product_embeddings, (ProductID17_history))\n",
    "    x_product_18 = tf.nn.embedding_lookup(product_embeddings, (ProductID18_history))\n",
    "    x_product_19 = tf.nn.embedding_lookup(product_embeddings, (ProductID19_history))\n",
    "    x_product_20 = tf.nn.embedding_lookup(product_embeddings, (ProductID20_history))\n",
    "    \n",
    "   \n",
    "    # sequence data\n",
    "    IsInOrder_history_oh = tf.one_hot(IsInOrder_history, 2)\n",
    "    order_dow_history_oh = tf.one_hot(order_dow_history, 8)\n",
    "    order_hour_of_day_history_oh = tf.one_hot(order_hour_of_day_history, 25)\n",
    "    days_since_prior_order_history_oh = tf.one_hot(days_since_prior_order_history, 31)\n",
    "    OrderSize_history_oh = tf.one_hot(OrderSize_history, 60)\n",
    "    IndexInOrder_history_oh = tf.one_hot(OrderSize_history, 60)\n",
    "    order_number_history_oh = tf.one_hot(order_number_history, 101)\n",
    "    NumProductsFromDep_history_oh = tf.one_hot(NumProductsFromDep_history, 50)\n",
    "    \n",
    "    order_dow_history_scalar = tf.expand_dims(tf.cast(order_dow_history, tf.float32) / 8.0, 2)\n",
    "    order_hour_of_day_history_scalar = tf.expand_dims(tf.cast(order_hour_of_day_history, tf.float32) / 25.0, 2)\n",
    "    days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "    OrderSize_history_scalar = tf.expand_dims(tf.cast(OrderSize_history, tf.float32) / 60.0, 2)\n",
    "    IndexInOrder_history_scalar = tf.expand_dims(tf.cast(IndexInOrder_history, tf.float32) / 60.0, 2)\n",
    "    order_number_history_scalar = tf.expand_dims(tf.cast(order_number_history, tf.float32) / 100.0, 2)\n",
    "    NumProductsFromDep_history_scalar = tf.expand_dims(tf.cast(NumProductsFromDep_history, tf.float32) / 50.0, 2)\n",
    "\n",
    "\n",
    "\n",
    "    x_history = tf.concat([IsInOrder_history_oh,order_dow_history_oh,order_hour_of_day_history_oh,days_since_prior_order_history_oh,\\\n",
    "        OrderSize_history_oh,IndexInOrder_history_oh,NumProductsFromDep_history_oh,order_number_history_oh,order_dow_history_scalar,\\\n",
    "        order_hour_of_day_history_scalar,days_since_prior_order_history_scalar,OrderSize_history_scalar,IndexInOrder_history_scalar,\\\n",
    "        order_number_history_scalar,NumProductsFromDep_history_scalar,x_product_1,x_product_2,x_product_3,\\\n",
    "        x_product_4,x_product_5,x_product_6,x_product_7,x_product_8,x_product_9\\\n",
    "        ,x_product_10,x_product_11,x_product_12,x_product_13,\\\n",
    "        x_product_14,x_product_15,x_product_16,x_product_17,x_product_18,\\\n",
    "        x_product_19,x_product_20,x_product_1a,x_product_2a,x_product_3a,\\\n",
    "        x_product_4a,x_product_5a,x_product_6a,x_product_7a,x_product_8a,x_product_9a\\\n",
    "        ,x_product_10a,x_product_11a,x_product_12a,x_product_13a,\\\n",
    "        x_product_14a,x_product_15a,x_product_16a,x_product_17a,x_product_18a,\\\n",
    "        x_product_19a,x_product_20a,ProductNameEmbedding_history] \\\n",
    "        , axis=2)\n",
    "    \n",
    "    x = tf.concat([x_history, x_aisle, x_user], axis=2)\n",
    "    #h = lstm_layer(x, history_length, lstm_size, scope='lstm1')\n",
    "\n",
    "    if WaveNet:\n",
    "        c = wavenet(x, dilations, filter_widths, skip_channels, residual_channels)\n",
    "        h = c\n",
    "    else:\n",
    "        h = lstm_layer(x, history_length, lstm_size, scope='lstm1')\n",
    "        h = tf.concat([h ], axis=2)\n",
    "    \n",
    "\n",
    "    h_final = time_distributed_dense_layer(h, 100 , activation=tf.nn.relu, scope='dense1') \n",
    "    #y_hat = tf.squeeze(time_distributed_dense_layer(h_final, 1, activation=tf.nn.sigmoid, scope='dense3'), 2)\n",
    "\n",
    "    n_components = 1\n",
    "    params = time_distributed_dense_layer(h_final, n_components*2, scope='dense-2', activation=None)\n",
    "    ps, mixing_coefs = tf.split(params, 2, axis=2)\n",
    "\n",
    "    # this is implemented incorrectly, but it still helped...\n",
    "    mixing_coefs = tf.nn.softmax(mixing_coefs - tf.reduce_min(mixing_coefs, 2, keep_dims=True))\n",
    "    ps = tf.nn.sigmoid(ps)\n",
    "\n",
    "    #loss = sequence_log_loss(NextInOrder_history, preds, history_length, N)\n",
    "    labels = tf.tile(tf.expand_dims(NextInOrder_history, 2), (1, 1, n_components))\n",
    "    losses = tf.reduce_sum(mixing_coefs*tensor_log_loss(labels, ps), axis=2)\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(history_length, maxlen=N), tf.float32)\n",
    "    loss = tf.reduce_sum(losses*sequence_mask) / tf.cast(tf.reduce_sum(history_length), tf.float32)\n",
    "\n",
    "    \n",
    "    final_temporal_idx = tf.stack([tf.range(tf.shape(history_length)[0]), history_length - 1], axis=1)\n",
    "    final_states = tf.gather_nd(h_final, final_temporal_idx)\n",
    "    #final_predictions = tf.gather_nd(y_hat, final_temporal_idx)\n",
    "    final_predictions = tf.gather_nd(ps, final_temporal_idx)\n",
    "    ground_truth = tf.gather_nd(NextInOrder_history, final_temporal_idx)\n",
    "    \n",
    "    prediction_tensors = {\n",
    "        'user_ids': user_id,\n",
    "        'aisle_ids': master_aisle_id,\n",
    "        'final_states': final_states,\n",
    "        'predictions': final_predictions,\n",
    "        'ground_truth': label, #BUG FIX\n",
    "        'history_length':history_length\n",
    "    }\n",
    "    #preds = y_hat\n",
    "    preds = ps\n",
    "        \n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate_var = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "    if regularization_constant != 0:\n",
    "        l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()])\n",
    "        loss = loss + regularization_constant*l2_norm\n",
    "\n",
    "    if opt == 'adam':\n",
    "        optimizer =  tf.train.AdamOptimizer(learning_rate_var)\n",
    "    elif opt == 'gd':\n",
    "        optimizer =  tf.train.GradientDescentOptimizer(learning_rate_var)\n",
    "    elif opt == 'rms':\n",
    "        optimizer =  tf.train.RMSPropOptimizer(learning_rate_var, decay=0.95, momentum=0.9)\n",
    "    else:\n",
    "        assert False, 'optimizer must be adam, gd, or rms'\n",
    "            \n",
    "    grads = optimizer.compute_gradients(loss)\n",
    "    clipped = [(tf.clip_by_value(g, -grad_clip, grad_clip), v_) for g, v_ in grads]\n",
    "\n",
    "    step = optimizer.apply_gradients(clipped, global_step=global_step)\n",
    "\n",
    "    print('trainable parameters:')\n",
    "    print([(var.name, shape(var)) for var in tf.trainable_variables()])\n",
    "\n",
    "    print('trainable parameter count:')\n",
    "    print(str(np.sum(np.prod(shape(var)) for var in tf.trainable_variables())))\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "session = tf.Session(graph=graph)\n",
    "print('built graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[step        0]]     [[train]]     loss: 0.61200756       [[val]]     loss: 0                [[val-seq]]     loss: 0.397838         \n",
      "[[step      999]]     [[train]]     loss: 0.16791001       [[val]]     loss: 0.50176212       [[val-seq]]     loss: 0.16738042       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:571: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[step     1998]]     [[train]]     loss: 0.1661299        [[val]]     loss: 0.50443552       [[val-seq]]     loss: 0.16611269       \n",
      "[[step     2997]]     [[train]]     loss: 0.16488369       [[val]]     loss: 0.49103145       [[val-seq]]     loss: 0.16486529       \n",
      "[[step     3996]]     [[train]]     loss: 0.16505543       [[val]]     loss: 0.50392556       [[val-seq]]     loss: 0.16503042       \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-0dfcc963cf7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    289\u001b[0m         val_seq_loss,np_tensors = session.run(\n\u001b[0;32m    290\u001b[0m             \u001b[0mfetches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf_tensors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_feed_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m         )\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with session.as_default():\n",
    "    debug = False\n",
    "\n",
    "    if warm_start_init_step:\n",
    "        restore(warm_start_init_step)\n",
    "        step_number = warm_start_init_step\n",
    "    else:\n",
    "        session.run(init)\n",
    "        step_number = 0\n",
    "\n",
    "    best_validation_loss, best_validation_tstep = float('inf'), 0\n",
    "    restarts = 0\n",
    "    \n",
    "    #<AUC change>\n",
    "    \n",
    "    validation_predictions = np.zeros(shape=[batch_size*log_interval,1],dtype=np.float32)\n",
    "    validation_aisle = np.zeros(shape=[batch_size*log_interval,1],dtype=np.int32)\n",
    "    validation_ground_truths = np.zeros(shape=[batch_size*log_interval,1],dtype=np.int32)\n",
    "    validation_index = 0\n",
    "    \n",
    "    #</AUC change>\n",
    "    \n",
    "    train_loss_history = deque(maxlen=loss_averaging_window)\n",
    "    val_loss_history = deque(maxlen=loss_averaging_window)\n",
    "    #val_loss_history.append(0)\n",
    "    val_seq_loss_history = deque(maxlen=loss_averaging_window)\n",
    "    \n",
    "    while step_number < num_training_steps:\n",
    "\n",
    "        next_batch =  next(train.next_batch(batch_size))\n",
    "                    \n",
    "        # train step\n",
    "        train_feed_dict = next_batch\n",
    "\n",
    "        # Use n-2 for training so that n-1 can be used for validation\n",
    "\n",
    "        if debug:\n",
    "            print(next_batch[globals()['history_length']])\n",
    "        train_feed_dict.update({globals()['history_length']:train_feed_dict[globals()['history_length']]-1})\n",
    "        \n",
    "        if debug:\n",
    "            print(train_feed_dict[globals()['history_length']])\n",
    "\n",
    "        sequence_lengths = train_feed_dict[globals()['history_length']]\n",
    "        if debug:\n",
    "            print(sequence_lengths)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "        \n",
    "        # Remove the input data for n-1 step for training to avoid leakage\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['NextInOrder_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['NextInOrder_history']:temp_array})\n",
    "            \n",
    "            temp_array = train_feed_dict[globals()['order_number_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['order_number_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['order_hour_of_day_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['order_hour_of_day_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['order_dow_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['order_dow_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['days_since_prior_order_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['days_since_prior_order_history']:temp_array})\n",
    "                       \n",
    "            temp_array = train_feed_dict[globals()['IsInOrder_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['IsInOrder_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['OrderSize_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['OrderSize_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['IndexInOrder_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['IndexInOrder_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['NumProductsFromDep_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['NumProductsFromDep_history']:temp_array})\n",
    "\n",
    "            if True:\n",
    "                temp_array = train_feed_dict[globals()['ProductID1_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID1_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID2_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID2_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID3_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID3_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID4_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID4_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID5_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID5_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID6_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID6_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID7_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID7_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID8_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID8_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID9_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID9_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID10_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID10_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID11_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID11_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID12_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID12_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID13_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID13_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID14_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID14_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID15_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID15_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID16_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID16_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID17_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID17_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID18_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID18_history_a']:temp_array})      \n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID19_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID19_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID20_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID20_history_a']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductNameEmbedding_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=np.zeros(shape=[50],dtype=np.float16)\n",
    "            train_feed_dict.update({globals()['ProductNameEmbedding_history']:temp_array})\n",
    "            \n",
    "            temp_array = train_feed_dict[globals()['ProductID1_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID1_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID2_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID2_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID3_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID3_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID4_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID4_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID5_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID5_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID6_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID6_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID7_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID7_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID8_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID8_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID9_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID9_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID10_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID10_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID11_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID11_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID12_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID12_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID13_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID13_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID14_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID14_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID15_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID15_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID16_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID16_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID17_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID17_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID18_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID18_history']:temp_array})      \n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID19_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID19_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID20_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID20_history']:temp_array})\n",
    "\n",
    "            \n",
    "            \n",
    "        train_feed_dict.update({learning_rate_var: learning_rate})\n",
    "        train_feed_dict.update({keep_prob: keep_prob_scalar})\n",
    "        train_feed_dict.update({is_training: True})\n",
    "                \n",
    "        debug_dict = {tensor_name: [] for tensor_name in prediction_tensors} #changed\n",
    "        tensor_names, tf_tensors = zip(*prediction_tensors.items()) #changed\n",
    "\n",
    "        train_loss,_,np_tensors = session.run(   #changed\n",
    "            fetches=[loss,step,tf_tensors], #changed\n",
    "            feed_dict=train_feed_dict\n",
    "        )\n",
    "        \n",
    "        for tensor_name, tensor in zip(tensor_names, np_tensors): #changed\n",
    "            debug_dict[tensor_name].append(tensor)#changed \n",
    "        \n",
    "        if debug:\n",
    "            print(debug_dict['history_length'])#changed\n",
    "        \n",
    "        train_loss_history.append(train_loss)\n",
    "        \n",
    "        #BEGIN\n",
    "        prediction_dict = {tensor_name: [] for tensor_name in prediction_tensors}\n",
    "\n",
    "        # test evaluation\n",
    "        \n",
    "        val_feed_dict = next_batch\n",
    "        val_feed_dict.update({learning_rate_var: learning_rate})\n",
    "        val_feed_dict.update({keep_prob: keep_prob_scalar})\n",
    "        val_feed_dict.update({is_training: False})\n",
    "\n",
    "        if debug:\n",
    "            print(val_feed_dict[globals()['history_length']]+1)\n",
    "        \n",
    "        tensor_names, tf_tensors = zip(*prediction_tensors.items())\n",
    "        val_seq_loss,np_tensors = session.run(\n",
    "            fetches=[loss,tf_tensors],\n",
    "            feed_dict=val_feed_dict\n",
    "        )\n",
    "\n",
    "        for tensor_name, tensor in zip(tensor_names, np_tensors):\n",
    "            prediction_dict[tensor_name].append(tensor)\n",
    "        #print(prediction_dict['ground_truth'])\n",
    "        #print(prediction_dict['predictions'])\n",
    "        if(np.sum(prediction_dict['ground_truth'][0])>0):\n",
    "            val_loss = log_loss(prediction_dict['ground_truth'][0],prediction_dict['predictions'][0])\n",
    "        #END\n",
    "            val_loss_history.append(val_loss)\n",
    "        val_seq_loss_history.append(val_seq_loss)\n",
    "        #<AUC change>\n",
    "        #print(validation_index,(validation_index+batch_size),(log_interval*batch_size))\n",
    "        validation_predictions[validation_index:(validation_index+batch_size)] = np.reshape(prediction_dict['predictions'][0],(batch_size,1))\n",
    "        validation_aisle[validation_index:(validation_index+batch_size)] = np.reshape(prediction_dict['aisle_ids'][0],(batch_size,1))\n",
    "        validation_ground_truths[validation_index:(validation_index+batch_size)] = np.reshape(prediction_dict['ground_truth'][0],(batch_size,1))\n",
    "        validation_index += batch_size\n",
    "        #</AUC change>\n",
    "        if debug:\n",
    "            #print(train_loss, val_loss)\n",
    "            #print(prediction_dict['ground_truth'])\n",
    "            print(prediction_dict['history_length'])\n",
    "            break\n",
    "        \n",
    "        if step_number % (log_interval-1) == 0: #<AUC CHange>\n",
    "            avg_train_loss = sum(train_loss_history)/len(train_loss_history)\n",
    "            denominator = len(val_loss_history)\n",
    "            if denominator ==0:\n",
    "                denominator=1\n",
    "            avg_val_loss = sum(val_loss_history)/denominator\n",
    "            avg_val_seq_loss = sum(val_seq_loss_history)/len(val_seq_loss_history)\n",
    "            \n",
    "             #<AUC change>  \n",
    "            if step_number>0:\n",
    "                avg_val_loss = -findAverageAUC(validation_predictions,validation_aisle,validation_ground_truths)\n",
    "                validation_predictions = np.zeros(shape=[batch_size*log_interval,1],dtype=np.float32)\n",
    "                validation_aisle = np.zeros(shape=[batch_size*log_interval,1],dtype=np.int32)\n",
    "                validation_ground_truths = np.zeros(shape=[batch_size*log_interval,1],dtype=np.int32)\n",
    "                validation_index = 0\n",
    "            else:\n",
    "                avg_val_loss = 0\n",
    "            #</AUC change>\n",
    "            \n",
    "            metric_log = (\n",
    "                \"[[step {:>8}]]     \"\n",
    "                \"[[train]]     loss: {:<12}     \"\n",
    "                \"[[val]]     loss: {:<12}     \"\n",
    "                \"[[val-seq]]     loss: {:<12}     \"\n",
    "            ).format(step_number, round(avg_train_loss, 8), round(-avg_val_loss, 8), round(avg_val_seq_loss, 8)) #<AUC Change>\n",
    "            print(metric_log)\n",
    "            \n",
    "            if avg_val_loss < best_validation_loss:\n",
    "                best_validation_loss = avg_val_loss\n",
    "                best_validation_tstep = step_number\n",
    "                if step_number > min_steps_to_checkpoint:\n",
    "                    model_path = os.path.join(checkpoint_dir, 'model{}.ckpt'.format(best_validation_tstep))\n",
    "                    saver.save(session, model_path, global_step=best_validation_tstep) \n",
    "                    \n",
    "            if step_number - best_validation_tstep > early_stopping_steps:\n",
    "\n",
    "                if num_restarts is None or restarts >= num_restarts:\n",
    "                    print('best validation loss of {} at training step {}'.format(\n",
    "                        best_validation_loss, best_validation_tstep))\n",
    "                    print('early stopping - ending training.')\n",
    "                    break\n",
    "\n",
    "                if restarts < num_restarts:\n",
    "                    model_path = os.path.join(checkpoint_dir, 'model{}.ckpt-{}'.format(best_validation_tstep,best_validation_tstep))\n",
    "                    saver.restore(session, model_path)\n",
    "                    print('halving learning rate')\n",
    "                    learning_rate /= 2.0\n",
    "                    early_stopping_steps /= 2\n",
    "                    step_number = best_validation_tstep\n",
    "                    restarts += 1\n",
    "\n",
    "        step_number += 1\n",
    "\n",
    "    if step_number <= min_steps_to_checkpoint:\n",
    "        best_validation_tstep = step_number\n",
    "        model_path = os.path.join(checkpoint_dir, 'model{}.ckpt'.format(best_validation_tstep))\n",
    "        saver.save(session, model_path, global_step=best_validation_tstep) \n",
    "                    \n",
    "    print('num_training_steps reached - ending training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41730564314492313, 31000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validation_loss, best_validation_tstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_validation_tstep = 31000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/userdata/projects/instakart/np/checkpoints/model31000.ckpt-31000'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join(checkpoint_dir, 'model{}.ckpt'.format(best_validation_tstep))\n",
    "model_path\n",
    "saver.save(session, model_path, global_step=best_validation_tstep) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = dataset(path_out,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /userdata/projects/instakart/np/checkpoints/model31000.ckpt-31000\n"
     ]
    }
   ],
   "source": [
    "model_path='/userdata/projects/instakart/np/checkpoints/model31000.ckpt-31000'\n",
    "saver.restore(session, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39062\n"
     ]
    }
   ],
   "source": [
    "prediction_dict = {tensor_name: [] for tensor_name in prediction_tensors}\n",
    "\n",
    "num_val_batches = int(val.arrays[0].shape[0]/batch_size)\n",
    "num_val_batches = int(5000000/batch_size)\n",
    "print(num_val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took me 710.586 seconds to do this..\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "for i in range(num_val_batches):\n",
    "    val_dict_list =  next(val.next_batch(batch_size))\n",
    "        \n",
    "    # test evaluation\n",
    "    val_feed_dict = val_dict_list\n",
    "    val_feed_dict.update({learning_rate_var: learning_rate})\n",
    "    val_feed_dict.update({keep_prob: 1.0})\n",
    "    val_feed_dict.update({is_training: False})\n",
    "\n",
    "    tensor_names, tf_tensors = zip(*prediction_tensors.items())\n",
    "    np_tensors = session.run(\n",
    "        fetches=tf_tensors,\n",
    "        feed_dict=val_feed_dict\n",
    "    )\n",
    "    for tensor_name, tensor in zip(tensor_names, np_tensors):\n",
    "        prediction_dict[tensor_name].append(tensor)\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving user_ids with shape (4999936,) to /userdata/projects/instakart/np/predictions/user_ids.npy\n",
      "saving aisle_ids with shape (4999936,) to /userdata/projects/instakart/np/predictions/aisle_ids.npy\n",
      "saving final_states with shape (4999936, 10) to /userdata/projects/instakart/np/predictions/final_states.npy\n",
      "saving predictions with shape (4999936, 1) to /userdata/projects/instakart/np/predictions/predictions.npy\n",
      "saving ground_truth with shape (4999936,) to /userdata/projects/instakart/np/predictions/ground_truth.npy\n",
      "saving history_length with shape (4999936,) to /userdata/projects/instakart/np/predictions/history_length.npy\n"
     ]
    }
   ],
   "source": [
    "for tensor_name, tensor in prediction_dict.items():\n",
    "    np_tensor = np.concatenate(tensor, 0)\n",
    "    save_file = os.path.join(prediction_dir, '{}.npy'.format(tensor_name))\n",
    "    print('saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "    np.save(save_file, np_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLift(predics,actual, user_ids , a_ids ,verbosity=0):\n",
    "    \n",
    "    aisles = np.unique(a_ids)\n",
    "    users = np.unique(user_ids)\n",
    "    cum_lift = 0.0\n",
    "    cnt = 0\n",
    "\n",
    "    for n in range(len(aisles)):\n",
    "\n",
    "        predics_aisle = predics[np.where(a_ids==aisles[n])]\n",
    "        actual_aisle = actual[np.where(a_ids==aisles[n])]\n",
    "        users_aisle = user_ids[np.where(a_ids==aisles[n])]\n",
    "\n",
    "        i = np.where(predics_aisle>np.percentile(predics_aisle,90))\n",
    "        j = np.where(predics_aisle>0)\n",
    "        num_i = len(i[0])*1.0\n",
    "        num_j = len(j[0])*1.0\n",
    "        num_users = users.shape[0]\n",
    "\n",
    "        up = (np.sum(actual_aisle[i[0]])*1.0)/num_i\n",
    "        down = (np.sum(actual_aisle[j[0]])*1.0)/num_j\n",
    "        lift = round(up/down,3)\n",
    "        if lift>0:\n",
    "            cum_lift += lift\n",
    "            cnt += 1\n",
    "\n",
    "        if verbosity==1:\n",
    "            print(lift , round(up,2),round(down,2), round(num_i/num_j,3), round((num_j*100.0)/num_users,2))\n",
    "\n",
    "    return round(cum_lift/cnt,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4999936, 1) (4999936,) (4999936,) (4999936,)\n"
     ]
    }
   ],
   "source": [
    "predics = np.load(prediction_dir+'/predictions.npy')\n",
    "actual = np.load(prediction_dir+'/ground_truth.npy')\n",
    "a_ids = np.load(prediction_dir+'/aisle_ids.npy')\n",
    "u_ids = np.load(prediction_dir+'/user_ids.npy')\n",
    "print(predics.shape,actual.shape, a_ids.shape, u_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.789 0.46 0.12 0.1 8.87\n",
      "4.55 0.5 0.11 0.1 13.45\n",
      "2.647 0.44 0.17 0.1 27.23\n",
      "3.167 0.45 0.14 0.1 23.15\n",
      "5.697 0.46 0.08 0.1 14.01\n",
      "5.551 0.51 0.09 0.1 7.56\n",
      "4.803 0.48 0.1 0.1 5.98\n",
      "5.178 0.49 0.1 0.1 5.65\n",
      "3.506 0.43 0.12 0.1 29.19\n",
      "7.86 0.49 0.06 0.1 2.57\n",
      "6.571 0.54 0.08 0.1 5.39\n",
      "4.719 0.47 0.1 0.1 5.13\n",
      "3.302 0.46 0.14 0.1 10.86\n",
      "3.038 0.45 0.15 0.1 11.45\n",
      "4.905 0.46 0.09 0.1 3.9\n",
      "2.834 0.44 0.16 0.1 32.22\n",
      "3.773 0.42 0.11 0.1 36.79\n",
      "4.286 0.48 0.11 0.1 2.68\n",
      "4.111 0.43 0.1 0.1 36.63\n",
      "5.199 0.48 0.09 0.1 11.66\n",
      "2.066 0.47 0.23 0.1 56.56\n",
      "5.74 0.44 0.08 0.1 6.14\n",
      "3.39 0.45 0.13 0.1 19.0\n",
      "1.482 0.6 0.4 0.1 74.64\n",
      "5.19 0.44 0.09 0.1 10.9\n",
      "3.009 0.45 0.15 0.1 19.88\n",
      "3.094 0.41 0.13 0.1 3.42\n",
      "3.419 0.41 0.12 0.1 2.88\n",
      "5.831 0.46 0.08 0.1 14.51\n",
      "4.745 0.47 0.1 0.1 13.19\n",
      "2.498 0.49 0.2 0.1 35.94\n",
      "2.509 0.52 0.21 0.1 19.7\n",
      "5.496 0.51 0.09 0.1 2.19\n",
      "4.024 0.47 0.12 0.1 11.03\n",
      "3.526 0.45 0.13 0.1 16.43\n",
      "3.549 0.44 0.12 0.1 33.68\n",
      "2.693 0.46 0.17 0.1 38.04\n",
      "2.654 0.47 0.18 0.1 24.89\n",
      "5.203 0.44 0.08 0.1 5.41\n",
      "3.237 0.46 0.14 0.1 3.16\n",
      "2.387 0.47 0.2 0.1 3.37\n",
      "3.147 0.48 0.15 0.1 10.04\n",
      "3.795 0.47 0.12 0.1 15.75\n",
      "7.074 0.56 0.08 0.1 2.7\n",
      "3.118 0.45 0.14 0.1 24.07\n",
      "4.136 0.48 0.12 0.1 2.86\n",
      "5.815 0.47 0.08 0.1 8.14\n",
      "3.425 0.46 0.13 0.1 8.04\n",
      "3.341 0.45 0.14 0.1 16.31\n",
      "3.304 0.44 0.13 0.1 17.83\n",
      "4.278 0.46 0.11 0.1 18.35\n",
      "2.836 0.45 0.16 0.1 20.57\n",
      "2.699 0.45 0.17 0.1 28.04\n",
      "2.971 0.47 0.16 0.1 25.06\n",
      "6.576 0.51 0.08 0.1 2.5\n",
      "4.29 0.49 0.11 0.1 3.09\n",
      "3.775 0.45 0.12 0.1 12.01\n",
      "4.881 0.44 0.09 0.1 6.77\n",
      "3.185 0.43 0.14 0.1 29.79\n",
      "6.111 0.51 0.08 0.1 7.59\n",
      "3.242 0.46 0.14 0.1 22.82\n",
      "3.219 0.44 0.14 0.1 2.66\n",
      "4.402 0.43 0.1 0.1 22.86\n",
      "2.866 0.49 0.17 0.1 8.42\n",
      "4.672 0.48 0.1 0.1 5.49\n",
      "4.55 0.45 0.1 0.1 23.37\n",
      "2.935 0.47 0.16 0.1 34.39\n",
      "5.158 0.43 0.08 0.1 3.0\n",
      "3.272 0.43 0.13 0.1 36.14\n",
      "5.241 0.5 0.09 0.1 4.95\n",
      "3.672 0.44 0.12 0.1 5.03\n",
      "4.189 0.42 0.1 0.1 34.2\n",
      "5.24 0.53 0.1 0.1 2.71\n",
      "4.903 0.41 0.08 0.1 17.16\n",
      "4.459 0.46 0.1 0.1 13.38\n",
      "5.416 0.46 0.09 0.1 2.62\n",
      "2.546 0.49 0.19 0.1 27.24\n",
      "2.727 0.46 0.17 0.1 38.21\n",
      "3.018 0.46 0.15 0.1 16.92\n",
      "6.645 0.54 0.08 0.1 3.95\n",
      "3.345 0.42 0.13 0.1 32.17\n",
      "5.274 0.46 0.09 0.1 1.48\n",
      "1.59 0.58 0.37 0.1 67.17\n",
      "2.014 0.51 0.25 0.1 45.64\n",
      "5.855 0.43 0.07 0.1 13.65\n",
      "2.219 0.45 0.2 0.1 38.52\n",
      "6.891 0.5 0.07 0.1 4.49\n",
      "3.576 0.44 0.12 0.1 34.87\n",
      "4.685 0.45 0.1 0.1 17.54\n",
      "5.665 0.49 0.09 0.1 4.08\n",
      "2.209 0.5 0.23 0.1 41.05\n",
      "2.479 0.46 0.19 0.1 10.83\n",
      "2.926 0.45 0.15 0.1 23.74\n",
      "3.579 0.46 0.13 0.1 22.71\n",
      "4.08 0.45 0.11 0.1 10.65\n",
      "2.592 0.45 0.17 0.1 32.9\n",
      "7.075 0.43 0.06 0.1 6.29\n",
      "2.956 0.46 0.15 0.1 31.77\n",
      "3.587 0.47 0.13 0.1 11.17\n",
      "2.263 0.5 0.22 0.1 12.78\n",
      "5.759 0.52 0.09 0.1 3.94\n",
      "6.406 0.4 0.06 0.1 2.09\n",
      "7.14 0.47 0.07 0.1 2.86\n",
      "4.462 0.41 0.09 0.1 32.69\n",
      "4.209 0.45 0.11 0.1 15.82\n",
      "2.919 0.43 0.15 0.1 31.18\n",
      "2.302 0.45 0.2 0.1 46.99\n",
      "3.047 0.44 0.14 0.1 33.31\n",
      "6.263 0.48 0.08 0.1 2.27\n",
      "4.444 0.45 0.1 0.1 16.35\n",
      "4.938 0.47 0.1 0.1 6.65\n",
      "2.253 0.45 0.2 0.1 43.54\n",
      "4.937 0.56 0.11 0.1 0.83\n",
      "4.548 0.45 0.1 0.1 16.74\n",
      "2.063 0.54 0.26 0.1 42.59\n",
      "2.77 0.45 0.16 0.1 42.64\n",
      "3.434 0.45 0.13 0.1 32.64\n",
      "7.299 0.53 0.07 0.1 2.8\n",
      "5.461 0.48 0.09 0.1 4.21\n",
      "2.014 0.52 0.26 0.1 52.64\n",
      "2.782 0.45 0.16 0.1 33.01\n",
      "4.024 0.47 0.12 0.1 10.96\n",
      "1.747 0.53 0.31 0.1 66.98\n",
      "3.259 0.46 0.14 0.1 2.33\n",
      "3.958 0.48 0.12 0.1 4.47\n",
      "5.8 0.52 0.09 0.1 3.79\n",
      "4.721 0.49 0.1 0.1 7.43\n",
      "3.442 0.45 0.13 0.1 23.41\n",
      "3.193 0.46 0.14 0.1 21.18\n",
      "4.212 0.43 0.1 0.1 23.03\n",
      "3.361 0.42 0.12 0.1 31.12\n",
      "7.519 0.47 0.06 0.1 1.81\n",
      "6.278 0.5 0.08 0.1 4.2\n",
      "4.541 0.48 0.1 0.1 1.59\n",
      "4.08\n"
     ]
    }
   ],
   "source": [
    "print(findLift(predics,actual,u_ids, a_ids,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "\n",
    "1. Batch Normalization\n",
    "2. 50, 300 User Embeddings\n",
    "3. Department Embeddings\n",
    "4. Product Level Model Stacked with Department Level Model\n",
    "5. wavenet\n",
    "6. dense layer"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
