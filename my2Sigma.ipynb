{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle : 2 Sigma Connect : Rental Listing Inquiries\n",
    "\n",
    "Purpose: To predict the interest level of a listing using the listing \n",
    "images only\n",
    "\n",
    "Author : Chandan Panda \n",
    "\n",
    "Version: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cp/home/ubuntu/data\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from random import uniform\n",
    "import bcolz\n",
    "import time\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers import Input, Dense\n",
    "import math\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils as u\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "%cd '/cp/home/ubuntu/data'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly divide the dataset into 5 folds and also keep a small sample dataset for debugging. Each fold contains about 54000 images\n",
    "\n",
    "The images are structured into folders named high, medium or low based on the interest level of the listing in which the image was uploaded\n",
    "\n",
    "We make sure that a listing's images are not scattered across both different folds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create 5 fold datasets\n",
    "\n",
    "%cd '/cp/home/ubuntu/data/photos'\n",
    "photos=glob(\"*.jpg\")\n",
    "\n",
    "CREATE_FOLDS = False\n",
    "NumberOfFolds = 5.0\n",
    "\n",
    "if CREATE_FOLDS==True:\n",
    "    p = pd.read_json('../train.json')\n",
    "    photos=glob(\"*.jpg\")\n",
    "    listings = list(set([f.split('_')[0] for f in photos]))\n",
    "    shuffled_listings = np.random.permutation(listings)\n",
    "    sample_size = int(round(len(listings) * (1/NumberOfFolds),0))\n",
    "    \n",
    "    for i in range(int(NumberOfFolds)): \n",
    "        os.mkdir('/cp/home/ubuntu/data/folds/fold_'+str(i))\n",
    "        for listing in shuffled_listings[range(i*sample_size,(i*sample_size)+sample_size)]:\n",
    "            pics=glob(listing+\"_*.jpg\")\n",
    "            for pic in pics:\n",
    "                os.rename(pic, '/cp/home/ubuntu/data/folds/'+'fold_'+str(i)+'/' + pic)\n",
    "\n",
    "        #Move photos into directories based on class type \n",
    "        s = '/cp/home/ubuntu/data/folds/fold_' + str(i)\n",
    "        os.chdir(s)\n",
    "        photos=glob('*.jpg')\n",
    "        os.mkdir('low')\n",
    "        os.mkdir('medium')\n",
    "        os.mkdir('high')\n",
    "        for photo in photos:\n",
    "            listing_id = photo.partition(\"_\")[0]\n",
    "            row = p.loc[p['listing_id'] == int(listing_id) ]\n",
    "            interest_level = 'NA'\n",
    "            if len(row)>0:\n",
    "                interest_level = row['interest_level'].iloc[0]\n",
    "                os.rename(photo, interest_level + '/' + photo)\n",
    "        os.chdir('/cp/home/ubuntu/data/photos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample a few images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To Be Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Pre-trained Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a version of the VGG network (a convolutional neural network by the Visual Geometry and Graphics group) that is trained of the ImageNet database to predict 1000 classes\n",
    "\n",
    "This version includes batch normalization layers that ensure that the weights not not become too large and improves the training time. In addition, it normalizes the image by subtracting the RGB mean values from the ImageNet database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 3, 224, 224)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  0           zeropadding2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  0           zeropadding2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 0           zeropadding2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 0           zeropadding2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   0           zeropadding2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   0           zeropadding2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   0           zeropadding2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   0           zeropadding2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   0           zeropadding2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 28, 28)   0           zeropadding2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 14, 14)   0           zeropadding2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 14, 14)   0           zeropadding2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 14, 14)   0           zeropadding2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 512, 7, 7)     0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25088)         0           maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 4096)          0           flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNormal(None, 4096)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4096)          0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4096)          0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNormal(None, 4096)          0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 4096)          0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 3)             12291       dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 12291\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from vgg16bn import Vgg16BN\n",
    "model = vgg_ft_bn(3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split the VGG network into the convolutional and dense layers. This is so that we can reuse the convolutional layers weights as it is and train only the final three dense layers. This will save enormous amount of time while making little difference to our final performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "gen=image.ImageDataGenerator(width_shift_range = 0.2 , height_shift_range = 0.2 , shear_range = 0.2 , zoom_range = 0.2)\n",
    "target_size=(224,224)\n",
    "batch_size = 64\n",
    "class_mode='categorical'\n",
    "shuffle=True\n",
    "dirname = 'train'\n",
    "model.compile(optimizer=Adam(1e-3),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "conv_layers,fc_layers = split_at(model, Convolution2D)\n",
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess all images in sample, training and validation sets by passing through the VGG network convolutional layers after a bit of data augmentation. These features (i.e. 512 X 14 X 14 per image) will then be used as inputs  to a dense layer network that finally outputs the probability of the image being low, medium or high interest_level\n",
    "\n",
    "Because of the large number of images (~200k training set, ~70k in validation set) and RAM constraint of 64 GB, we will divide the features into chunks of 10k images and use the fit_generator and evaluate_generator to iterate through each file chunk as we train and validate the dense layer network.\n",
    "\n",
    "Also, the images are randomly width shifted, height shifted, sheared and zoomed to introduce bias in order to reduce variance\n",
    "\n",
    "Each epoch will take approximately an hour!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateFeatures(dirname,CHUNK_SIZE = 10000):\n",
    "    GENERATE_FEATURES = True\n",
    "    if GENERATE_FEATURES==True:\n",
    "        batches = gen.flow_from_directory('/cp/home/ubuntu/data/folds/'+dirname, \n",
    "                                                         target_size=target_size,class_mode=class_mode, \n",
    "                                          shuffle=True, batch_size=1)\n",
    "        labels_all = onehot(batches.classes) \n",
    "        totalImages = batches.nb_sample\n",
    "        listing_ids_all = np.ndarray([totalImages,1],dtype=np.int64)\n",
    "        listing_ids_all = [int((f.split('_')[0]).split('/')[1]) for f in batches.filenames]\n",
    "        batchIndex = 0\n",
    "        numImages = 0\n",
    "        images = np.ndarray([CHUNK_SIZE,3, 224, 224],dtype=np.float32)\n",
    "        labels = np.ndarray([CHUNK_SIZE,3],dtype=np.float64)\n",
    "        listing_ids = np.ndarray([CHUNK_SIZE,1],dtype=np.int64)   \n",
    "        for i in range(totalImages):\n",
    "            if (numImages > (CHUNK_SIZE - 1)) or (i == (totalImages-1)):\n",
    "                labels = labels_all[batchIndex*CHUNK_SIZE:batchIndex*CHUNK_SIZE + numImages]\n",
    "                listing_ids = listing_ids_all[batchIndex*CHUNK_SIZE:batchIndex*CHUNK_SIZE + numImages]\n",
    "                save_array('features/' + dirname + '_labels_%d.dat'%batchIndex,labels[0:numImages])               \n",
    "                save_array('features/' + dirname + '_listing_ids_%d.dat'%batchIndex,listing_ids[0:numImages])                     \n",
    "                conv_feat = conv_model.predict_generator(generator=gen.flow(images[0:numImages],batch_size=batch_size)\n",
    "                                                         ,val_samples=numImages)        \n",
    "                save_array('features/' + dirname + '_trainX_%d.dat'%batchIndex,conv_feat[0:numImages])  \n",
    "                batchIndex += 1\n",
    "                images = np.ndarray([CHUNK_SIZE,3, 224, 224],dtype=np.float32)\n",
    "                labels = np.ndarray([CHUNK_SIZE,3],dtype=np.float64)\n",
    "                numImages = 0\n",
    "                del(conv_feat)\n",
    "                del(labels)\n",
    "                gc.collect()            \n",
    "            x,y = batches.next()\n",
    "            images[numImages] = x[0]\n",
    "            numImages += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generateFeatures('fold_0',CHUNK_SIZE=20000)\n",
    "generateFeatures('fold_1',CHUNK_SIZE=20000)\n",
    "generateFeatures('fold_2',CHUNK_SIZE=20000)\n",
    "generateFeatures('fold_3',CHUNK_SIZE=20000)\n",
    "generateFeatures('fold_4',CHUNK_SIZE=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generateFeatures('sample',CHUNK_SIZE=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move augmented features to another folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd '/cp/home/ubuntu/data/features'\n",
    "photos=glob(\"*fold*.dat\")\n",
    "for photo in photos:\n",
    "    os.rename(photo, '/cp/home/ubuntu/data/features-aug/'+photo)\n",
    "%cd '/cp/home/ubuntu/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to randomize fold feature chunks. This is currently hardcoded to handle exactly 3 filechunks per fold!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomizeFeatures(fold,folder):\n",
    "    os.chdir(folder)\n",
    "    Files = glob(fold+'_trainX*.dat')\n",
    "    chunk0 = load_array(Files[0])\n",
    "    chunk1 = load_array(Files[1])\n",
    "    chunk2 = load_array(Files[2])\n",
    "    chunk0size = chunk0.shape[0]\n",
    "    chunk1size = chunk1.shape[0]\n",
    "    chunk2size = chunk2.shape[0]\n",
    "    labels_chunk0 = load_array(Files[0].replace('trainX','labels'))\n",
    "    labels_chunk1 = load_array(Files[1].replace('trainX','labels'))\n",
    "    labels_chunk2 = load_array(Files[2].replace('trainX','labels'))\n",
    "    listing_ids_chunk0 = load_array(Files[0].replace('trainX','listing_ids'))\n",
    "    listing_ids_chunk1 = load_array(Files[1].replace('trainX','listing_ids'))\n",
    "    listing_ids_chunk2 = load_array(Files[2].replace('trainX','listing_ids'))\n",
    "    features = np.vstack((chunk0,chunk1,chunk2))\n",
    "    del(chunk0)\n",
    "    del(chunk1)\n",
    "    del(chunk2)\n",
    "    gc.collect()\n",
    "    labels = np.vstack((labels_chunk0,labels_chunk1,labels_chunk2))\n",
    "    del(labels_chunk0)\n",
    "    del(labels_chunk1)\n",
    "    del(labels_chunk2)\n",
    "    gc.collect()\n",
    "    listing_ids = np.concatenate((listing_ids_chunk0,listing_ids_chunk1,listing_ids_chunk2))\n",
    "    del(listing_ids_chunk0)\n",
    "    del(listing_ids_chunk1)\n",
    "    del(listing_ids_chunk2)\n",
    "    gc.collect()\n",
    "\n",
    "    seq = np.random.permutation(range(features.shape[0]))\n",
    "\n",
    "    tempArrayFeatures = np.ndarray([1,512, 14, 14],dtype=np.float32)\n",
    "    tempArrayListingIds = 0\n",
    "    tempArrayLabels = np.ndarray([1,3],dtype=np.float32)\n",
    "\n",
    "    for i in range(features.shape[0]):\n",
    "        tempArrayFeatures[0] = features[seq[i]]\n",
    "        features[seq[i]] = features[i]\n",
    "        features[i] = tempArrayFeatures[0]\n",
    "        tempArrayListingIds = listing_ids[seq[i]]\n",
    "        listing_ids[seq[i]] = listing_ids[i]\n",
    "        listing_ids[i] = tempArrayListingIds\n",
    "        tempArrayLabels[0] = labels[seq[i]]\n",
    "        labels[seq[i]] = labels[i]\n",
    "        labels[i] = tempArrayLabels[0]\n",
    "\n",
    "    save_array(Files[0],features[range(chunk0size)])\n",
    "    save_array(Files[1],features[range(chunk0size,chunk0size+chunk1size)])\n",
    "    save_array(Files[2],features[range(chunk0size+chunk1size,features.shape[0])])\n",
    "    save_array(Files[0].replace('trainX','labels'),labels[range(chunk0size)])\n",
    "    save_array(Files[1].replace('trainX','labels'),labels[range(chunk0size,chunk0size+chunk1size)])\n",
    "    save_array(Files[2].replace('trainX','labels'),labels[range(chunk0size+chunk1size,features.shape[0])])\n",
    "    save_array(Files[0].replace('trainX','listing_ids'),listing_ids[range(chunk0size)])\n",
    "    save_array(Files[1].replace('trainX','listing_ids'),listing_ids[range(chunk0size,chunk0size+chunk1size)])\n",
    "    save_array(Files[2].replace('trainX','listing_ids'),listing_ids[range(chunk0size+chunk1size,features.shape[0])])\n",
    "    os.chdir('/cp/home/ubuntu/data')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkLabels(folder):    \n",
    "    os.chdir(folder)\n",
    "    chunks=glob(\"fold_*_labels_*.dat\")\n",
    "    gc.collect()\n",
    "    for chunk in chunks:\n",
    "        labels = load_array(chunk)\n",
    "        print(chunk,np.sum(labels[:,0]),np.sum(labels[:,1]),np.sum(labels[:,2]))\n",
    "    os.chdir('/cp/home/ubuntu/data')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 20000.0, 0.0)\n",
      "(0.0, 1098.0, 12921.0)\n",
      "(0.0, 1428.0, 12412.0)\n",
      "(0.0, 1076.0, 12578.0)\n",
      "(4459.0, 15541.0, 0.0)\n",
      "(0.0, 1282.0, 12909.0)\n",
      "(0.0, 20000.0, 0.0)\n",
      "(0.0, 1111.0, 13174.0)\n",
      "(0.0, 20000.0, 0.0)\n",
      "(4304.0, 15696.0, 0.0)\n",
      "(0.0, 20000.0, 0.0)\n",
      "(4358.0, 15642.0, 0.0)\n",
      "(4152.0, 15848.0, 0.0)\n",
      "(0.0, 20000.0, 0.0)\n",
      "(4376.0, 15624.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "checkLabels('/cp/home/ubuntu/data/features-aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomizeFeatures('fold_0','/cp/home/ubuntu/data/features-aug')\n",
    "randomizeFeatures('fold_1','/cp/home/ubuntu/data/features-aug')\n",
    "randomizeFeatures('fold_2','/cp/home/ubuntu/data/features-aug')\n",
    "randomizeFeatures('fold_3','/cp/home/ubuntu/data/features-aug')\n",
    "randomizeFeatures('fold_4','/cp/home/ubuntu/data/features-aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fold_2_labels_1.dat', 1503.0, 13037.0, 5460.0)\n",
      "('fold_4_labels_2.dat', 1150.0, 9249.0, 3620.0)\n",
      "('fold_1_labels_2.dat', 1453.0, 8807.0, 3580.0)\n",
      "('fold_0_labels_2.dat', 981.0, 7311.0, 5362.0)\n",
      "('fold_0_labels_0.dat', 1328.0, 13337.0, 5335.0)\n",
      "('fold_3_labels_2.dat', 1192.0, 9329.0, 3670.0)\n",
      "('fold_3_labels_1.dat', 741.0, 14978.0, 4281.0)\n",
      "('fold_2_labels_2.dat', 441.0, 7969.0, 5875.0)\n",
      "('fold_0_labels_1.dat', 2150.0, 15969.0, 1881.0)\n",
      "('fold_2_labels_0.dat', 2360.0, 15801.0, 1839.0)\n",
      "('fold_1_labels_1.dat', 1067.0, 14188.0, 4745.0)\n",
      "('fold_3_labels_0.dat', 2425.0, 12617.0, 4958.0)\n",
      "('fold_1_labels_0.dat', 1632.0, 14281.0, 4087.0)\n",
      "('fold_4_labels_1.dat', 792.0, 14920.0, 4288.0)\n",
      "('fold_4_labels_0.dat', 2434.0, 12553.0, 5013.0)\n"
     ]
    }
   ],
   "source": [
    "checkLabels('/cp/home/ubuntu/data/features-aug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dense layer network with batch normalization to reduce the time it takes to train. Also include a drop out of 0.4 to improve the validation accuracy. This network takes the pre-processed features from the saved numpy arrays and outputs 3 probabilities - interest level will be low, medium or high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        BatchNormalization(axis=1),\n",
    "        Dropout(p/4),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(3, activation='softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a network on such a large dataset requires us to use the generator functions in keras. These functions loop \n",
    "indefinitely and return to the yield statement the next time the function is called. This allows us to iterate through the \n",
    "image features one batch at a time till all of them are passed through the network. Once a numpy array is completed, the next \n",
    "one is loaded from memory so that at any given time, a maximum of 20k images are in memory.\n",
    "\n",
    "The metrics such as validation categorical cross-entropy loss and accuracy are logged to a log file so that even if \n",
    "we lose the connection midway, we can review how the metrics changed across the epochs\n",
    "\n",
    "Finally, we keep saving the best model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainMyNw(nb_epoch,fold,folder):\n",
    "    \n",
    "    myGen=image.ImageDataGenerator()\n",
    "    best_loss_metric = 999\n",
    "\n",
    "    for e in tqdm(range(nb_epoch)):\n",
    "        \n",
    "        start = time.clock()\n",
    "        f = open('/cp/home/ubuntu/data/results/log.txt', 'a')       \n",
    "        featureFiles = glob(folder + '/' + 'fold_*_trainX_*.dat')        \n",
    "        chunkCount=0\n",
    "        \n",
    "        for files in featureFiles:\n",
    "            if files.find(fold) == -1:\n",
    "                features = load_array(files)\n",
    "                labels = load_array(files.replace('trainX','labels'))\n",
    "                chunkCount += 1\n",
    "                numImages = features.shape[0]\n",
    "                bn_model.fit_generator(generator=myGen.flow(features,labels),samples_per_epoch = numImages,nb_epoch=1,verbose=0)  \n",
    "                del(features)\n",
    "                del(labels)\n",
    "                gc.collect()\n",
    "            \n",
    "        featureFiles = glob(folder + '/' + fold + '_trainX_*.dat')        \n",
    "        accuracy = 0\n",
    "        loss = 0\n",
    "        avg_accuracy = 0\n",
    "        avg_loss = 0\n",
    "        chunkCount = 0\n",
    "        totalImages = 0\n",
    "\n",
    "        for files in featureFiles:\n",
    "            features = load_array(files)\n",
    "            labels = load_array(files.replace('trainX','labels'))\n",
    "            chunkCount += 1\n",
    "            numImages = features.shape[0]\n",
    "            loss_metrics = bn_model.evaluate_generator(generator=myGen.flow(features,labels),\n",
    "                                                       val_samples = numImages) \n",
    "            accuracy += (loss_metrics[1]*numImages)\n",
    "            loss += (loss_metrics[0]*numImages)\n",
    "            totalImages += numImages\n",
    "            del(features)\n",
    "            del(labels) \n",
    "            gc.collect()            \n",
    "            \n",
    "        if chunkCount>0: \n",
    "            avg_accuracy = accuracy/totalImages\n",
    "            avg_loss = loss/totalImages\n",
    "            \n",
    "        print bn_model.metrics_names,avg_loss,avg_accuracy, \"time :\", time.clock() - start\n",
    "        f.write(str(bn_model.metrics_names))\n",
    "        f.write(\" : \")\n",
    "        f.write(str(avg_loss))\n",
    "        f.write(\" : \")\n",
    "        f.write(str(avg_accuracy))\n",
    "        f.write(\" : time :\")\n",
    "        f.write(str(time.clock() - start))\n",
    "        f.write(\"\\n\")        \n",
    "        f.close() \n",
    "        if (avg_loss<best_loss_metric):\n",
    "            bn_model.save_weights('/cp/home/ubuntu/data/results/best.hdf5') \n",
    "            best_loss_metric = avg_loss\n",
    "            \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the network with a high learning rate once and then gradually decrease the learning rate as we go through about 12 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dropout of 0.2 is optimal for training on train dataset while a dropout of 0.4 suits the valid dataset. Train the network twice, once on the training and once on the validation dataset and save the versions with the lowest loss for later use for stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save best model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "p=0.9\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "trainMyNw(1,'fold_0','features-aug')\n",
    "bn_model.optimizer.lr=0.001\n",
    "trainMyNw(1,'fold_0','features-aug')\n",
    "os.rename('/cp/home/ubuntu/data/results/best.hdf5', '/cp/home/ubuntu/data/results/fold_0_XXXXn.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.9\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "trainMyNw(1,'fold_1','features-aug')\n",
    "bn_model.optimizer.lr=0.001\n",
    "trainMyNw(1,'fold_1','features-aug')\n",
    "os.rename('/cp/home/ubuntu/data/results/best.hdf5', '/cp/home/ubuntu/data/results/fold_1_XXXXn.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.9\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "trainMyNw(1,'fold_2','features-aug')\n",
    "bn_model.optimizer.lr=0.001\n",
    "trainMyNw(1,'fold_2','features-aug')\n",
    "os.rename('/cp/home/ubuntu/data/results/best.hdf5', '/cp/home/ubuntu/data/results/fold_2_XXXXn.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.9\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "trainMyNw(1,'fold_3','features-aug')\n",
    "bn_model.optimizer.lr=0.001\n",
    "trainMyNw(1,'fold_3','features-aug')\n",
    "os.rename('/cp/home/ubuntu/data/results/best.hdf5', '/cp/home/ubuntu/data/results/fold_3_XXXXn.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.9\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "trainMyNw(1,'fold_4','features-aug')\n",
    "bn_model.optimizer.lr=0.001\n",
    "trainMyNw(1,'fold_4','features-aug')\n",
    "os.rename('/cp/home/ubuntu/data/results/best.hdf5', '/cp/home/ubuntu/data/results/fold_4_XXXXn.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train final model on entire dataset. Manually set the number of epochs based on analysis of training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p=0.9\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "trainMyNw(1,'sample','features-aug')\n",
    "bn_model.optimizer.lr=0.001\n",
    "trainMyNw(1,'sample','features-aug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights('/cp/home/ubuntu/data/results/fold_all_XXXXn.hdf5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder with photos for entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate features for final scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate features for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen=image.ImageDataGenerator()\n",
    "generateFeatures('test',CHUNK_SIZE=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate features for all folds without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen=image.ImageDataGenerator()\n",
    "generateFeatures('fold_0',CHUNK_SIZE=20000)\n",
    "generateFeatures('fold_1',CHUNK_SIZE=20000)\n",
    "generateFeatures('fold_2',CHUNK_SIZE=20000)\n",
    "generateFeatures('fold_3',CHUNK_SIZE=20000)\n",
    "generateFeatures('fold_4',CHUNK_SIZE=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "randomizeFeatures('fold_0','/cp/home/ubuntu/data/features')\n",
    "randomizeFeatures('fold_1','/cp/home/ubuntu/data/features')\n",
    "randomizeFeatures('fold_2','/cp/home/ubuntu/data/features')\n",
    "randomizeFeatures('fold_3','/cp/home/ubuntu/data/features')\n",
    "randomizeFeatures('fold_4','/cp/home/ubuntu/data/features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkLabels('/cp/home/ubuntu/data/features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scoreMyData(fold,weights_path):\n",
    "    gen=image.ImageDataGenerator()\n",
    "    bn_model.load_weights(weights_path) \n",
    "    featureFiles = glob('features/' + fold + '_trainX_*.dat')        \n",
    "\n",
    "    for files in featureFiles:\n",
    "        features = load_array(files)\n",
    "        numImages = features.shape[0]\n",
    "        scores = np.ndarray([numImages,3],dtype=np.float64)\n",
    "        scores = bn_model.predict_generator(generator=gen.flow(features),val_samples = numImages)       \n",
    "        save_array('predictions/' + files.split('/')[1].replace('trainX','scores'),scores)               \n",
    "        del(scores)\n",
    "        del(features)\n",
    "        gc.collect()            \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoreMyData('sample','/cp/home/ubuntu/data/results/fold_0_XXXXn.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Score all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoreMyData('fold_0','/cp/home/ubuntu/data/results/fold_0_XXXXn.hdf5')\n",
    "scoreMyData('fold_1','/cp/home/ubuntu/data/results/fold_1_XXXXn.hdf5')\n",
    "scoreMyData('fold_2','/cp/home/ubuntu/data/results/fold_2_XXXXn.hdf5')\n",
    "scoreMyData('fold_3','/cp/home/ubuntu/data/results/fold_3_XXXXn.hdf5')\n",
    "scoreMyData('fold_4','/cp/home/ubuntu/data/results/fold_4_XXXXn.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Score test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoreMyData('test','/cp/home/ubuntu/data/results/fold_all_XXXXn.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scoreMyData('test','/cp/home/ubuntu/data/results/fold_all_XXXXnE.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create final Predictions file extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cp/home/ubuntu/data/predictions\n"
     ]
    }
   ],
   "source": [
    "%cd '/cp/home/ubuntu/data/predictions'\n",
    "scoreFiles = glob('*scores*.dat') \n",
    "listing_ids_all = load_array('/cp/home/ubuntu/data/features/' + scoreFiles[0].replace('scores','listing_ids'))\n",
    "scores_all = load_array(scoreFiles[0])\n",
    "\n",
    "for i in range(2,len(scoreFiles)):\n",
    "    files =  scoreFiles[i]\n",
    "    listing_ids = load_array('/cp/home/ubuntu/data/features/' + files.replace('scores','listing_ids'))\n",
    "    scores = load_array(files)\n",
    "    listing_ids_all = np.concatenate((listing_ids_all, listing_ids))\n",
    "    scores_all = np.vstack((scores_all, scores))\n",
    "\n",
    "listing_ids_all.shape,scores_all.shape  \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(listing_ids_all)\n",
    "df.to_csv(\"listing_ids_all.csv\")\n",
    "\n",
    "df = pd.DataFrame(scores_all)\n",
    "df.to_csv(\"scores_all.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all Folks !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features1 = load_array('/cp/home/ubuntu/data/backup/fold_2_trainX_2.dat')\n",
    "labels1 = load_array('/cp/home/ubuntu/data/backup/fold_2_labels_2.dat')\n",
    "features2 = load_array('/cp/home/ubuntu/data/backup/fold_3_trainX_0.dat')\n",
    "labels2 = load_array('/cp/home/ubuntu/data/backup/fold_3_labels_0.dat')\n",
    "\n",
    "features = np.vstack((features1,features2))\n",
    "labels = np.vstack((labels1,labels2))\n",
    "\n",
    "del(features1)\n",
    "del(features2)\n",
    "del(labels1)\n",
    "del(labels2)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features1 = load_array('/cp/home/ubuntu/data/backup/fold_3_trainX_2.dat')\n",
    "labels1 = load_array('/cp/home/ubuntu/data/backup/fold_3_labels_2.dat')\n",
    "features2 = load_array('/cp/home/ubuntu/data/backup/fold_4_trainX_0.dat')\n",
    "labels2 = load_array('/cp/home/ubuntu/data/backup/fold_4_labels_0.dat')\n",
    "\n",
    "features_val = np.vstack((features1,features2))\n",
    "labels_val = np.vstack((labels1,labels2))\n",
    "\n",
    "del(features1)\n",
    "del(features2)\n",
    "del(labels1)\n",
    "del(labels2)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34285, 512, 14, 14), (34191, 512, 14, 14), (34285, 3), (34191, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape,features_val.shape,labels.shape,labels_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.2\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/5\n",
      "34285/34285 [==============================] - 35s - loss: 1.2586 - acc: 0.4036 - val_loss: 1.0881 - val_acc: 0.4511\n",
      "Epoch 2/5\n",
      "34285/34285 [==============================] - 35s - loss: 0.8116 - acc: 0.6285 - val_loss: 1.1434 - val_acc: 0.4557\n",
      "Epoch 3/5\n",
      "34285/34285 [==============================] - 35s - loss: 0.5783 - acc: 0.7634 - val_loss: 1.2395 - val_acc: 0.4393\n",
      "Epoch 4/5\n",
      "34285/34285 [==============================] - 35s - loss: 0.3923 - acc: 0.8544 - val_loss: 1.4213 - val_acc: 0.4406\n",
      "Epoch 5/5\n",
      "34285/34285 [==============================] - 35s - loss: 0.2707 - acc: 0.9028 - val_loss: 1.5513 - val_acc: 0.4352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb28cee9590>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=5,validation_data=(features_val, labels_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.5\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/5\n",
      "34285/34285 [==============================] - 35s - loss: 1.3512 - acc: 0.3978 - val_loss: 1.0769 - val_acc: 0.4746\n",
      "Epoch 2/5\n",
      "34285/34285 [==============================] - 35s - loss: 1.0894 - acc: 0.4825 - val_loss: 1.0597 - val_acc: 0.4758\n",
      "Epoch 3/5\n",
      "34285/34285 [==============================] - 35s - loss: 0.9715 - acc: 0.5382 - val_loss: 1.0775 - val_acc: 0.4721\n",
      "Epoch 4/5\n",
      "34285/34285 [==============================] - 35s - loss: 0.8729 - acc: 0.5968 - val_loss: 1.1066 - val_acc: 0.4663\n",
      "Epoch 5/5\n",
      "34285/34285 [==============================] - 36s - loss: 0.7713 - acc: 0.6519 - val_loss: 1.1655 - val_acc: 0.4613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2827e2a10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=5,validation_data=(features_val, labels_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/10\n",
      "34285/34285 [==============================] - 38s - loss: 1.6478 - acc: 0.3414 - val_loss: 1.2229 - val_acc: 0.3798\n",
      "Epoch 2/10\n",
      "34285/34285 [==============================] - 37s - loss: 1.4212 - acc: 0.3881 - val_loss: 1.1560 - val_acc: 0.4120\n",
      "Epoch 3/10\n",
      "34285/34285 [==============================] - 38s - loss: 1.2844 - acc: 0.4212 - val_loss: 1.1244 - val_acc: 0.4324\n",
      "Epoch 4/10\n",
      "34285/34285 [==============================] - 38s - loss: 1.1798 - acc: 0.4660 - val_loss: 1.1036 - val_acc: 0.4475\n",
      "Epoch 5/10\n",
      "34285/34285 [==============================] - 38s - loss: 1.1180 - acc: 0.4881 - val_loss: 1.0920 - val_acc: 0.4526\n",
      "Epoch 6/10\n",
      "34285/34285 [==============================] - 38s - loss: 1.0726 - acc: 0.5082 - val_loss: 1.0859 - val_acc: 0.4567\n",
      "Epoch 7/10\n",
      "34285/34285 [==============================] - 38s - loss: 1.0293 - acc: 0.5279 - val_loss: 1.0887 - val_acc: 0.4592\n",
      "Epoch 8/10\n",
      "34285/34285 [==============================] - 37s - loss: 0.9787 - acc: 0.5530 - val_loss: 1.0966 - val_acc: 0.4625\n",
      "Epoch 9/10\n",
      "34285/34285 [==============================] - 38s - loss: 0.9523 - acc: 0.5677 - val_loss: 1.1049 - val_acc: 0.4612\n",
      "Epoch 10/10\n",
      "34285/34285 [==============================] - 38s - loss: 0.9133 - acc: 0.5856 - val_loss: 1.1101 - val_acc: 0.4601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2784e0410>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0.5\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=10,validation_data=(features_val, labels_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/5\n",
      "34285/34285 [==============================] - 37s - loss: 1.1336 - acc: 0.4375 - val_loss: 0.9996 - val_acc: 0.4619\n",
      "Epoch 2/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.9857 - acc: 0.4908 - val_loss: 1.0133 - val_acc: 0.4622\n",
      "Epoch 3/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.9182 - acc: 0.5436 - val_loss: 1.0677 - val_acc: 0.4368\n",
      "Epoch 4/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.8018 - acc: 0.6241 - val_loss: 1.1420 - val_acc: 0.4427\n",
      "Epoch 5/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.6687 - acc: 0.7027 - val_loss: 1.2121 - val_acc: 0.4456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb27ad8e0d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0.5\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=5,validation_data=(features_val, labels_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/5\n",
      "34285/34285 [==============================] - 38s - loss: 1.1263 - acc: 0.4453 - val_loss: 1.0356 - val_acc: 0.4365\n",
      "Epoch 2/5\n",
      "34285/34285 [==============================] - 38s - loss: 1.0014 - acc: 0.4743 - val_loss: 1.0264 - val_acc: 0.4690\n",
      "Epoch 3/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.9480 - acc: 0.5198 - val_loss: 1.0167 - val_acc: 0.4585\n",
      "Epoch 4/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.8927 - acc: 0.5627 - val_loss: 1.0601 - val_acc: 0.4641\n",
      "Epoch 5/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.8191 - acc: 0.6144 - val_loss: 1.0887 - val_acc: 0.4453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb275848950>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0.5\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=5,validation_data=(features_val, labels_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/5\n",
      "34285/34285 [==============================] - 38s - loss: 1.1241 - acc: 0.4491 - val_loss: 0.9942 - val_acc: 0.4857\n",
      "Epoch 2/5\n",
      "34285/34285 [==============================] - 39s - loss: 0.9839 - acc: 0.4798 - val_loss: 0.9933 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "34285/34285 [==============================] - 39s - loss: 0.9652 - acc: 0.5009 - val_loss: 0.9978 - val_acc: 0.4682\n",
      "Epoch 4/5\n",
      "34285/34285 [==============================] - 39s - loss: 0.9444 - acc: 0.5224 - val_loss: 1.0237 - val_acc: 0.4795\n",
      "Epoch 5/5\n",
      "34285/34285 [==============================] - 39s - loss: 0.9197 - acc: 0.5425 - val_loss: 1.0683 - val_acc: 0.4532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb27339e1d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0.5\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=5,validation_data=(features_val, labels_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/5\n",
      "34285/34285 [==============================] - 39s - loss: 1.1307 - acc: 0.4367 - val_loss: 1.0019 - val_acc: 0.4667\n",
      "Epoch 2/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.9970 - acc: 0.4808 - val_loss: 1.0224 - val_acc: 0.4678\n",
      "Epoch 3/5\n",
      "34285/34285 [==============================] - 39s - loss: 0.9492 - acc: 0.5229 - val_loss: 1.0390 - val_acc: 0.4644\n",
      "Epoch 4/5\n",
      "34285/34285 [==============================] - 40s - loss: 0.8755 - acc: 0.5709 - val_loss: 1.0864 - val_acc: 0.4644\n",
      "Epoch 5/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.7756 - acc: 0.6411 - val_loss: 1.1546 - val_acc: 0.4361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2720c2ed0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0.6\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=5,validation_data=(features_val, labels_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/5\n",
      "34285/34285 [==============================] - 39s - loss: 1.1506 - acc: 0.4445 - val_loss: 0.9887 - val_acc: 0.4944\n",
      "Epoch 2/5\n",
      "34285/34285 [==============================] - 39s - loss: 0.9860 - acc: 0.4784 - val_loss: 0.9870 - val_acc: 0.4942\n",
      "Epoch 3/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.9779 - acc: 0.4870 - val_loss: 0.9966 - val_acc: 0.4621\n",
      "Epoch 4/5\n",
      "34285/34285 [==============================] - 39s - loss: 0.9752 - acc: 0.4909 - val_loss: 0.9944 - val_acc: 0.4895\n",
      "Epoch 5/5\n",
      "34285/34285 [==============================] - 40s - loss: 0.9756 - acc: 0.4947 - val_loss: 1.0008 - val_acc: 0.4808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb26e022410>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0.7\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=5,validation_data=(features_val, labels_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/1\n",
      "34285/34285 [==============================] - 39s - loss: 1.1373 - acc: 0.4461 - val_loss: 1.0036 - val_acc: 0.4945\n",
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/1\n",
      "34285/34285 [==============================] - 38s - loss: 0.9870 - acc: 0.4788 - val_loss: 0.9851 - val_acc: 0.4879\n",
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/3\n",
      "34285/34285 [==============================] - 39s - loss: 0.9787 - acc: 0.4857 - val_loss: 0.9991 - val_acc: 0.4275\n",
      "Epoch 2/3\n",
      "34285/34285 [==============================] - 38s - loss: 0.9740 - acc: 0.4908 - val_loss: 1.0190 - val_acc: 0.4870\n",
      "Epoch 3/3\n",
      "34285/34285 [==============================] - 40s - loss: 0.9761 - acc: 0.4958 - val_loss: 1.0172 - val_acc: 0.4784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb26b998f50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0.7\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=1,validation_data=(features_val, labels_val),verbose=1)\n",
    "bn_model.optimizer.lr = 0.001\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=1,validation_data=(features_val, labels_val),verbose=1)\n",
    "bn_model.optimizer.lr = 0.00001\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=3,validation_data=(features_val, labels_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/1\n",
      "34285/34285 [==============================] - 39s - loss: 1.1497 - acc: 0.4456 - val_loss: 0.9969 - val_acc: 0.4871\n",
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/3\n",
      "34285/34285 [==============================] - 39s - loss: 0.9859 - acc: 0.4784 - val_loss: 0.9833 - val_acc: 0.4898\n",
      "Epoch 2/3\n",
      "34285/34285 [==============================] - 39s - loss: 0.9775 - acc: 0.4873 - val_loss: 0.9856 - val_acc: 0.4934\n",
      "Epoch 3/3\n",
      "34285/34285 [==============================] - 39s - loss: 0.9670 - acc: 0.5015 - val_loss: 1.0088 - val_acc: 0.4462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb26b45f290>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0.7\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=1,validation_data=(features_val, labels_val),verbose=1)\n",
    "bn_model.optimizer.lr = 0.00001\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=3,validation_data=(features_val, labels_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/5\n",
      "34285/34285 [==============================] - 38s - loss: 1.4684 - acc: 0.3474 - val_loss: 1.3273 - val_acc: 0.3513\n",
      "Epoch 2/5\n",
      "34285/34285 [==============================] - 38s - loss: 1.0632 - acc: 0.4852 - val_loss: 1.2647 - val_acc: 0.3787\n",
      "Epoch 3/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.8376 - acc: 0.6158 - val_loss: 1.2392 - val_acc: 0.4015\n",
      "Epoch 4/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.6795 - acc: 0.7191 - val_loss: 1.2428 - val_acc: 0.4116\n",
      "Epoch 5/5\n",
      "34285/34285 [==============================] - 38s - loss: 0.5465 - acc: 0.7958 - val_loss: 1.2675 - val_acc: 0.4203\n",
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/1\n",
      "34285/34285 [==============================] - 39s - loss: 0.4460 - acc: 0.8484 - val_loss: 1.2969 - val_acc: 0.4199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2689f2610>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0.1\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=5,validation_data=(features_val, labels_val),verbose=1)\n",
    "bn_model.optimizer.lr = 0.001\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=1,validation_data=(features_val, labels_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/1\n",
      "34285/34285 [==============================] - 39s - loss: 1.1670 - acc: 0.4458 - val_loss: 0.9929 - val_acc: 0.4849\n",
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/2\n",
      "34285/34285 [==============================] - 39s - loss: 0.9887 - acc: 0.4749 - val_loss: 0.9796 - val_acc: 0.4945\n",
      "Epoch 2/2\n",
      "34285/34285 [==============================] - 39s - loss: 0.9862 - acc: 0.4786 - val_loss: 0.9877 - val_acc: 0.4923\n",
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/3\n",
      "34285/34285 [==============================] - 39s - loss: 0.9967 - acc: 0.4677 - val_loss: 0.9964 - val_acc: 0.4935\n",
      "Epoch 2/3\n",
      "34285/34285 [==============================] - 40s - loss: 1.0036 - acc: 0.4626 - val_loss: 0.9880 - val_acc: 0.4872\n",
      "Epoch 3/3\n",
      "34285/34285 [==============================] - 39s - loss: 1.0037 - acc: 0.4626 - val_loss: 1.0039 - val_acc: 0.4867\n",
      "Train on 34285 samples, validate on 34191 samples\n",
      "Epoch 1/3\n",
      "34285/34285 [==============================] - 38s - loss: 1.0040 - acc: 0.4632 - val_loss: 0.9811 - val_acc: 0.4940\n",
      "Epoch 2/3\n",
      "34285/34285 [==============================] - 39s - loss: 1.0020 - acc: 0.4668 - val_loss: 0.9994 - val_acc: 0.3962\n",
      "Epoch 3/3\n",
      "34285/34285 [==============================] - 41s - loss: 1.0047 - acc: 0.4634 - val_loss: 0.9880 - val_acc: 0.4940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb26628af50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0.9\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=1,validation_data=(features_val, labels_val),verbose=1)\n",
    "bn_model.optimizer.lr=0.001\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=2,validation_data=(features_val, labels_val),verbose=1)\n",
    "bn_model.optimizer.lr=0.0001\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=3,validation_data=(features_val, labels_val),verbose=1)\n",
    "bn_model.optimizer.lr=0.00001\n",
    "bn_model.fit(features, labels, batch_size=32, shuffle=True ,nb_epoch=3,validation_data=(features_val, labels_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "print inspect.getsource(bn_model.evaluate_generator)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
