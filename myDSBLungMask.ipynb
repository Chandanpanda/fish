{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use pretrained unet network - DONE\n",
    "#Add information about slice - TODAY\n",
    "#Are all lungs facing the same way = TOMMOROW?\n",
    "#Add information about nodule - TOMMOROW\n",
    "#Lung mask as per simple thresholding? Maybe No. But do measure jaccard/dice for thresholding Vs deep learning\n",
    "#Add information about patient\n",
    "#Try 3D Unet\n",
    "#Process data to have 2 classes - lung, Nodule\n",
    "#Use Jacckard index instead of dice coefficient\n",
    "#initialise using tutorial weights\n",
    "#preprocess to make 0 or 1 in training data\n",
    "#train on segmented lungs data using unet \n",
    "#test on validation set\n",
    "#Expand data from 160 to zoom on lung\n",
    "#does every CT scan have patient oriented in same way? TO BE VERIFIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from random import uniform\n",
    "import bcolz\n",
    "import time\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers import Input, Dense\n",
    "import math\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils as u\n",
    "\n",
    "numGPUs = 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/luna/home/ubuntu/data/masks\n",
      "/dsb/home/ubuntu/data/lungs\n",
      "/luna/home/ubuntu/data/lungs\n",
      "/luna/home/ubuntu/data\n"
     ]
    }
   ],
   "source": [
    "%cd '/luna/home/ubuntu/data/masks'\n",
    "lung_mask_files=glob(\"*.mhd\")\n",
    "%cd '/dsb/home/ubuntu/data/lungs'\n",
    "lung_files=glob(\"*.mhd\")\n",
    "%cd '/luna/home/ubuntu/data/lungs'\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_itk_image(filename):\n",
    "    itkimage = sitk.ReadImage(filename)\n",
    "    numpyImage = sitk.GetArrayFromImage(itkimage)\n",
    "    numpyOrigin = np.array(list(reversed(itkimage.GetOrigin())))\n",
    "    numpySpacing = np.array(list(reversed(itkimage.GetSpacing())))\n",
    "    return numpyImage, numpyOrigin, numpySpacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dsb/home/ubuntu/data/lungs\n",
      "/luna/home/ubuntu/data/lungs\n",
      "/dsb/home/ubuntu/data/lungs-valid\n",
      "/luna/home/ubuntu/data/lungs\n",
      "/luna/home/ubuntu/data\n"
     ]
    }
   ],
   "source": [
    "%cd '/dsb/home/ubuntu/data/lungs'\n",
    "lungs = glob('*.mhd')\n",
    "num_lungs = len(lungs)\n",
    "%cd '/luna/home/ubuntu/data/lungs'\n",
    "%cd '/dsb/home/ubuntu/data/lungs-valid'\n",
    "lungs_valid = glob('*.mhd')\n",
    "num_lungs_valid = len(lungs_valid)\n",
    "%cd '/luna/home/ubuntu/data/lungs'\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load data\n",
    "\n",
    "## load test lung\n",
    "img_path = \"/dsb/home/ubuntu/data/lungs/1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860.mhd\"\n",
    "numpyImage, numpyOrigin, numpySpacing = load_itk_image(img_path)\n",
    "print numpyImage.shape\n",
    "print numpyOrigin\n",
    "print numpySpacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(numpyImage[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## load test Mask\n",
    "img_path = \"masks/1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860.mhd\"\n",
    "numpyImage, numpyOrigin, numpySpacing = load_itk_image(img_path)\n",
    "print numpyImage.shape\n",
    "print numpyOrigin\n",
    "print numpySpacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(numpyImage[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = np.ndarray([512,512],dtype=np.int8)\n",
    "mask[:] = 0\n",
    "mask = numpyImage>0\n",
    "plt.imshow(mask[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_np(y_true,y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smooth_j = 1e-12\n",
    "\n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    # __author__ = Vladimir Iglovikov\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth_j) / (sum_ - intersection + smooth_j)\n",
    "    return K.mean(jac)\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    # __author__ = Vladimir Iglovikov\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth_j) / (sum_ - intersection + smooth_j)\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_loss(y_true, y_pred):\n",
    "    return -jaccard_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create categorical slice numbers\n",
    "rawFiles = glob(\"/luna/home/ubuntu/data/temp/final_slice_indices*.dat\")\n",
    "max_slice_global = 0\n",
    "for filename in rawFiles:\n",
    "    f=load_array(filename)\n",
    "    length = f.shape[0]\n",
    "    max_slice = np.max(f[range(length),])\n",
    "    if max_slice_global < max_slice: max_slice_global = max_slice\n",
    "#Repeat for validation set\n",
    "rawFiles = glob(\"/dsb/home/ubuntu/data/temp/final_slice_indices*.dat\")\n",
    "for filename in rawFiles:\n",
    "    f=load_array(filename)\n",
    "    length = f.shape[0]\n",
    "    max_slice = np.max(f[range(length),])\n",
    "    if max_slice_global < max_slice: max_slice_global = max_slice\n",
    "max_slice_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawFiles = glob(\"/luna/home/ubuntu/data/temp/final_slice_indices*.dat\")\n",
    "for filename in rawFiles:\n",
    "    f=load_array(filename)\n",
    "    length = f.shape[0]\n",
    "    a = u.to_categorical(f[0:length,1],max_slice_global+1)\n",
    "    save_array(filename.replace(\"final_slice_indices\",\"final_slice_indices_cat\"),a)\n",
    "rawFiles = glob(\"/dsb/home/ubuntu/data/temp/final_slice_indices*.dat\")\n",
    "for filename in rawFiles:\n",
    "    f=load_array(filename)\n",
    "    length = f.shape[0]\n",
    "    a = u.to_categorical(f[0:length,1],max_slice_global+1)\n",
    "    save_array(filename.replace(\"final_slice_indices\",\"final_slice_indices_cat\"),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_last_layer(s,sliceIndices):\n",
    "    \n",
    "    inputs = Input(shape=s)\n",
    "    sz_inp = Input(shape=sliceIndices.shape)\n",
    "    x = Dense(256*512, activation='softmax')(sz_inp)\n",
    "    \n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "    conv2 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)   \n",
    "    conv3 = Convolution2D(1,1, 1, activation='sigmoid')(conv2)\n",
    "    conv4 = MaxPooling2D((2,1))(conv3)\n",
    "\n",
    "    conv4f = Flatten()(conv4)\n",
    "    \n",
    "    dense4 = merge([conv4f,x], 'concat')\n",
    "    \n",
    "    dense5 = Reshape((1,512,512)) (dense4)\n",
    "    \n",
    "    model = Model(input=[inputs,sz_inp], output=dense5)    \n",
    "    return model  \n",
    "\n",
    "t = np.ndarray([723],dtype=np.int8)\n",
    "lrg_model = get_last_layer((96,512,512),t)  \n",
    "lrg_model.summary()\n",
    "lrg_model.compile(Adam(lr=0.001), loss=dice_coef_loss, metrics=[jaccard_coef_int,dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 96, 512, 512)  0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 32, 512, 512)  27680       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 32, 512, 512)  9248        convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 1, 512, 512)   33          convolution2d_2[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 36961\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_last_layer(s):\n",
    "    \n",
    "    inputs = Input(shape=s)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "    conv2 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)   \n",
    "    conv3 = Convolution2D(1,1, 1, activation='sigmoid')(conv2)        \n",
    "    model = Model(input=inputs, output=conv3)    \n",
    "    return model  \n",
    "\n",
    "lrg_model = get_last_layer((96,512,512))  \n",
    "lrg_model.summary()\n",
    "lrg_model.compile(Adam(lr=0.001), loss=dice_coef_loss, metrics=[jaccard_coef_int,dice_coef])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches from training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_training_batches(batch_size,data_size):\n",
    "    for i in range(data_size):\n",
    "        featuresFiles = glob('/dsb/home/ubuntu/data/temp/final_images_features_%d.dat'%i)\n",
    "        featuresFiles = np.random.permutation(featuresFiles)\n",
    "        for f in featuresFiles:\n",
    "            features = load_array(f)\n",
    "            masks = load_array(f.replace(\"images_features\",\"images_masks\"))\n",
    "            sliceIndices = load_array(f.replace(\"images_features\",\"slice_indices_cat\"))\n",
    "            numRows = features.shape[0]\n",
    "            count = 0\n",
    "            while (count < numRows) and (count+batch_size<numRows):\n",
    "                x = features[count:(count+batch_size)]\n",
    "                y = masks[count:(count+batch_size)]\n",
    "                z = sliceIndices[count:(count+batch_size)]\n",
    "                count += batch_size\n",
    "                yield (x,y,z)\n",
    "\n",
    "def generate_validation_batches(batch_size,data_size):\n",
    "    for i in range(data_size):\n",
    "        featuresFiles = glob('/luna/home/ubuntu/data/temp/final_images_features_valid_%d.dat'%i)\n",
    "        for f in featuresFiles:\n",
    "            features = load_array(f)\n",
    "            masks = load_array(f.replace(\"images_features\",\"images_masks\"))\n",
    "            sliceIndices = load_array(f.replace(\"images_features\",\"slice_indices_cat\"))\n",
    "            numRows = features.shape[0]\n",
    "            count = 0\n",
    "            while (count < numRows) and (count+batch_size<numRows):\n",
    "                x = features[count:(count+batch_size)]\n",
    "                y = masks[count:(count+batch_size)]\n",
    "                z = sliceIndices[count:(count+batch_size)]\n",
    "                count += batch_size\n",
    "                yield (x,y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainMyNw(nb_epoch):\n",
    "    best_loss_metric = 0\n",
    "    print \"training network using features from:\"\n",
    "    %cd '/dsb/home/ubuntu/data/temp'\n",
    "    for e in range(nb_epoch):\n",
    "        print 'epoch %d:'%e\n",
    "        start = time.clock()\n",
    "        f = open('/luna/home/ubuntu/log.txt', 'a')\n",
    "        for x,y,z in generate_training_batches(4,12):\n",
    "            #lrg_model.fit([x,z], y, batch_size = 4, nb_epoch=1, verbose=0,shuffle=True)\n",
    "            lrg_model.fit(x, y, batch_size = 4, nb_epoch=1, verbose=0,shuffle=True)\n",
    "        for x,y,z in generate_validation_batches(4,12):\n",
    "            #loss_metrics = lrg_model.evaluate([x,z], y, batch_size = 4, verbose=0)\n",
    "            loss_metrics = lrg_model.evaluate(x, y, batch_size = 4, verbose=0)\n",
    "        print lrg_model.metrics_names,loss_metrics, \"time :\", time.clock() - start\n",
    "        f.write(str(lrg_model.metrics_names))\n",
    "        f.write(\" : \")\n",
    "        f.write(str(loss_metrics))\n",
    "        f.write(\" : time :\")\n",
    "        f.write(str(time.clock() - start))\n",
    "        f.write(\"\\n\")        \n",
    "        f.close() \n",
    "        if (loss_metrics[0]<best_loss_metric):\n",
    "            lrg_model.save_weights('/dsb/home/ubuntu/lung_mask_best.hdf5') \n",
    "            best_loss_metric = loss_metrics[0]\n",
    "    return loss_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.optimizer.lr = 1e-2\n",
    "loss_metrics=trainMyNw(2)\n",
    "lrg_model.optimizer.lr = 1e-3\n",
    "loss_metrics=trainMyNw(9)\n",
    "lrg_model.optimizer.lr = 1e-4\n",
    "loss_metrics=trainMyNw(10)        \n",
    "lrg_model.save_weights('/dsb/home/ubuntu/lung_mask.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training network using features from:\n",
      "/dsb/home/ubuntu/data/temp\n",
      "epoch 0:\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8a6a8965f95f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlrg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainMyNw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-7b3a676d3d5f>\u001b[0m in \u001b[0;36mtrainMyNw\u001b[0;34m(nb_epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m#lrg_model.fit([x,z], y, batch_size = 4, nb_epoch=1, verbose=0,shuffle=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mlrg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_validation_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;31m#loss_metrics = lrg_model.evaluate([x,z], y, batch_size = 4, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-4d1783db17cc>\u001b[0m in \u001b[0;36mgenerate_validation_batches\u001b[0;34m(batch_size, data_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeaturesFiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images_features\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"images_masks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0msliceIndices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images_features\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"slice_indices_cat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mnumRows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-facf925e36af>\u001b[0m in \u001b[0;36mload_array\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbcolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mbcolz/carray_ext.pyx\u001b[0m in \u001b[0;36mbcolz.carray_ext.carray.__getitem__ (bcolz/carray_ext.c:25668)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lrg_model.optimizer.lr = 1e-1\n",
    "loss_metrics=trainMyNw(2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
