{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\chapanda\\\\OneDrive - Epsilon\\\\cp\\\\ACG\\\\03_Practise\\\\RnD\\\\crosssell'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import csv\n",
    "from collections import deque\n",
    "from glob import glob\n",
    "import bcolz\n",
    "from sklearn.metrics import log_loss\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True,gpu_options=gpu_options))\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\'\n",
    "#path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "N = 100\n",
    "debug = False\n",
    "batch_size = 128\n",
    "num_training_steps = 1000000\n",
    "learning_rate = 0.0003\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 500000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 50000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = False\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "filter_widths=filter_widths+filter_widths\n",
    "skip_channels=124\n",
    "residual_channels=256\n",
    "\n",
    "prediction_dir = path_out + \"predictions\"\n",
    "checkpoint_dir = path_out + \"checkpoints\"\n",
    "\n",
    "if not os.path.isdir(prediction_dir):\n",
    "    os.makedirs(prediction_dir)\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "def tic():\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "def toc():\n",
    "    global start_time\n",
    "    elapsed_time = (time.time() - start_time)\n",
    "    print(\"took me \" + str(round(elapsed_time, 3))+\" seconds to do this..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(tensor, dim=None):\n",
    "    \"\"\"Get tensor shape/dimension as list/int\"\"\"\n",
    "    if dim is None:\n",
    "        return tensor.shape.as_list()\n",
    "    else:\n",
    "        return tensor.shape.as_list()[dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(object):\n",
    "\n",
    "    def __init__(self, path_out, suffix):\n",
    "        \n",
    "        self.filenames = [\n",
    "            'user_id',\n",
    "            'master_aisle_id',\n",
    "            'master_department_id',\n",
    "            'IsInOrder_history',\n",
    "            'NextInOrder_history',\n",
    "            'NumProductsFromDep_history',\n",
    "            'OrderSize_history',\n",
    "            'IndexInOrder_history',\n",
    "            'order_number_history',\n",
    "            'order_hour_of_day_history',\n",
    "            'order_dow_history',\n",
    "            'days_since_prior_order_history',\n",
    "            'history_length',\n",
    "            #'ProductID1_history_a',\n",
    "            #'ProductID2_history_a',\n",
    "            #'ProductID3_history_a',\n",
    "            #'ProductID4_history_a',\n",
    "            #'ProductID5_history_a',\n",
    "            #'ProductID6_history_a',\n",
    "            #'ProductID7_history_a',\n",
    "            #'ProductID8_history_a',\n",
    "            #'ProductID9_history_a',\n",
    "            #'ProductID10_history_a',\n",
    "            #'ProductID11_history_a',\n",
    "            #'ProductID12_history_a',\n",
    "            #'ProductID13_history_a',\n",
    "            #'ProductID14_history_a',\n",
    "            #'ProductID15_history_a',\n",
    "            #'ProductID16_history_a',\n",
    "            #'ProductID17_history_a',\n",
    "            #'ProductID18_history_a',   \n",
    "            #'ProductID19_history_a',\n",
    "            #'ProductID20_history_a',\n",
    "            'ProductID1_history',\n",
    "            'ProductID2_history',\n",
    "            'ProductID3_history',\n",
    "            'ProductID4_history',\n",
    "            'ProductID5_history',\n",
    "            'ProductID6_history',\n",
    "            'ProductID7_history',\n",
    "            'ProductID8_history',\n",
    "            'ProductID9_history',\n",
    "            'ProductID10_history',\n",
    "            'ProductID11_history',\n",
    "            'ProductID12_history',\n",
    "            'ProductID13_history',\n",
    "            'ProductID14_history',\n",
    "            'ProductID15_history',\n",
    "            'ProductID16_history',\n",
    "            'ProductID17_history',\n",
    "            'ProductID18_history',   \n",
    "            'ProductID19_history',\n",
    "            'ProductID20_history',\n",
    "            #'ProductNameEmbedding_history',\n",
    "            'label'\n",
    "        ]\n",
    "        \n",
    "        self.ids = 0\n",
    "        self.start = 0\n",
    "        self.dataFiles = glob(path_out+'user_id*.dat') \n",
    "        self.num_chunks = len(self.dataFiles)\n",
    "        self.chunks_index = 0\n",
    "        \n",
    "        self.start = 0 \n",
    "        self.arrays = [(bcolz.open(self.dataFiles[0].replace('user_id',i))[:]) for i in self.filenames]\n",
    "        self.ids = np.arange(0,len(self.arrays[0]))\n",
    "\n",
    "    def next_batch(self, batch_size, shuffle=True, is_test=False):\n",
    "        while(1):\n",
    "            if (self.arrays[0].shape[0] - self.start) >= batch_size:\n",
    "                batch_ids = self.ids[self.start: (self.start + batch_size)]\n",
    "                self.start += batch_size\n",
    "                yield {globals()[placeholder] : copy.copy(array[batch_ids]) for placeholder, array in zip(self.filenames,self.arrays)} \n",
    "            else:\n",
    "                self.start = 0 \n",
    "                self.chunks_index += 1\n",
    "                if self.chunks_index >= self.num_chunks:\n",
    "                    self.chunks_index = 0\n",
    "                self.arrays = [(bcolz.open(self.dataFiles[self.chunks_index].replace('user_id',i))[:]) for i in self.filenames]\n",
    "                self.ids = np.arange(0,len(self.arrays[0]))\n",
    "                if shuffle:\n",
    "                    random.shuffle(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset(path_out,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_log_loss(y, y_hat, sequence_lengths, max_sequence_length, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Calculates average log loss on variable length sequences.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor of shape [batch size, max_sequence_length, input units].\n",
    "        y_hat: Prediction tensor, same shape as y.\n",
    "        sequence_lengths: Sequence lengths.  Tensor of shape [batch_size].\n",
    "        max_sequence_length: maximum length of padded sequence tensor.\n",
    "\n",
    "    Returns:\n",
    "        Log loss. 0-dimensional tensor.\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.minimum(tf.maximum(y_hat, eps), 1.0 - eps)\n",
    "    log_losses = y*tf.log(y_hat) + (1.0 - y)*tf.log(1.0 - y_hat)\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n",
    "    avg_log_loss = -tf.reduce_sum(log_losses*sequence_mask) / tf.cast(tf.reduce_sum(sequence_lengths), tf.float32)\n",
    "    return avg_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_convolution_layer(inputs, output_units, convolution_width, causal=False, dilation_rate=[1], bias=True,\n",
    "                               activation=None, dropout=None, scope='temporal-convolution-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Convolution over the temporal axis of sequence data.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, input_units].\n",
    "        output_units: Output channels for convolution.\n",
    "        convolution_width: Number of timesteps to use in convolution.\n",
    "        causal: Output at timestep t is a function of inputs at or before timestep t.\n",
    "        dilation_rate:  Dilation rate along temporal axis.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        if causal:\n",
    "            shift = int((convolution_width / 2) + (int(dilation_rate[0] - 1) / 2)) #FIXED THIS\n",
    "            pad = tf.zeros([tf.shape(inputs)[0], shift, inputs.shape.as_list()[2]])\n",
    "            inputs = tf.concat([pad, inputs], axis=1)\n",
    "\n",
    "        W = tf.get_variable(\n",
    "            name='weights',\n",
    "            initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            shape=[convolution_width, shape(inputs, 2), output_units]\n",
    "        )\n",
    "\n",
    "        z = tf.nn.convolution(inputs, W, padding='SAME', dilation_rate=dilation_rate)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                name='biases',\n",
    "                initializer=tf.constant_initializer(),\n",
    "                shape=[output_units]\n",
    "            )\n",
    "            z = z + b\n",
    "        z = activation(z) if activation else z\n",
    "        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "        z = z[:, :-shift, :] if causal else z\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavenet(x, dilations, filter_widths, skip_channels, residual_channels, scope='wavenet', reuse=False):\n",
    "    \"\"\"\n",
    "    A stack of causal dilated convolutions with paramaterized residual and skip connections as described\n",
    "    in the WaveNet paper (with some minor differences).\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape [batch size, max sequence length, input units].\n",
    "        dilations: List of dilations for each layer.  len(dilations) is the number of layers\n",
    "        filter_widths: List of filter widths.  Same length as dilations.\n",
    "        skip_channels: Number of channels to use for skip connections.\n",
    "        residual_channels: Number of channels to use for residual connections.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, len(dilations)*skip_channels].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        # wavenet uses 2x1 conv here\n",
    "        inputs = time_distributed_dense_layer(x, residual_channels, activation=tf.nn.tanh, scope='x-proj')\n",
    "\n",
    "        skip_outputs = []\n",
    "        for i, (dilation, filter_width) in enumerate(zip(dilations, filter_widths)):\n",
    "            dilated_conv = temporal_convolution_layer(\n",
    "                inputs=inputs,\n",
    "                output_units=2*residual_channels,\n",
    "                convolution_width=filter_width,\n",
    "                causal=True,# CHECK THIS OUT\n",
    "                dilation_rate=[dilation],\n",
    "                scope='cnn-{}'.format(i) \n",
    "            )\n",
    "            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "            output_units = skip_channels + residual_channels\n",
    "            outputs = time_distributed_dense_layer(dilated_conv, output_units, scope='cnn-{}-proj'.format(i))\n",
    "            skips, residuals = tf.split(outputs, [skip_channels, residual_channels], axis=2)\n",
    "\n",
    "            inputs += residuals\n",
    "            skip_outputs.append(skips)\n",
    "\n",
    "        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "        return skip_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_layer(inputs, lengths, state_size, keep_prob=1.0, scope='lstm-layer', reuse=False, return_final_state=False):\n",
    "    \"\"\"\n",
    "    LSTM layer.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, ...].\n",
    "        lengths: Tensor of shape [batch size].\n",
    "        state_size: LSTM state size.\n",
    "        keep_prob: 1 - p, where p is the dropout probability.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, state_size] containing the lstm\n",
    "        outputs at each timestep.\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        cell_fw = tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.contrib.rnn.LSTMCell(\n",
    "                state_size,\n",
    "                reuse=reuse\n",
    "            ),\n",
    "            output_keep_prob=keep_prob\n",
    "        )\n",
    "        outputs, output_state = tf.nn.dynamic_rnn(\n",
    "            inputs=inputs,\n",
    "            cell=cell_fw,\n",
    "            sequence_length=lengths,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        if return_final_state:\n",
    "            return outputs, output_state\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_distributed_dense_layer(inputs, output_units, bias=True, activation=None, batch_norm=None,\n",
    "                                 dropout=None, scope='time-distributed-dense-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Applies a shared dense layer to each timestep of a tensor of shape [batch_size, max_seq_len, input_units]\n",
    "    to produce a tensor of shape [batch_size, max_seq_len, output_units].\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, ...].\n",
    "        output_units: Number of output units.\n",
    "        activation: activation function.\n",
    "        dropout: dropout keep prob.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            name='weights',\n",
    "            initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            shape=[shape(inputs, -1), output_units]\n",
    "        )\n",
    "        z = tf.einsum('ijk,kl->ijl', inputs, W)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                name='biases',\n",
    "                initializer=tf.constant_initializer(),\n",
    "                shape=[output_units]\n",
    "            )\n",
    "            z = z + b\n",
    "\n",
    "        if batch_norm is not None:\n",
    "            z = tf.layers.batch_normalization(z, training=batch_norm, reuse=reuse)\n",
    "\n",
    "        z = activation(z) if activation else z\n",
    "        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_log_loss(y, y_hat, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Calculates log loss between two tensors.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor.\n",
    "        y_hat: Prediction tensor\n",
    "\n",
    "    Returns:\n",
    "        Log loss. 0-dimensional tensor.\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.minimum(tf.maximum(y_hat, eps), 1.0 - eps)\n",
    "    log_loss = -tf.reduce_mean(y*tf.log(y_hat) + (1.0 - y)*tf.log(1.0 - y_hat))\n",
    "    return log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable parameters:\n",
      "[('aisle_embeddings:0', [135, 50]), ('dept_embeddings:0', [22, 10]), ('user_embeddings:0', [207000, 50]), ('product_embeddings:0', [50000, 50]), ('lstm1/rnn/lstm_cell/kernel:0', [1754, 1200]), ('lstm1/rnn/lstm_cell/bias:0', [1200]), ('dense1/weights:0', [1754, 50]), ('dense1/biases:0', [50]), ('dense-2/weights:0', [50, 2]), ('dense-2/biases:0', [2])]\n",
      "trainable parameter count:\n",
      "15050822\n",
      "built graph\n"
     ]
    }
   ],
   "source": [
    " with tf.Graph().as_default() as graph:\n",
    "        \n",
    "    user_id = tf.placeholder(tf.int32, [None])\n",
    "    master_aisle_id = tf.placeholder(tf.int32, [None])\n",
    "    master_department_id = tf.placeholder(tf.int32, [None])\n",
    "    history_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    IsInOrder_history = tf.placeholder(tf.int32, [None, N]) #\n",
    "    order_dow_history = tf.placeholder(tf.int32, [None, N])\n",
    "    order_hour_of_day_history = tf.placeholder(tf.int32, [None, N])\n",
    "    days_since_prior_order_history = tf.placeholder(tf.int32, [None, N])\n",
    "    OrderSize_history = tf.placeholder(tf.int32, [None, N]) #\n",
    "    IndexInOrder_history = tf.placeholder(tf.int32, [None, N]) #\n",
    "    order_number_history = tf.placeholder(tf.int32, [None, N])\n",
    "    NumProductsFromDep_history = tf.placeholder(tf.int32, [None, N]) #\n",
    "    NextInOrder_history = tf.placeholder(tf.int32, [None, N])\n",
    "    \n",
    "    label = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    \n",
    "    #ProductID1_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID2_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID3_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID4_history_a = tf.placeholder(tf.int32, [None, N])   \n",
    "    #ProductID5_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID6_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID7_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID8_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID9_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID11_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID10_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID12_history_a = tf.placeholder(tf.int32, [None, N])   \n",
    "    #ProductID13_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID14_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID15_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID16_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID17_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID18_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID19_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    #ProductID20_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    \n",
    "    #ProductNameEmbedding_history = tf.placeholder(tf.float32, [None, N, 50])\n",
    "\n",
    "    ProductID1_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID2_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID3_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID4_history = tf.placeholder(tf.int32, [None, N])   \n",
    "    ProductID5_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID6_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID7_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID8_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID9_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID11_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID10_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID12_history = tf.placeholder(tf.int32, [None, N])   \n",
    "    ProductID13_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID14_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID15_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID16_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID17_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID18_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID19_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID20_history = tf.placeholder(tf.int32, [None, N])\n",
    "  \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    # aisle data\n",
    "    aisle_embeddings = tf.get_variable(\n",
    "        name='aisle_embeddings',\n",
    "        shape=[135, 50],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "  \n",
    "    # department data\n",
    "    dept_embeddings = tf.get_variable(\n",
    "        name='dept_embeddings',\n",
    "        shape=[22, 10],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "  \n",
    "    x_aisle = tf.concat([\n",
    "        tf.nn.embedding_lookup(aisle_embeddings, master_aisle_id),\n",
    "        tf.nn.embedding_lookup(dept_embeddings, master_department_id),\n",
    "    ], axis=1)\n",
    "    x_aisle = tf.tile(tf.expand_dims(x_aisle, 1), (1, N, 1))\n",
    "\n",
    "    # user data\n",
    "    user_embeddings = tf.get_variable(\n",
    "        name='user_embeddings',\n",
    "        shape=[207000, min(lstm_size,50)],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    x_user = tf.nn.embedding_lookup(user_embeddings, user_id)\n",
    "    x_user = tf.tile(tf.expand_dims(x_user, 1), (1, N, 1))\n",
    "\n",
    "    # product data\n",
    "    product_embeddings = tf.get_variable(\n",
    "        name='product_embeddings',\n",
    "        shape=[50000, 50],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "       \n",
    "    #x_product_1a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID1_history_a))\n",
    "    #x_product_2a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID2_history_a))\n",
    "    #x_product_3a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID3_history_a))\n",
    "    #x_product_4a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID4_history_a))\n",
    "    #x_product_5a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID5_history_a))\n",
    "    #x_product_6a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID6_history_a))\n",
    "    #x_product_7a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID7_history_a))\n",
    "    #x_product_8a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID8_history_a))\n",
    "    #x_product_9a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID9_history_a))\n",
    "    #x_product_10a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID10_history_a))\n",
    "    #x_product_11a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID11_history_a))\n",
    "    #x_product_12a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID12_history_a))\n",
    "    #x_product_13a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID13_history_a))\n",
    "    #x_product_14a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID14_history_a))\n",
    "    #x_product_15a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID15_history_a))\n",
    "    #x_product_16a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID16_history_a))\n",
    "    #x_product_17a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID17_history_a))\n",
    "    #x_product_18a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID18_history_a))\n",
    "    #x_product_19a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID19_history_a))\n",
    "    #x_product_20a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID20_history_a))\n",
    "    \n",
    "    x_product_1 = tf.nn.embedding_lookup(product_embeddings, (ProductID1_history))\n",
    "    x_product_2 = tf.nn.embedding_lookup(product_embeddings, (ProductID2_history))\n",
    "    x_product_3 = tf.nn.embedding_lookup(product_embeddings, (ProductID3_history))\n",
    "    x_product_4 = tf.nn.embedding_lookup(product_embeddings, (ProductID4_history))\n",
    "    x_product_5 = tf.nn.embedding_lookup(product_embeddings, (ProductID5_history))\n",
    "    x_product_6 = tf.nn.embedding_lookup(product_embeddings, (ProductID6_history))\n",
    "    x_product_7 = tf.nn.embedding_lookup(product_embeddings, (ProductID7_history))\n",
    "    x_product_8 = tf.nn.embedding_lookup(product_embeddings, (ProductID8_history))\n",
    "    x_product_9 = tf.nn.embedding_lookup(product_embeddings, (ProductID9_history))\n",
    "    x_product_10 = tf.nn.embedding_lookup(product_embeddings, (ProductID10_history))\n",
    "    x_product_11 = tf.nn.embedding_lookup(product_embeddings, (ProductID11_history))\n",
    "    x_product_12 = tf.nn.embedding_lookup(product_embeddings, (ProductID12_history))\n",
    "    x_product_13 = tf.nn.embedding_lookup(product_embeddings, (ProductID13_history))\n",
    "    x_product_14 = tf.nn.embedding_lookup(product_embeddings, (ProductID14_history))\n",
    "    x_product_15 = tf.nn.embedding_lookup(product_embeddings, (ProductID15_history))\n",
    "    x_product_16 = tf.nn.embedding_lookup(product_embeddings, (ProductID16_history))\n",
    "    x_product_17 = tf.nn.embedding_lookup(product_embeddings, (ProductID17_history))\n",
    "    x_product_18 = tf.nn.embedding_lookup(product_embeddings, (ProductID18_history))\n",
    "    x_product_19 = tf.nn.embedding_lookup(product_embeddings, (ProductID19_history))\n",
    "    x_product_20 = tf.nn.embedding_lookup(product_embeddings, (ProductID20_history))\n",
    "    \n",
    "   \n",
    "    # sequence data\n",
    "    IsInOrder_history_oh = tf.one_hot(IsInOrder_history, 2)\n",
    "    order_dow_history_oh = tf.one_hot(order_dow_history, 8)\n",
    "    order_hour_of_day_history_oh = tf.one_hot(order_hour_of_day_history, 25)\n",
    "    days_since_prior_order_history_oh = tf.one_hot(days_since_prior_order_history, 31)\n",
    "    OrderSize_history_oh = tf.one_hot(OrderSize_history, 60)\n",
    "    IndexInOrder_history_oh = tf.one_hot(OrderSize_history, 60)\n",
    "    order_number_history_oh = tf.one_hot(order_number_history, 101)\n",
    "    NumProductsFromDep_history_oh = tf.one_hot(NumProductsFromDep_history, 50)\n",
    "    \n",
    "    order_dow_history_scalar = tf.expand_dims(tf.cast(order_dow_history, tf.float32) / 8.0, 2)\n",
    "    order_hour_of_day_history_scalar = tf.expand_dims(tf.cast(order_hour_of_day_history, tf.float32) / 25.0, 2)\n",
    "    days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "    OrderSize_history_scalar = tf.expand_dims(tf.cast(OrderSize_history, tf.float32) / 60.0, 2)\n",
    "    IndexInOrder_history_scalar = tf.expand_dims(tf.cast(IndexInOrder_history, tf.float32) / 60.0, 2)\n",
    "    order_number_history_scalar = tf.expand_dims(tf.cast(order_number_history, tf.float32) / 100.0, 2)\n",
    "    NumProductsFromDep_history_scalar = tf.expand_dims(tf.cast(NumProductsFromDep_history, tf.float32) / 50.0, 2)\n",
    "\n",
    "\n",
    "\n",
    "    x_history = tf.concat([IsInOrder_history_oh,order_dow_history_oh,order_hour_of_day_history_oh,days_since_prior_order_history_oh,\\\n",
    "        OrderSize_history_oh,IndexInOrder_history_oh,NumProductsFromDep_history_oh,order_number_history_oh,order_dow_history_scalar,\\\n",
    "        order_hour_of_day_history_scalar,days_since_prior_order_history_scalar,OrderSize_history_scalar,IndexInOrder_history_scalar,\\\n",
    "        order_number_history_scalar,NumProductsFromDep_history_scalar,x_product_1,x_product_2,x_product_3,\\\n",
    "        x_product_4,x_product_5,x_product_6,x_product_7,x_product_8,x_product_9\\\n",
    "        ,x_product_10,x_product_11,x_product_12,x_product_13,\\\n",
    "        x_product_14,x_product_15,x_product_16,x_product_17,x_product_18,\\\n",
    "        x_product_19,x_product_20#,x_product_1a,x_product_2a,x_product_3a,\\\n",
    "        #x_product_4a,x_product_5a,x_product_6a,x_product_7a,x_product_8a,x_product_9a\\\n",
    "        #,x_product_10a,x_product_11a,x_product_12a,x_product_13a,\\\n",
    "        #x_product_14a,x_product_15a,x_product_16a,x_product_17a,x_product_18a,\\\n",
    "        #x_product_19a,x_product_20a], axis=2) #ProductNameEmbedding_history\n",
    "        ], axis=2)\n",
    "    \n",
    "    x = tf.concat([x_history, x_aisle, x_user], axis=2)\n",
    "    #h = lstm_layer(x, history_length, lstm_size, scope='lstm1')\n",
    "\n",
    "    if WaveNet:\n",
    "        c = wavenet(x, dilations, filter_widths, skip_channels, residual_channels)\n",
    "        h = tf.concat([c , h], axis=2)\n",
    "    else:\n",
    "        h = lstm_layer(x, history_length, lstm_size, scope='lstm1')\n",
    "        h = tf.concat([h , x], axis=2)\n",
    "    \n",
    "\n",
    "    h_final = time_distributed_dense_layer(h, 50 , activation=tf.nn.relu, scope='dense1') \n",
    "    #y_hat = tf.squeeze(time_distributed_dense_layer(h_final, 1, activation=tf.nn.sigmoid, scope='dense3'), 2)\n",
    "\n",
    "    n_components = 1\n",
    "    params = time_distributed_dense_layer(h_final, n_components*2, scope='dense-2', activation=None)\n",
    "    ps, mixing_coefs = tf.split(params, 2, axis=2)\n",
    "\n",
    "    # this is implemented incorrectly, but it still helped...\n",
    "    mixing_coefs = tf.nn.softmax(mixing_coefs - tf.reduce_min(mixing_coefs, 2, keep_dims=True))\n",
    "    ps = tf.nn.sigmoid(ps)\n",
    "\n",
    "    #loss = sequence_log_loss(NextInOrder_history, preds, history_length, N)\n",
    "    labels = tf.tile(tf.expand_dims(NextInOrder_history, 2), (1, 1, n_components))\n",
    "    losses = tf.reduce_sum(mixing_coefs*tensor_log_loss(labels, ps), axis=2)\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(history_length, maxlen=N), tf.float32)\n",
    "    loss = tf.reduce_sum(losses*sequence_mask) / tf.cast(tf.reduce_sum(history_length), tf.float32)\n",
    "\n",
    "    \n",
    "    final_temporal_idx = tf.stack([tf.range(tf.shape(history_length)[0]), history_length - 1], axis=1)\n",
    "    final_states = tf.gather_nd(h_final, final_temporal_idx)\n",
    "    #final_predictions = tf.gather_nd(y_hat, final_temporal_idx)\n",
    "    final_predictions = tf.gather_nd(ps, final_temporal_idx)\n",
    "    ground_truth = tf.gather_nd(NextInOrder_history, final_temporal_idx)\n",
    "    \n",
    "    prediction_tensors = {\n",
    "        'user_ids': user_id,\n",
    "        'aisle_ids': master_aisle_id,\n",
    "        'final_states': final_states,\n",
    "        'predictions': final_predictions,\n",
    "        'ground_truth': ground_truth,\n",
    "        'history_length':history_length\n",
    "    }\n",
    "    #preds = y_hat\n",
    "    preds = ps\n",
    "        \n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate_var = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "    if regularization_constant != 0:\n",
    "        l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()])\n",
    "        loss = loss + regularization_constant*l2_norm\n",
    "\n",
    "    if opt == 'adam':\n",
    "        optimizer =  tf.train.AdamOptimizer(learning_rate_var)\n",
    "    elif opt == 'gd':\n",
    "        optimizer =  tf.train.GradientDescentOptimizer(learning_rate_var)\n",
    "    elif opt == 'rms':\n",
    "        optimizer =  tf.train.RMSPropOptimizer(learning_rate_var, decay=0.95, momentum=0.9)\n",
    "    else:\n",
    "        assert False, 'optimizer must be adam, gd, or rms'\n",
    "            \n",
    "    grads = optimizer.compute_gradients(loss)\n",
    "    clipped = [(tf.clip_by_value(g, -grad_clip, grad_clip), v_) for g, v_ in grads]\n",
    "\n",
    "    step = optimizer.apply_gradients(clipped, global_step=global_step)\n",
    "\n",
    "    print('trainable parameters:')\n",
    "    print([(var.name, shape(var)) for var in tf.trainable_variables()])\n",
    "\n",
    "    print('trainable parameter count:')\n",
    "    print(str(np.sum(np.prod(shape(var)) for var in tf.trainable_variables())))\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "session = tf.Session(graph=graph)\n",
    "print('built graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[step        0]]     [[train]]     loss: 0.68232197       [[val]]     loss: 0.68599959       [[val-seq]]     loss: 0.6716857        \n",
      "[[step     1000]]     [[train]]     loss: 0.10434882       [[val]]     loss: 0.50269874       [[val-seq]]     loss: 0.10351389       \n",
      "[[step     2000]]     [[train]]     loss: 0.08133827       [[val]]     loss: 0.48954841       [[val-seq]]     loss: 0.08100628       \n",
      "[[step     3000]]     [[train]]     loss: 0.08045529       [[val]]     loss: 0.47539102       [[val-seq]]     loss: 0.0800922        \n",
      "[[step     4000]]     [[train]]     loss: 0.07994609       [[val]]     loss: 0.46771655       [[val-seq]]     loss: 0.07956188       \n",
      "[[step     5000]]     [[train]]     loss: 0.07979196       [[val]]     loss: 0.46430372       [[val-seq]]     loss: 0.07939229       \n",
      "[[step     6000]]     [[train]]     loss: 0.07997105       [[val]]     loss: 0.46312255       [[val-seq]]     loss: 0.07956305       \n",
      "[[step     7000]]     [[train]]     loss: 0.07959625       [[val]]     loss: 0.45990644       [[val-seq]]     loss: 0.07918738       \n",
      "[[step     8000]]     [[train]]     loss: 0.07909027       [[val]]     loss: 0.45751373       [[val-seq]]     loss: 0.07868226       \n",
      "[[step     9000]]     [[train]]     loss: 0.07896094       [[val]]     loss: 0.45016431       [[val-seq]]     loss: 0.07854193       \n",
      "[[step    10000]]     [[train]]     loss: 0.07891905       [[val]]     loss: 0.4532611        [[val-seq]]     loss: 0.07850747       \n",
      "[[step    11000]]     [[train]]     loss: 0.07895777       [[val]]     loss: 0.45359302       [[val-seq]]     loss: 0.078547         \n",
      "[[step    12000]]     [[train]]     loss: 0.07871256       [[val]]     loss: 0.45022033       [[val-seq]]     loss: 0.07829677       \n",
      "[[step    13000]]     [[train]]     loss: 0.07874456       [[val]]     loss: 0.44885174       [[val-seq]]     loss: 0.07833112       \n",
      "[[step    14000]]     [[train]]     loss: 0.07887424       [[val]]     loss: 0.4489904        [[val-seq]]     loss: 0.07846212       \n",
      "[[step    15000]]     [[train]]     loss: 0.07853866       [[val]]     loss: 0.44592302       [[val-seq]]     loss: 0.07812886       \n",
      "[[step    16000]]     [[train]]     loss: 0.07786896       [[val]]     loss: 0.44713002       [[val-seq]]     loss: 0.07745991       \n",
      "[[step    17000]]     [[train]]     loss: 0.07801581       [[val]]     loss: 0.44591978       [[val-seq]]     loss: 0.07761441       \n",
      "[[step    18000]]     [[train]]     loss: 0.07810663       [[val]]     loss: 0.44412972       [[val-seq]]     loss: 0.07770286       \n",
      "[[step    19000]]     [[train]]     loss: 0.07806666       [[val]]     loss: 0.44403877       [[val-seq]]     loss: 0.07766465       \n",
      "[[step    20000]]     [[train]]     loss: 0.07787024       [[val]]     loss: 0.44548859       [[val-seq]]     loss: 0.07746778       \n",
      "[[step    21000]]     [[train]]     loss: 0.07829067       [[val]]     loss: 0.44342787       [[val-seq]]     loss: 0.07789184       \n",
      "[[step    22000]]     [[train]]     loss: 0.078131         [[val]]     loss: 0.43930097       [[val-seq]]     loss: 0.07772905       \n",
      "[[step    23000]]     [[train]]     loss: 0.0778648        [[val]]     loss: 0.44125393       [[val-seq]]     loss: 0.07746505       \n",
      "[[step    24000]]     [[train]]     loss: 0.07794297       [[val]]     loss: 0.43771784       [[val-seq]]     loss: 0.07754524       \n",
      "[[step    25000]]     [[train]]     loss: 0.07787568       [[val]]     loss: 0.43596639       [[val-seq]]     loss: 0.0774784        \n",
      "[[step    26000]]     [[train]]     loss: 0.07780864       [[val]]     loss: 0.43511102       [[val-seq]]     loss: 0.07741047       \n",
      "[[step    27000]]     [[train]]     loss: 0.07771245       [[val]]     loss: 0.43560703       [[val-seq]]     loss: 0.07731642       \n",
      "[[step    28000]]     [[train]]     loss: 0.07747102       [[val]]     loss: 0.43837774       [[val-seq]]     loss: 0.07707632       \n",
      "[[step    29000]]     [[train]]     loss: 0.07765185       [[val]]     loss: 0.43447131       [[val-seq]]     loss: 0.07725671       \n",
      "[[step    30000]]     [[train]]     loss: 0.07779654       [[val]]     loss: 0.4344607        [[val-seq]]     loss: 0.07740047       \n",
      "[[step    31000]]     [[train]]     loss: 0.07746097       [[val]]     loss: 0.43557022       [[val-seq]]     loss: 0.07707065       \n",
      "[[step    32000]]     [[train]]     loss: 0.07777955       [[val]]     loss: 0.43546011       [[val-seq]]     loss: 0.07739269       \n",
      "[[step    33000]]     [[train]]     loss: 0.07747816       [[val]]     loss: 0.43255836       [[val-seq]]     loss: 0.07708589       \n",
      "[[step    34000]]     [[train]]     loss: 0.07750881       [[val]]     loss: 0.4308127        [[val-seq]]     loss: 0.07711664       \n",
      "[[step    35000]]     [[train]]     loss: 0.07736895       [[val]]     loss: 0.43144278       [[val-seq]]     loss: 0.07698231       \n",
      "[[step    36000]]     [[train]]     loss: 0.07731505       [[val]]     loss: 0.43335478       [[val-seq]]     loss: 0.07692917       \n",
      "[[step    37000]]     [[train]]     loss: 0.07731502       [[val]]     loss: 0.4323562        [[val-seq]]     loss: 0.07692713       \n",
      "[[step    38000]]     [[train]]     loss: 0.07723954       [[val]]     loss: 0.43180804       [[val-seq]]     loss: 0.07685309       \n",
      "[[step    39000]]     [[train]]     loss: 0.07689572       [[val]]     loss: 0.42870004       [[val-seq]]     loss: 0.07651216       \n",
      "[[step    40000]]     [[train]]     loss: 0.07733199       [[val]]     loss: 0.43008969       [[val-seq]]     loss: 0.07694578       \n",
      "[[step    41000]]     [[train]]     loss: 0.07701986       [[val]]     loss: 0.42918518       [[val-seq]]     loss: 0.07663257       \n",
      "[[step    42000]]     [[train]]     loss: 0.0774741        [[val]]     loss: 0.42708624       [[val-seq]]     loss: 0.07708814       \n",
      "[[step    43000]]     [[train]]     loss: 0.07731719       [[val]]     loss: 0.42954897       [[val-seq]]     loss: 0.0769366        \n",
      "[[step    44000]]     [[train]]     loss: 0.07702913       [[val]]     loss: 0.4274282        [[val-seq]]     loss: 0.07664996       \n",
      "[[step    45000]]     [[train]]     loss: 0.07700821       [[val]]     loss: 0.4292015        [[val-seq]]     loss: 0.07662311       \n",
      "[[step    46000]]     [[train]]     loss: 0.07656665       [[val]]     loss: 0.42367955       [[val-seq]]     loss: 0.07617266       \n",
      "[[step    47000]]     [[train]]     loss: 0.07657496       [[val]]     loss: 0.4237224        [[val-seq]]     loss: 0.07615704       \n",
      "[[step    48000]]     [[train]]     loss: 0.07645329       [[val]]     loss: 0.42036978       [[val-seq]]     loss: 0.07600446       \n",
      "[[step    49000]]     [[train]]     loss: 0.07605277       [[val]]     loss: 0.41767769       [[val-seq]]     loss: 0.07559996       \n",
      "[[step    50000]]     [[train]]     loss: 0.07618439       [[val]]     loss: 0.41909765       [[val-seq]]     loss: 0.07571732       \n",
      "[[step    51000]]     [[train]]     loss: 0.07643688       [[val]]     loss: 0.41636928       [[val-seq]]     loss: 0.07597215       \n",
      "[[step    52000]]     [[train]]     loss: 0.07656798       [[val]]     loss: 0.41429805       [[val-seq]]     loss: 0.07610068       \n",
      "[[step    53000]]     [[train]]     loss: 0.07604277       [[val]]     loss: 0.41706457       [[val-seq]]     loss: 0.07557185       \n",
      "[[step    54000]]     [[train]]     loss: 0.07579088       [[val]]     loss: 0.4149543        [[val-seq]]     loss: 0.07532125       \n",
      "[[step    55000]]     [[train]]     loss: 0.07588792       [[val]]     loss: 0.41218545       [[val-seq]]     loss: 0.07541779       \n",
      "[[step    56000]]     [[train]]     loss: 0.07614349       [[val]]     loss: 0.41417098       [[val-seq]]     loss: 0.07567007       \n",
      "[[step    57000]]     [[train]]     loss: 0.07561213       [[val]]     loss: 0.41522655       [[val-seq]]     loss: 0.07513628       \n",
      "[[step    58000]]     [[train]]     loss: 0.07602619       [[val]]     loss: 0.41121041       [[val-seq]]     loss: 0.0755506        \n",
      "[[step    59000]]     [[train]]     loss: 0.07586172       [[val]]     loss: 0.41142725       [[val-seq]]     loss: 0.07539699       \n",
      "[[step    60000]]     [[train]]     loss: 0.07585018       [[val]]     loss: 0.41344306       [[val-seq]]     loss: 0.07537967       \n",
      "[[step    61000]]     [[train]]     loss: 0.07568754       [[val]]     loss: 0.41193111       [[val-seq]]     loss: 0.07521087       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[step    62000]]     [[train]]     loss: 0.07537154       [[val]]     loss: 0.413019         [[val-seq]]     loss: 0.07490084       \n",
      "[[step    63000]]     [[train]]     loss: 0.0756379        [[val]]     loss: 0.41322129       [[val-seq]]     loss: 0.07516719       \n",
      "[[step    64000]]     [[train]]     loss: 0.07536603       [[val]]     loss: 0.41245326       [[val-seq]]     loss: 0.07488664       \n",
      "[[step    65000]]     [[train]]     loss: 0.07526311       [[val]]     loss: 0.41284661       [[val-seq]]     loss: 0.07478981       \n",
      "[[step    66000]]     [[train]]     loss: 0.07566459       [[val]]     loss: 0.41498711       [[val-seq]]     loss: 0.07519048       \n",
      "[[step    67000]]     [[train]]     loss: 0.07600604       [[val]]     loss: 0.4102098        [[val-seq]]     loss: 0.07552564       \n",
      "[[step    68000]]     [[train]]     loss: 0.07536942       [[val]]     loss: 0.41124126       [[val-seq]]     loss: 0.0748886        \n",
      "[[step    69000]]     [[train]]     loss: 0.0755214        [[val]]     loss: 0.40927832       [[val-seq]]     loss: 0.07504002       \n",
      "[[step    70000]]     [[train]]     loss: 0.07546147       [[val]]     loss: 0.40801399       [[val-seq]]     loss: 0.07497711       \n",
      "[[step    71000]]     [[train]]     loss: 0.07567071       [[val]]     loss: 0.40738239       [[val-seq]]     loss: 0.07518897       \n",
      "[[step    72000]]     [[train]]     loss: 0.07522717       [[val]]     loss: 0.40905553       [[val-seq]]     loss: 0.07474285       \n",
      "[[step    73000]]     [[train]]     loss: 0.07553205       [[val]]     loss: 0.40969877       [[val-seq]]     loss: 0.07504871       \n",
      "[[step    74000]]     [[train]]     loss: 0.07520778       [[val]]     loss: 0.40801397       [[val-seq]]     loss: 0.07472425       \n",
      "[[step    75000]]     [[train]]     loss: 0.07547316       [[val]]     loss: 0.40850897       [[val-seq]]     loss: 0.07498893       \n",
      "[[step    76000]]     [[train]]     loss: 0.07571531       [[val]]     loss: 0.40898817       [[val-seq]]     loss: 0.07522925       \n",
      "[[step    77000]]     [[train]]     loss: 0.07538005       [[val]]     loss: 0.40847515       [[val-seq]]     loss: 0.07490085       \n",
      "[[step    78000]]     [[train]]     loss: 0.07539654       [[val]]     loss: 0.4076099        [[val-seq]]     loss: 0.07491231       \n",
      "[[step    79000]]     [[train]]     loss: 0.07548949       [[val]]     loss: 0.40729917       [[val-seq]]     loss: 0.07500916       \n",
      "[[step    80000]]     [[train]]     loss: 0.07536698       [[val]]     loss: 0.40732062       [[val-seq]]     loss: 0.07489009       \n",
      "[[step    81000]]     [[train]]     loss: 0.07529956       [[val]]     loss: 0.40653514       [[val-seq]]     loss: 0.07482095       \n",
      "[[step    82000]]     [[train]]     loss: 0.07546554       [[val]]     loss: 0.41054776       [[val-seq]]     loss: 0.07498526       \n",
      "[[step    83000]]     [[train]]     loss: 0.07530919       [[val]]     loss: 0.40832545       [[val-seq]]     loss: 0.07482846       \n",
      "[[step    84000]]     [[train]]     loss: 0.07503798       [[val]]     loss: 0.40614698       [[val-seq]]     loss: 0.07455874       \n",
      "[[step    85000]]     [[train]]     loss: 0.07483882       [[val]]     loss: 0.40323826       [[val-seq]]     loss: 0.07434947       \n",
      "[[step    86000]]     [[train]]     loss: 0.07553256       [[val]]     loss: 0.40881614       [[val-seq]]     loss: 0.07504787       \n",
      "[[step    87000]]     [[train]]     loss: 0.07557474       [[val]]     loss: 0.40431342       [[val-seq]]     loss: 0.07509208       \n",
      "[[step    88000]]     [[train]]     loss: 0.0749487        [[val]]     loss: 0.40792851       [[val-seq]]     loss: 0.07447208       \n",
      "[[step    89000]]     [[train]]     loss: 0.07547797       [[val]]     loss: 0.40645179       [[val-seq]]     loss: 0.07500085       \n",
      "[[step    90000]]     [[train]]     loss: 0.07530841       [[val]]     loss: 0.40687308       [[val-seq]]     loss: 0.07482736       \n",
      "[[step    91000]]     [[train]]     loss: 0.07469951       [[val]]     loss: 0.40496659       [[val-seq]]     loss: 0.07421234       \n",
      "[[step    92000]]     [[train]]     loss: 0.0749026        [[val]]     loss: 0.40060832       [[val-seq]]     loss: 0.07439134       \n",
      "[[step    93000]]     [[train]]     loss: 0.07482473       [[val]]     loss: 0.40248539       [[val-seq]]     loss: 0.07428399       \n",
      "[[step    95000]]     [[train]]     loss: 0.07439999       [[val]]     loss: 0.39702595       [[val-seq]]     loss: 0.07382971       \n",
      "[[step    96000]]     [[train]]     loss: 0.07461976       [[val]]     loss: 0.39476847       [[val-seq]]     loss: 0.07404672       \n",
      "[[step    97000]]     [[train]]     loss: 0.07473483       [[val]]     loss: 0.397912         [[val-seq]]     loss: 0.07414876       \n",
      "[[step    98000]]     [[train]]     loss: 0.07421213       [[val]]     loss: 0.39321331       [[val-seq]]     loss: 0.07362426       \n",
      "[[step    99000]]     [[train]]     loss: 0.07408296       [[val]]     loss: 0.39304828       [[val-seq]]     loss: 0.07350031       \n",
      "[[step   100000]]     [[train]]     loss: 0.07407155       [[val]]     loss: 0.39152749       [[val-seq]]     loss: 0.0734839        \n",
      "[[step   101000]]     [[train]]     loss: 0.07409783       [[val]]     loss: 0.39193393       [[val-seq]]     loss: 0.07349943       \n",
      "[[step   102000]]     [[train]]     loss: 0.07393642       [[val]]     loss: 0.39275694       [[val-seq]]     loss: 0.07333426       \n",
      "[[step   103000]]     [[train]]     loss: 0.07426227       [[val]]     loss: 0.39017537       [[val-seq]]     loss: 0.07365886       \n",
      "[[step   104000]]     [[train]]     loss: 0.07446788       [[val]]     loss: 0.3916849        [[val-seq]]     loss: 0.07387931       \n",
      "[[step   105000]]     [[train]]     loss: 0.07410468       [[val]]     loss: 0.39358543       [[val-seq]]     loss: 0.07353046       \n",
      "[[step   106000]]     [[train]]     loss: 0.07366003       [[val]]     loss: 0.39243587       [[val-seq]]     loss: 0.07307921       \n",
      "[[step   107000]]     [[train]]     loss: 0.07413472       [[val]]     loss: 0.3931448        [[val-seq]]     loss: 0.0735472        \n",
      "[[step   108000]]     [[train]]     loss: 0.07368553       [[val]]     loss: 0.39176952       [[val-seq]]     loss: 0.07310312       \n",
      "[[step   109000]]     [[train]]     loss: 0.07373705       [[val]]     loss: 0.39248266       [[val-seq]]     loss: 0.07314994       \n",
      "[[step   110000]]     [[train]]     loss: 0.07367822       [[val]]     loss: 0.39431078       [[val-seq]]     loss: 0.07309717       \n",
      "[[step   111000]]     [[train]]     loss: 0.07389192       [[val]]     loss: 0.39168727       [[val-seq]]     loss: 0.07330833       \n",
      "[[step   112000]]     [[train]]     loss: 0.07435227       [[val]]     loss: 0.39079176       [[val-seq]]     loss: 0.07376927       \n",
      "[[step   113000]]     [[train]]     loss: 0.07403493       [[val]]     loss: 0.39137458       [[val-seq]]     loss: 0.0734531        \n",
      "[[step   114000]]     [[train]]     loss: 0.07381562       [[val]]     loss: 0.39256949       [[val-seq]]     loss: 0.07322608       \n",
      "[[step   115000]]     [[train]]     loss: 0.07380336       [[val]]     loss: 0.38834302       [[val-seq]]     loss: 0.07322412       \n",
      "[[step   116000]]     [[train]]     loss: 0.07406373       [[val]]     loss: 0.39057003       [[val-seq]]     loss: 0.0734839        \n",
      "[[step   117000]]     [[train]]     loss: 0.07395897       [[val]]     loss: 0.38932677       [[val-seq]]     loss: 0.073377         \n",
      "[[step   118000]]     [[train]]     loss: 0.07383995       [[val]]     loss: 0.39094496       [[val-seq]]     loss: 0.07326213       \n",
      "[[step   119000]]     [[train]]     loss: 0.07341731       [[val]]     loss: 0.38986436       [[val-seq]]     loss: 0.07284302       \n",
      "[[step   120000]]     [[train]]     loss: 0.07402747       [[val]]     loss: 0.39211694       [[val-seq]]     loss: 0.07345129       \n",
      "[[step   121000]]     [[train]]     loss: 0.07385406       [[val]]     loss: 0.39030431       [[val-seq]]     loss: 0.0732725        \n",
      "[[step   122000]]     [[train]]     loss: 0.07364738       [[val]]     loss: 0.38889396       [[val-seq]]     loss: 0.07306909       \n",
      "[[step   123000]]     [[train]]     loss: 0.07430186       [[val]]     loss: 0.39075234       [[val-seq]]     loss: 0.07372205       \n",
      "[[step   124000]]     [[train]]     loss: 0.07381752       [[val]]     loss: 0.38704982       [[val-seq]]     loss: 0.07323882       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[step   125000]]     [[train]]     loss: 0.07395663       [[val]]     loss: 0.39155695       [[val-seq]]     loss: 0.07338876       \n",
      "[[step   126000]]     [[train]]     loss: 0.07336153       [[val]]     loss: 0.38946542       [[val-seq]]     loss: 0.07279541       \n",
      "[[step   127000]]     [[train]]     loss: 0.07415682       [[val]]     loss: 0.39117238       [[val-seq]]     loss: 0.07358691       \n",
      "[[step   128000]]     [[train]]     loss: 0.07399205       [[val]]     loss: 0.3920305        [[val-seq]]     loss: 0.07341879       \n",
      "[[step   129000]]     [[train]]     loss: 0.07353377       [[val]]     loss: 0.38683516       [[val-seq]]     loss: 0.07296143       \n",
      "[[step   130000]]     [[train]]     loss: 0.07340056       [[val]]     loss: 0.38795441       [[val-seq]]     loss: 0.07282348       \n",
      "[[step   131000]]     [[train]]     loss: 0.0738987        [[val]]     loss: 0.38846179       [[val-seq]]     loss: 0.07331807       \n",
      "[[step   132000]]     [[train]]     loss: 0.07398581       [[val]]     loss: 0.38813432       [[val-seq]]     loss: 0.07340768       \n",
      "[[step   133000]]     [[train]]     loss: 0.07348981       [[val]]     loss: 0.38684429       [[val-seq]]     loss: 0.07291058       \n",
      "[[step   134000]]     [[train]]     loss: 0.07364678       [[val]]     loss: 0.38877862       [[val-seq]]     loss: 0.07307195       \n",
      "[[step   135000]]     [[train]]     loss: 0.07393408       [[val]]     loss: 0.39079034       [[val-seq]]     loss: 0.07336026       \n",
      "[[step   136000]]     [[train]]     loss: 0.07376744       [[val]]     loss: 0.38912786       [[val-seq]]     loss: 0.07318838       \n",
      "[[step   137000]]     [[train]]     loss: 0.07303699       [[val]]     loss: 0.38272654       [[val-seq]]     loss: 0.07244017       \n",
      "[[step   138000]]     [[train]]     loss: 0.07314188       [[val]]     loss: 0.38084406       [[val-seq]]     loss: 0.07252609       \n",
      "[[step   139000]]     [[train]]     loss: 0.07325019       [[val]]     loss: 0.38175212       [[val-seq]]     loss: 0.07261738       \n",
      "[[step   140000]]     [[train]]     loss: 0.07280347       [[val]]     loss: 0.37767102       [[val-seq]]     loss: 0.07216138       \n",
      "[[step   141000]]     [[train]]     loss: 0.07313705       [[val]]     loss: 0.37661538       [[val-seq]]     loss: 0.07249813       \n",
      "[[step   142000]]     [[train]]     loss: 0.07303732       [[val]]     loss: 0.37817748       [[val-seq]]     loss: 0.07238261       \n",
      "[[step   143000]]     [[train]]     loss: 0.07312638       [[val]]     loss: 0.37343445       [[val-seq]]     loss: 0.07245664       \n",
      "[[step   144000]]     [[train]]     loss: 0.07251772       [[val]]     loss: 0.37692806       [[val-seq]]     loss: 0.0718644        \n",
      "[[step   145000]]     [[train]]     loss: 0.07245976       [[val]]     loss: 0.37284979       [[val-seq]]     loss: 0.0718016        \n",
      "[[step   146000]]     [[train]]     loss: 0.07260618       [[val]]     loss: 0.37078373       [[val-seq]]     loss: 0.07192616       \n",
      "[[step   147000]]     [[train]]     loss: 0.07256831       [[val]]     loss: 0.37373428       [[val-seq]]     loss: 0.07189584       \n",
      "[[step   148000]]     [[train]]     loss: 0.07263671       [[val]]     loss: 0.37557466       [[val-seq]]     loss: 0.07196413       \n",
      "[[step   149000]]     [[train]]     loss: 0.07252777       [[val]]     loss: 0.36881098       [[val-seq]]     loss: 0.07185328       \n",
      "[[step   150000]]     [[train]]     loss: 0.07285712       [[val]]     loss: 0.37646263       [[val-seq]]     loss: 0.07221441       \n",
      "[[step   151000]]     [[train]]     loss: 0.07287787       [[val]]     loss: 0.37632411       [[val-seq]]     loss: 0.07222032       \n",
      "[[step   152000]]     [[train]]     loss: 0.07233902       [[val]]     loss: 0.37268615       [[val-seq]]     loss: 0.07167104       \n",
      "[[step   153000]]     [[train]]     loss: 0.07243534       [[val]]     loss: 0.37648857       [[val-seq]]     loss: 0.07178464       \n",
      "[[step   154000]]     [[train]]     loss: 0.07184572       [[val]]     loss: 0.37263019       [[val-seq]]     loss: 0.07118816       \n",
      "[[step   155000]]     [[train]]     loss: 0.07261929       [[val]]     loss: 0.37699478       [[val-seq]]     loss: 0.07195553       \n",
      "[[step   156000]]     [[train]]     loss: 0.07240338       [[val]]     loss: 0.37613701       [[val-seq]]     loss: 0.07174845       \n",
      "[[step   157000]]     [[train]]     loss: 0.07274554       [[val]]     loss: 0.3770591        [[val-seq]]     loss: 0.07209694       \n",
      "[[step   158000]]     [[train]]     loss: 0.07267728       [[val]]     loss: 0.37416568       [[val-seq]]     loss: 0.07201685       \n",
      "[[step   159000]]     [[train]]     loss: 0.07256666       [[val]]     loss: 0.37346202       [[val-seq]]     loss: 0.07190281       \n",
      "[[step   160000]]     [[train]]     loss: 0.07239595       [[val]]     loss: 0.37321095       [[val-seq]]     loss: 0.07174745       \n",
      "[[step   161000]]     [[train]]     loss: 0.07265531       [[val]]     loss: 0.3746624        [[val-seq]]     loss: 0.07200971       \n",
      "[[step   162000]]     [[train]]     loss: 0.07266717       [[val]]     loss: 0.37256191       [[val-seq]]     loss: 0.07201582       \n",
      "[[step   163000]]     [[train]]     loss: 0.07252958       [[val]]     loss: 0.37682996       [[val-seq]]     loss: 0.07188891       \n",
      "[[step   164000]]     [[train]]     loss: 0.07238924       [[val]]     loss: 0.37601223       [[val-seq]]     loss: 0.07174745       \n",
      "[[step   165000]]     [[train]]     loss: 0.07237104       [[val]]     loss: 0.37338612       [[val-seq]]     loss: 0.07172964       \n",
      "[[step   166000]]     [[train]]     loss: 0.0727249        [[val]]     loss: 0.37502013       [[val-seq]]     loss: 0.07208          \n",
      "[[step   167000]]     [[train]]     loss: 0.07241646       [[val]]     loss: 0.37130096       [[val-seq]]     loss: 0.07177035       \n",
      "[[step   168000]]     [[train]]     loss: 0.07261275       [[val]]     loss: 0.37585605       [[val-seq]]     loss: 0.07196522       \n",
      "[[step   169000]]     [[train]]     loss: 0.07266266       [[val]]     loss: 0.37337655       [[val-seq]]     loss: 0.07201137       \n",
      "[[step   171000]]     [[train]]     loss: 0.07266101       [[val]]     loss: 0.37317242       [[val-seq]]     loss: 0.07202275       \n",
      "[[step   172000]]     [[train]]     loss: 0.07256289       [[val]]     loss: 0.37758723       [[val-seq]]     loss: 0.07192507       \n",
      "[[step   173000]]     [[train]]     loss: 0.07235501       [[val]]     loss: 0.37394747       [[val-seq]]     loss: 0.07171484       \n",
      "[[step   174000]]     [[train]]     loss: 0.07266105       [[val]]     loss: 0.37581762       [[val-seq]]     loss: 0.07201855       \n",
      "[[step   175000]]     [[train]]     loss: 0.07198671       [[val]]     loss: 0.37146124       [[val-seq]]     loss: 0.07134405       \n",
      "[[step   176000]]     [[train]]     loss: 0.07236288       [[val]]     loss: 0.37258594       [[val-seq]]     loss: 0.07171455       \n",
      "[[step   177000]]     [[train]]     loss: 0.07268939       [[val]]     loss: 0.37415798       [[val-seq]]     loss: 0.07203817       \n",
      "[[step   178000]]     [[train]]     loss: 0.0725129        [[val]]     loss: 0.3718787        [[val-seq]]     loss: 0.07186597       \n",
      "[[step   179000]]     [[train]]     loss: 0.07220974       [[val]]     loss: 0.37245026       [[val-seq]]     loss: 0.07156153       \n",
      "[[step   180000]]     [[train]]     loss: 0.07242684       [[val]]     loss: 0.37332899       [[val-seq]]     loss: 0.07178236       \n",
      "[[step   181000]]     [[train]]     loss: 0.0727559        [[val]]     loss: 0.37521292       [[val-seq]]     loss: 0.07210367       \n",
      "[[step   182000]]     [[train]]     loss: 0.0718773        [[val]]     loss: 0.36888653       [[val-seq]]     loss: 0.07121962       \n",
      "[[step   183000]]     [[train]]     loss: 0.07191462       [[val]]     loss: 0.36560557       [[val-seq]]     loss: 0.07123706       \n",
      "[[step   184000]]     [[train]]     loss: 0.07174465       [[val]]     loss: 0.36581194       [[val-seq]]     loss: 0.07104893       \n",
      "[[step   185000]]     [[train]]     loss: 0.0718961        [[val]]     loss: 0.3627444        [[val-seq]]     loss: 0.07119998       \n",
      "[[step   186000]]     [[train]]     loss: 0.0718292        [[val]]     loss: 0.36424322       [[val-seq]]     loss: 0.07113566       \n",
      "[[step   187000]]     [[train]]     loss: 0.07150529       [[val]]     loss: 0.36058473       [[val-seq]]     loss: 0.0708047        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[step   188000]]     [[train]]     loss: 0.07206294       [[val]]     loss: 0.35971982       [[val-seq]]     loss: 0.07134641       \n",
      "[[step   189000]]     [[train]]     loss: 0.07155173       [[val]]     loss: 0.3600603        [[val-seq]]     loss: 0.07084775       \n",
      "[[step   190000]]     [[train]]     loss: 0.07131578       [[val]]     loss: 0.3594293        [[val-seq]]     loss: 0.07061077       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1694: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1694: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[step   191000]]     [[train]]     loss: nan              [[val]]     loss: nan              [[val-seq]]     loss: nan              \n",
      "[[step   192000]]     [[train]]     loss: 0.07236744       [[val]]     loss: 0.3774675        [[val-seq]]     loss: 0.07188709       \n",
      "[[step   193000]]     [[train]]     loss: 0.07334141       [[val]]     loss: 0.38646228       [[val-seq]]     loss: 0.07286074       \n",
      "[[step   194000]]     [[train]]     loss: 0.07419034       [[val]]     loss: 0.39522999       [[val-seq]]     loss: 0.07371237       \n",
      "[[step   195000]]     [[train]]     loss: 0.07410553       [[val]]     loss: 0.39429719       [[val-seq]]     loss: 0.07364827       \n",
      "[[step   196000]]     [[train]]     loss: 0.07411413       [[val]]     loss: 0.39654429       [[val-seq]]     loss: 0.07366943       \n",
      "[[step   197000]]     [[train]]     loss: 0.07374326       [[val]]     loss: 0.3939028        [[val-seq]]     loss: 0.07331389       \n",
      "[[step   198000]]     [[train]]     loss: 0.07360201       [[val]]     loss: 0.3941802        [[val-seq]]     loss: 0.07318541       \n",
      "[[step   199000]]     [[train]]     loss: 0.07389121       [[val]]     loss: 0.39815194       [[val-seq]]     loss: 0.073485         \n",
      "[[step   200000]]     [[train]]     loss: 0.07389669       [[val]]     loss: 0.39793358       [[val-seq]]     loss: 0.07350001       \n",
      "[[step   201000]]     [[train]]     loss: 0.0733749        [[val]]     loss: 0.3997197        [[val-seq]]     loss: 0.07298444       \n",
      "[[step   202000]]     [[train]]     loss: 0.07356269       [[val]]     loss: 0.39327484       [[val-seq]]     loss: 0.07317882       \n",
      "[[step   203000]]     [[train]]     loss: 0.0737548        [[val]]     loss: 0.394039         [[val-seq]]     loss: 0.07337526       \n",
      "[[step   204000]]     [[train]]     loss: 0.07318706       [[val]]     loss: 0.38929          [[val-seq]]     loss: 0.0728076        \n",
      "[[step   205000]]     [[train]]     loss: 0.07280526       [[val]]     loss: 0.38503505       [[val-seq]]     loss: 0.0724145        \n",
      "[[step   206000]]     [[train]]     loss: 0.07222806       [[val]]     loss: 0.37872348       [[val-seq]]     loss: 0.0718195        \n",
      "[[step   207000]]     [[train]]     loss: 0.07249749       [[val]]     loss: 0.37579498       [[val-seq]]     loss: 0.0720591        \n",
      "[[step   208000]]     [[train]]     loss: 0.0720618        [[val]]     loss: 0.36909301       [[val-seq]]     loss: 0.07157835       \n",
      "[[step   209000]]     [[train]]     loss: 0.07131499       [[val]]     loss: 0.36850896       [[val-seq]]     loss: 0.0707815        \n",
      "[[step   210000]]     [[train]]     loss: 0.07126648       [[val]]     loss: 0.36541937       [[val-seq]]     loss: 0.07069168       \n",
      "[[step   211000]]     [[train]]     loss: 0.07163246       [[val]]     loss: 0.36352966       [[val-seq]]     loss: 0.07101701       \n",
      "[[step   212000]]     [[train]]     loss: 0.07103754       [[val]]     loss: 0.3602164        [[val-seq]]     loss: 0.07038979       \n",
      "[[step   213000]]     [[train]]     loss: 0.07118707       [[val]]     loss: 0.35802198       [[val-seq]]     loss: 0.07051354       \n",
      "[[step   214000]]     [[train]]     loss: 0.07139047       [[val]]     loss: 0.35627017       [[val-seq]]     loss: 0.07069111       \n",
      "[[step   215000]]     [[train]]     loss: 0.07106537       [[val]]     loss: 0.35780791       [[val-seq]]     loss: 0.07035385       \n",
      "[[step   216000]]     [[train]]     loss: 0.07104327       [[val]]     loss: 0.35846765       [[val-seq]]     loss: 0.07033204       \n",
      "[[step   217000]]     [[train]]     loss: 0.07114553       [[val]]     loss: 0.36019162       [[val-seq]]     loss: 0.07043388       \n",
      "[[step   218000]]     [[train]]     loss: 0.07111676       [[val]]     loss: 0.35894231       [[val-seq]]     loss: 0.0704045        \n",
      "[[step   219000]]     [[train]]     loss: nan              [[val]]     loss: nan              [[val-seq]]     loss: nan              \n",
      "[[step   220000]]     [[train]]     loss: 0.07108245       [[val]]     loss: 0.36050384       [[val-seq]]     loss: 0.07059223       \n",
      "[[step   221000]]     [[train]]     loss: 0.07172212       [[val]]     loss: 0.36449881       [[val-seq]]     loss: 0.0712386        \n",
      "[[step   222000]]     [[train]]     loss: 0.07223331       [[val]]     loss: 0.37014491       [[val-seq]]     loss: 0.07175335       \n",
      "[[step   223000]]     [[train]]     loss: 0.0725205        [[val]]     loss: 0.37337381       [[val-seq]]     loss: 0.07204425       \n",
      "[[step   224000]]     [[train]]     loss: 0.07251706       [[val]]     loss: 0.37346971       [[val-seq]]     loss: 0.07204959       \n",
      "[[step   225000]]     [[train]]     loss: 0.07238795       [[val]]     loss: 0.37824549       [[val-seq]]     loss: 0.07192923       \n",
      "[[step   226000]]     [[train]]     loss: 0.07288508       [[val]]     loss: 0.38057312       [[val-seq]]     loss: 0.0724344        \n",
      "[[step   227000]]     [[train]]     loss: 0.07248665       [[val]]     loss: 0.37841738       [[val-seq]]     loss: 0.07204462       \n",
      "[[step   228000]]     [[train]]     loss: 0.07218964       [[val]]     loss: 0.37736292       [[val-seq]]     loss: 0.07175826       \n",
      "[[step   229000]]     [[train]]     loss: 0.07261567       [[val]]     loss: 0.38375698       [[val-seq]]     loss: 0.07219261       \n",
      "[[step   230000]]     [[train]]     loss: 0.07248769       [[val]]     loss: 0.37953162       [[val-seq]]     loss: 0.0720713        \n",
      "[[step   231000]]     [[train]]     loss: 0.07236563       [[val]]     loss: 0.37969245       [[val-seq]]     loss: 0.0719532        \n",
      "[[step   232000]]     [[train]]     loss: 0.07238833       [[val]]     loss: 0.37986264       [[val-seq]]     loss: 0.07197619       \n",
      "[[step   233000]]     [[train]]     loss: 0.07229375       [[val]]     loss: 0.3748423        [[val-seq]]     loss: 0.07186913       \n",
      "[[step   234000]]     [[train]]     loss: 0.07176938       [[val]]     loss: 0.36874531       [[val-seq]]     loss: 0.07130574       \n",
      "[[step   235000]]     [[train]]     loss: 0.07111877       [[val]]     loss: 0.36670231       [[val-seq]]     loss: 0.07061736       \n",
      "[[step   236000]]     [[train]]     loss: 0.07132492       [[val]]     loss: 0.36314693       [[val-seq]]     loss: 0.07080333       \n",
      "[[step   237000]]     [[train]]     loss: 0.07138646       [[val]]     loss: 0.36226519       [[val-seq]]     loss: 0.07083842       \n",
      "[[step   238000]]     [[train]]     loss: 0.07128584       [[val]]     loss: 0.36379967       [[val-seq]]     loss: 0.07071704       \n",
      "[[step   239000]]     [[train]]     loss: 0.07109174       [[val]]     loss: 0.35978458       [[val-seq]]     loss: 0.07049501       \n",
      "[[step   240000]]     [[train]]     loss: 0.07136154       [[val]]     loss: 0.36042811       [[val-seq]]     loss: 0.07073596       \n",
      "[[step   241000]]     [[train]]     loss: 0.07168616       [[val]]     loss: 0.3614074        [[val-seq]]     loss: 0.07105682       \n",
      "[[step   242000]]     [[train]]     loss: 0.07082397       [[val]]     loss: 0.3591794        [[val-seq]]     loss: 0.07017598       \n",
      "[[step   243000]]     [[train]]     loss: 0.07104244       [[val]]     loss: 0.3570566        [[val-seq]]     loss: 0.07036861       \n",
      "[[step   244000]]     [[train]]     loss: 0.07078886       [[val]]     loss: nan              [[val-seq]]     loss: nan              \n",
      "[[step   245000]]     [[train]]     loss: 0.07139066       [[val]]     loss: 0.36168557       [[val-seq]]     loss: 0.07072361       \n",
      "[[step   246000]]     [[train]]     loss: 0.07085199       [[val]]     loss: 0.36145492       [[val-seq]]     loss: 0.07017524       \n",
      "[[step   247000]]     [[train]]     loss: 0.07088073       [[val]]     loss: 0.36277097       [[val-seq]]     loss: 0.07020754       \n",
      "[[step   248000]]     [[train]]     loss: nan              [[val]]     loss: 0.3623481        [[val-seq]]     loss: nan              \n",
      "[[step   249000]]     [[train]]     loss: 0.07182698       [[val]]     loss: 0.36381385       [[val-seq]]     loss: 0.07135724       \n",
      "[[step   250000]]     [[train]]     loss: 0.07182384       [[val]]     loss: 0.36674504       [[val-seq]]     loss: 0.0713686        \n",
      "[[step   251000]]     [[train]]     loss: 0.07210267       [[val]]     loss: 0.37111754       [[val-seq]]     loss: 0.07165191       \n",
      "[[step   252000]]     [[train]]     loss: 0.07263721       [[val]]     loss: 0.37427153       [[val-seq]]     loss: 0.072195         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[step   253000]]     [[train]]     loss: 0.07259782       [[val]]     loss: 0.37892899       [[val-seq]]     loss: 0.07216712       \n",
      "[[step   254000]]     [[train]]     loss: 0.07272825       [[val]]     loss: 0.38322921       [[val-seq]]     loss: 0.07230866       \n",
      "[[step   255000]]     [[train]]     loss: 0.07244787       [[val]]     loss: 0.38210882       [[val-seq]]     loss: 0.07203892       \n",
      "[[step   256000]]     [[train]]     loss: 0.07221163       [[val]]     loss: 0.38095781       [[val-seq]]     loss: 0.07180948       \n",
      "[[step   257000]]     [[train]]     loss: nan              [[val]]     loss: nan              [[val-seq]]     loss: nan              \n",
      "[[step   258000]]     [[train]]     loss: 0.0730997        [[val]]     loss: 0.38547002       [[val-seq]]     loss: 0.0726903        \n",
      "[[step   259000]]     [[train]]     loss: 0.07265147       [[val]]     loss: 0.3804719        [[val-seq]]     loss: 0.07225596       \n",
      "[[step   260000]]     [[train]]     loss: 0.07274516       [[val]]     loss: 0.38228609       [[val-seq]]     loss: 0.07235702       \n",
      "[[step   261000]]     [[train]]     loss: 0.07267261       [[val]]     loss: 0.37826002       [[val-seq]]     loss: 0.07228582       \n",
      "[[step   262000]]     [[train]]     loss: 0.07227011       [[val]]     loss: 0.38029134       [[val-seq]]     loss: 0.07188401       \n",
      "[[step   263000]]     [[train]]     loss: 0.07224671       [[val]]     loss: 0.38095416       [[val-seq]]     loss: 0.07185858       \n",
      "[[step   264000]]     [[train]]     loss: 0.07241964       [[val]]     loss: 0.38245679       [[val-seq]]     loss: 0.07202964       \n",
      "[[step   265000]]     [[train]]     loss: 0.07229895       [[val]]     loss: 0.37759966       [[val-seq]]     loss: 0.0719101        \n",
      "[[step   266000]]     [[train]]     loss: 0.07214616       [[val]]     loss: 0.3804786        [[val-seq]]     loss: 0.07175817       \n",
      "[[step   267000]]     [[train]]     loss: 0.07277411       [[val]]     loss: 0.38364581       [[val-seq]]     loss: 0.07238288       \n",
      "[[step   268000]]     [[train]]     loss: 0.07260871       [[val]]     loss: 0.3788871        [[val-seq]]     loss: 0.07221516       \n",
      "[[step   269000]]     [[train]]     loss: 0.07218421       [[val]]     loss: 0.37804953       [[val-seq]]     loss: 0.07179019       \n",
      "[[step   270000]]     [[train]]     loss: nan              [[val]]     loss: 0.39263755       [[val-seq]]     loss: nan              \n",
      "[[step   271000]]     [[train]]     loss: 0.07346064       [[val]]     loss: 0.39374454       [[val-seq]]     loss: 0.07307156       \n",
      "[[step   272000]]     [[train]]     loss: 0.07362019       [[val]]     loss: 0.39354947       [[val-seq]]     loss: 0.07324049       \n",
      "[[step   273000]]     [[train]]     loss: 0.07303202       [[val]]     loss: 0.38905667       [[val-seq]]     loss: 0.07265595       \n",
      "[[step   274000]]     [[train]]     loss: 0.07282095       [[val]]     loss: 0.38935325       [[val-seq]]     loss: 0.07244266       \n",
      "[[step   275000]]     [[train]]     loss: 0.07316685       [[val]]     loss: 0.39016058       [[val-seq]]     loss: 0.07278673       \n",
      "[[step   276000]]     [[train]]     loss: 0.0733222        [[val]]     loss: 0.38866236       [[val-seq]]     loss: 0.07294152       \n",
      "[[step   277000]]     [[train]]     loss: 0.07284762       [[val]]     loss: 0.38893769       [[val-seq]]     loss: 0.07246921       \n",
      "[[step   278000]]     [[train]]     loss: 0.07323101       [[val]]     loss: 0.38910345       [[val-seq]]     loss: 0.07285187       \n",
      "[[step   279000]]     [[train]]     loss: 0.07351699       [[val]]     loss: 0.3894891        [[val-seq]]     loss: 0.07313633       \n",
      "[[step   280000]]     [[train]]     loss: 0.07261362       [[val]]     loss: 0.38432814       [[val-seq]]     loss: 0.07222899       \n",
      "[[step   281000]]     [[train]]     loss: 0.0720834        [[val]]     loss: 0.38069546       [[val-seq]]     loss: 0.0716865        \n",
      "[[step   282000]]     [[train]]     loss: 0.0720993        [[val]]     loss: 0.37540837       [[val-seq]]     loss: 0.07169443       \n",
      "[[step   283000]]     [[train]]     loss: 0.07171496       [[val]]     loss: 0.37463152       [[val-seq]]     loss: 0.0712965        \n",
      "[[step   284000]]     [[train]]     loss: 0.07136018       [[val]]     loss: 0.36876676       [[val-seq]]     loss: 0.07091903       \n",
      "[[step   285000]]     [[train]]     loss: 0.07093817       [[val]]     loss: 0.35892776       [[val-seq]]     loss: 0.07047562       \n",
      "[[step   286000]]     [[train]]     loss: 0.07053481       [[val]]     loss: 0.35538524       [[val-seq]]     loss: 0.07003777       \n",
      "[[step   287000]]     [[train]]     loss: nan              [[val]]     loss: nan              [[val-seq]]     loss: nan              \n",
      "[[step   288000]]     [[train]]     loss: 0.07170332       [[val]]     loss: 0.37670017       [[val-seq]]     loss: 0.0712387        \n",
      "[[step   289000]]     [[train]]     loss: 0.07218934       [[val]]     loss: 0.38007524       [[val-seq]]     loss: 0.07172773       \n",
      "[[step   290000]]     [[train]]     loss: 0.07187031       [[val]]     loss: 0.37874833       [[val-seq]]     loss: 0.07141511       \n",
      "[[step   291000]]     [[train]]     loss: 0.07185825       [[val]]     loss: 0.38267218       [[val-seq]]     loss: 0.07140763       \n",
      "[[step   292000]]     [[train]]     loss: 0.07185927       [[val]]     loss: 0.38228538       [[val-seq]]     loss: 0.07141745       \n",
      "[[step   293000]]     [[train]]     loss: 0.07224274       [[val]]     loss: 0.38348962       [[val-seq]]     loss: 0.07181095       \n",
      "[[step   294000]]     [[train]]     loss: 0.07245239       [[val]]     loss: 0.38063062       [[val-seq]]     loss: 0.07203609       \n",
      "[[step   295000]]     [[train]]     loss: 0.07255136       [[val]]     loss: 0.38582681       [[val-seq]]     loss: 0.07215059       \n",
      "[[step   296000]]     [[train]]     loss: 0.07220226       [[val]]     loss: 0.37970115       [[val-seq]]     loss: 0.07181271       \n",
      "[[step   297000]]     [[train]]     loss: 0.0720715        [[val]]     loss: 0.37959359       [[val-seq]]     loss: 0.07168623       \n",
      "[[step   298000]]     [[train]]     loss: 0.07248255       [[val]]     loss: 0.37844569       [[val-seq]]     loss: 0.07210117       \n",
      "[[step   299000]]     [[train]]     loss: 0.071788         [[val]]     loss: 0.37747503       [[val-seq]]     loss: 0.07140397       \n",
      "[[step   300000]]     [[train]]     loss: 0.07160805       [[val]]     loss: 0.37286436       [[val-seq]]     loss: 0.07121384       \n",
      "[[step   301000]]     [[train]]     loss: 0.07130675       [[val]]     loss: 0.36786551       [[val-seq]]     loss: 0.07089444       \n",
      "[[step   302000]]     [[train]]     loss: 0.07081182       [[val]]     loss: 0.36411645       [[val-seq]]     loss: 0.07037212       \n",
      "[[step   303000]]     [[train]]     loss: nan              [[val]]     loss: nan              [[val-seq]]     loss: nan              \n",
      "[[step   304000]]     [[train]]     loss: 0.07278952       [[val]]     loss: 0.3881801        [[val-seq]]     loss: 0.07237928       \n",
      "[[step   305000]]     [[train]]     loss: 0.07258909       [[val]]     loss: 0.38538637       [[val-seq]]     loss: 0.07218392       \n",
      "[[step   306000]]     [[train]]     loss: 0.0726518        [[val]]     loss: 0.38427319       [[val-seq]]     loss: 0.07224864       \n",
      "[[step   307000]]     [[train]]     loss: 0.07276937       [[val]]     loss: 0.38568438       [[val-seq]]     loss: 0.07236317       \n",
      "[[step   308000]]     [[train]]     loss: nan              [[val]]     loss: 0.45124097       [[val-seq]]     loss: nan              \n",
      "[[step   309000]]     [[train]]     loss: 0.08149567       [[val]]     loss: 0.45544958       [[val-seq]]     loss: 0.08097715       \n",
      "[[step   310000]]     [[train]]     loss: 0.08008361       [[val]]     loss: 0.44784475       [[val-seq]]     loss: 0.0796323        \n",
      "[[step   311000]]     [[train]]     loss: 0.07863399       [[val]]     loss: 0.44142071       [[val-seq]]     loss: 0.07820187       \n",
      "[[step   312000]]     [[train]]     loss: nan              [[val]]     loss: 0.4879458        [[val-seq]]     loss: 0.08588632       \n",
      "[[step   313000]]     [[train]]     loss: 0.09108361       [[val]]     loss: 0.51898048       [[val-seq]]     loss: 0.09054102       \n",
      "[[step   314000]]     [[train]]     loss: 0.08926377       [[val]]     loss: 0.50521197       [[val-seq]]     loss: 0.08878316       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[step   315000]]     [[train]]     loss: 0.08707934       [[val]]     loss: 0.49752996       [[val-seq]]     loss: 0.08662464       \n",
      "[[step   316000]]     [[train]]     loss: 0.08562909       [[val]]     loss: 0.4914991        [[val-seq]]     loss: 0.08518977       \n",
      "[[step   317000]]     [[train]]     loss: 0.08444945       [[val]]     loss: 0.4823785        [[val-seq]]     loss: 0.08402412       \n",
      "[[step   318000]]     [[train]]     loss: 0.08262073       [[val]]     loss: 0.47252082       [[val-seq]]     loss: 0.08222297       \n",
      "[[step   319000]]     [[train]]     loss: 0.08089941       [[val]]     loss: 0.46293309       [[val-seq]]     loss: 0.08052511       \n",
      "[[step   320000]]     [[train]]     loss: 0.07987365       [[val]]     loss: 0.45713147       [[val-seq]]     loss: 0.0795128        \n",
      "[[step   321000]]     [[train]]     loss: 0.07835572       [[val]]     loss: 0.44580176       [[val-seq]]     loss: 0.07801548       \n",
      "[[step   322000]]     [[train]]     loss: 0.07720789       [[val]]     loss: 0.43399127       [[val-seq]]     loss: 0.07687441       \n",
      "[[step   323000]]     [[train]]     loss: 0.07589671       [[val]]     loss: 0.42424534       [[val-seq]]     loss: 0.07557034       \n",
      "[[step   324000]]     [[train]]     loss: 0.07547766       [[val]]     loss: 0.41614557       [[val-seq]]     loss: 0.07515313       \n",
      "[[step   325000]]     [[train]]     loss: 0.07430535       [[val]]     loss: 0.4043206        [[val-seq]]     loss: 0.07397433       \n",
      "[[step   326000]]     [[train]]     loss: 0.07309227       [[val]]     loss: 0.39549787       [[val-seq]]     loss: 0.07273872       \n",
      "[[step   327000]]     [[train]]     loss: 0.07233817       [[val]]     loss: 0.38469406       [[val-seq]]     loss: 0.07195296       \n",
      "[[step   328000]]     [[train]]     loss: 0.07161543       [[val]]     loss: 0.37313807       [[val-seq]]     loss: 0.07118475       \n",
      "[[step   329000]]     [[train]]     loss: 0.07085928       [[val]]     loss: 0.36424123       [[val-seq]]     loss: 0.07037609       \n",
      "[[step   330000]]     [[train]]     loss: 0.07040393       [[val]]     loss: 0.35625551       [[val-seq]]     loss: 0.06986048       \n",
      "[[step   331000]]     [[train]]     loss: 0.07038928       [[val]]     loss: 0.3490233        [[val-seq]]     loss: 0.06979          \n",
      "[[step   332000]]     [[train]]     loss: 0.06987213       [[val]]     loss: 0.34900177       [[val-seq]]     loss: 0.06923671       \n",
      "[[step   333000]]     [[train]]     loss: 0.0696717        [[val]]     loss: 0.34678968       [[val-seq]]     loss: 0.06900969       \n",
      "[[step   334000]]     [[train]]     loss: 0.06965684       [[val]]     loss: 0.34636818       [[val-seq]]     loss: 0.06897071       \n",
      "[[step   335000]]     [[train]]     loss: nan              [[val]]     loss: nan              [[val-seq]]     loss: nan              \n",
      "[[step   336000]]     [[train]]     loss: nan              [[val]]     loss: 0.39715115       [[val-seq]]     loss: 0.07347331       \n",
      "[[step   337000]]     [[train]]     loss: 0.07766043       [[val]]     loss: 0.42931585       [[val-seq]]     loss: nan              \n",
      "[[step   338000]]     [[train]]     loss: nan              [[val]]     loss: 0.48947606       [[val-seq]]     loss: 0.0856343        \n",
      "[[step   339000]]     [[train]]     loss: nan              [[val]]     loss: 0.55451951       [[val-seq]]     loss: nan              \n",
      "[[step   340000]]     [[train]]     loss: nan              [[val]]     loss: 0.65674911       [[val-seq]]     loss: nan              \n",
      "[[step   341000]]     [[train]]     loss: 0.11389341       [[val]]     loss: 0.66623547       [[val-seq]]     loss: 0.11324361       \n",
      "[[step   342000]]     [[train]]     loss: nan              [[val]]     loss: 0.66617106       [[val-seq]]     loss: 0.1130544        \n",
      "[[step   343000]]     [[train]]     loss: 0.11200083       [[val]]     loss: 0.65891788       [[val-seq]]     loss: 0.11138729       \n",
      "[[step   344000]]     [[train]]     loss: 0.11005758       [[val]]     loss: 0.6418507        [[val-seq]]     loss: 0.10945632       \n",
      "[[step   345000]]     [[train]]     loss: 0.10635716       [[val]]     loss: 0.62654358       [[val-seq]]     loss: 0.1057759        \n",
      "[[step   346000]]     [[train]]     loss: 0.10339951       [[val]]     loss: 0.60517822       [[val-seq]]     loss: 0.10283193       \n",
      "[[step   347000]]     [[train]]     loss: 0.09982793       [[val]]     loss: 0.58605208       [[val-seq]]     loss: 0.09930999       \n",
      "[[step   348000]]     [[train]]     loss: 0.09825481       [[val]]     loss: 0.57390593       [[val-seq]]     loss: 0.09776913       \n",
      "[[step   349000]]     [[train]]     loss: 0.0961735        [[val]]     loss: 0.55991045       [[val-seq]]     loss: 0.0957236        \n",
      "[[step   350000]]     [[train]]     loss: 0.09514585       [[val]]     loss: 0.55429382       [[val-seq]]     loss: 0.09472135       \n",
      "[[step   351000]]     [[train]]     loss: 0.09445054       [[val]]     loss: 0.54988575       [[val-seq]]     loss: 0.09405657       \n",
      "[[step   352000]]     [[train]]     loss: 0.09370516       [[val]]     loss: 0.54541304       [[val-seq]]     loss: 0.09332172       \n",
      "[[step   353000]]     [[train]]     loss: 0.09189029       [[val]]     loss: 0.5402934        [[val-seq]]     loss: 0.09154392       \n",
      "[[step   354000]]     [[train]]     loss: 0.09170011       [[val]]     loss: 0.53534161       [[val-seq]]     loss: 0.09137014       \n",
      "[[step   355000]]     [[train]]     loss: 0.0911248        [[val]]     loss: 0.52951212       [[val-seq]]     loss: 0.09080403       \n",
      "[[step   356000]]     [[train]]     loss: 0.08588191       [[val]]     loss: 0.50309678       [[val-seq]]     loss: 0.08556116       \n",
      "[[step   357000]]     [[train]]     loss: 0.08397139       [[val]]     loss: 0.49266453       [[val-seq]]     loss: 0.08365914       \n",
      "[[step   358000]]     [[train]]     loss: 0.08428529       [[val]]     loss: 0.4922385        [[val-seq]]     loss: 0.08397347       \n",
      "[[step   359000]]     [[train]]     loss: 0.08372395       [[val]]     loss: 0.4867425        [[val-seq]]     loss: 0.0833834        \n",
      "[[step   360000]]     [[train]]     loss: 0.08289032       [[val]]     loss: 0.48128322       [[val-seq]]     loss: 0.08254192       \n",
      "[[step   361000]]     [[train]]     loss: 0.08207583       [[val]]     loss: 0.47374649       [[val-seq]]     loss: 0.08171249       \n",
      "[[step   362000]]     [[train]]     loss: 0.0815362        [[val]]     loss: 0.46838722       [[val-seq]]     loss: 0.08116355       \n",
      "[[step   363000]]     [[train]]     loss: 0.07954514       [[val]]     loss: 0.45176565       [[val-seq]]     loss: 0.07916635       \n",
      "[[step   364000]]     [[train]]     loss: 0.07690616       [[val]]     loss: 0.43181008       [[val-seq]]     loss: 0.07652378       \n",
      "[[step   365000]]     [[train]]     loss: 0.075308         [[val]]     loss: 0.41332176       [[val-seq]]     loss: 0.07490152       \n",
      "[[step   366000]]     [[train]]     loss: 0.07397727       [[val]]     loss: 0.40028968       [[val-seq]]     loss: 0.07355118       \n",
      "[[step   367000]]     [[train]]     loss: 0.0730161        [[val]]     loss: 0.38267171       [[val-seq]]     loss: 0.07253763       \n",
      "[[step   368000]]     [[train]]     loss: 0.07242181       [[val]]     loss: 0.37612476       [[val-seq]]     loss: 0.07189009       \n",
      "[[step   369000]]     [[train]]     loss: 0.07156883       [[val]]     loss: 0.36790713       [[val-seq]]     loss: 0.07100332       \n",
      "[[step   370000]]     [[train]]     loss: 0.07165919       [[val]]     loss: 0.36162318       [[val-seq]]     loss: 0.07105015       \n",
      "[[step   371000]]     [[train]]     loss: 0.07098997       [[val]]     loss: 0.35899387       [[val-seq]]     loss: 0.07034847       \n",
      "[[step   372000]]     [[train]]     loss: 0.07038796       [[val]]     loss: 0.3515937        [[val-seq]]     loss: 0.06973059       \n",
      "[[step   373000]]     [[train]]     loss: 0.0697122        [[val]]     loss: 0.34750244       [[val-seq]]     loss: 0.06903301       \n",
      "[[step   374000]]     [[train]]     loss: nan              [[val]]     loss: 0.35349613       [[val-seq]]     loss: 0.0694131        \n",
      "[[step   375000]]     [[train]]     loss: 0.07082725       [[val]]     loss: 0.36538467       [[val-seq]]     loss: 0.07037876       \n",
      "[[step   376000]]     [[train]]     loss: 0.07098464       [[val]]     loss: 0.36197965       [[val-seq]]     loss: 0.07053074       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[step   377000]]     [[train]]     loss: 0.07172615       [[val]]     loss: 0.37363766       [[val-seq]]     loss: 0.0712679        \n",
      "[[step   378000]]     [[train]]     loss: 0.0717372        [[val]]     loss: 0.37562885       [[val-seq]]     loss: 0.07129104       \n",
      "[[step   379000]]     [[train]]     loss: 0.07162637       [[val]]     loss: 0.37695455       [[val-seq]]     loss: 0.0711972        \n",
      "[[step   380000]]     [[train]]     loss: 0.07164965       [[val]]     loss: 0.37743896       [[val-seq]]     loss: 0.07123956       \n",
      "[[step   381000]]     [[train]]     loss: 0.07157105       [[val]]     loss: 0.37595168       [[val-seq]]     loss: 0.07117276       \n",
      "[[step   382000]]     [[train]]     loss: 0.0722569        [[val]]     loss: 0.37887946       [[val-seq]]     loss: 0.07187329       \n",
      "[[step   383000]]     [[train]]     loss: 0.07221874       [[val]]     loss: 0.38186655       [[val-seq]]     loss: 0.07184636       \n",
      "[[step   384000]]     [[train]]     loss: 0.07266074       [[val]]     loss: 0.3824928        [[val-seq]]     loss: 0.07229485       \n",
      "[[step   385000]]     [[train]]     loss: 0.07303455       [[val]]     loss: 0.38278993       [[val-seq]]     loss: 0.07268102       \n",
      "[[step   386000]]     [[train]]     loss: 0.07306742       [[val]]     loss: 0.38709476       [[val-seq]]     loss: 0.07272259       \n",
      "[[step   387000]]     [[train]]     loss: 0.07269269       [[val]]     loss: 0.38305805       [[val-seq]]     loss: 0.07234679       \n",
      "[[step   388000]]     [[train]]     loss: 0.07239686       [[val]]     loss: 0.3821939        [[val-seq]]     loss: 0.07204671       \n",
      "[[step   389000]]     [[train]]     loss: 0.07274853       [[val]]     loss: 0.37917667       [[val-seq]]     loss: 0.0723904        \n",
      "[[step   390000]]     [[train]]     loss: 0.07197046       [[val]]     loss: 0.37820853       [[val-seq]]     loss: 0.07159296       \n",
      "[[step   391000]]     [[train]]     loss: 0.07181188       [[val]]     loss: 0.3729482        [[val-seq]]     loss: 0.07140754       \n",
      "[[step   392000]]     [[train]]     loss: 0.0715316        [[val]]     loss: 0.37039988       [[val-seq]]     loss: 0.07108992       \n",
      "[[step   393000]]     [[train]]     loss: 0.07148133       [[val]]     loss: 0.36900525       [[val-seq]]     loss: 0.0710008        \n",
      "[[step   394000]]     [[train]]     loss: 0.07127766       [[val]]     loss: 0.36378719       [[val-seq]]     loss: 0.07076109       \n",
      "[[step   395000]]     [[train]]     loss: 0.07158827       [[val]]     loss: 0.36439852       [[val-seq]]     loss: 0.07103764       \n",
      "[[step   396000]]     [[train]]     loss: 0.07125573       [[val]]     loss: 0.36276628       [[val-seq]]     loss: 0.07067249       \n",
      "[[step   397000]]     [[train]]     loss: 0.07117398       [[val]]     loss: 0.35685487       [[val-seq]]     loss: 0.07056589       \n",
      "[[step   398000]]     [[train]]     loss: 0.07104798       [[val]]     loss: 0.36072365       [[val-seq]]     loss: 0.07042587       \n",
      "[[step   399000]]     [[train]]     loss: 0.07125432       [[val]]     loss: 0.36610677       [[val-seq]]     loss: 0.0706489        \n",
      "[[step   400000]]     [[train]]     loss: 0.07172355       [[val]]     loss: 0.36948737       [[val-seq]]     loss: 0.07111683       \n",
      "[[step   401000]]     [[train]]     loss: 0.07154835       [[val]]     loss: 0.36550297       [[val-seq]]     loss: 0.0709455        \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-27e8e2d42091>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    252\u001b[0m         train_loss,_,np_tensors = session.run(   #changed\n\u001b[0;32m    253\u001b[0m             \u001b[0mfetches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf_tensors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#changed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m         )\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with session.as_default():\n",
    "    debug = False\n",
    "\n",
    "    if warm_start_init_step:\n",
    "        restore(warm_start_init_step)\n",
    "        step_number = warm_start_init_step\n",
    "    else:\n",
    "        session.run(init)\n",
    "        step_number = 0\n",
    "\n",
    "    best_validation_loss, best_validation_tstep = float('inf'), 0\n",
    "    restarts = 0\n",
    "    \n",
    "    train_loss_history = deque(maxlen=loss_averaging_window)\n",
    "    val_loss_history = deque(maxlen=loss_averaging_window)\n",
    "    val_seq_loss_history = deque(maxlen=loss_averaging_window)\n",
    "    \n",
    "    while step_number < num_training_steps:\n",
    "\n",
    "        next_batch =  next(train.next_batch(batch_size))\n",
    "                    \n",
    "        # train step\n",
    "        train_feed_dict = next_batch\n",
    "\n",
    "        # Use n-2 for training so that n-1 can be used for validation\n",
    "\n",
    "        if debug:\n",
    "            print(next_batch[globals()['history_length']])\n",
    "        train_feed_dict.update({globals()['history_length']:train_feed_dict[globals()['history_length']]-1})\n",
    "        \n",
    "        if debug:\n",
    "            print(train_feed_dict[globals()['history_length']])\n",
    "\n",
    "        sequence_lengths = train_feed_dict[globals()['history_length']]\n",
    "        if debug:\n",
    "            print(sequence_lengths)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "        \n",
    "        # Remove the input data for n-1 step for training to avoid leakage\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['NextInOrder_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['NextInOrder_history']:temp_array})\n",
    "            \n",
    "            temp_array = train_feed_dict[globals()['order_number_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['order_number_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['order_hour_of_day_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['order_hour_of_day_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['order_dow_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['order_dow_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['days_since_prior_order_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['days_since_prior_order_history']:temp_array})\n",
    "                       \n",
    "            temp_array = train_feed_dict[globals()['IsInOrder_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['IsInOrder_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['OrderSize_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['OrderSize_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['IndexInOrder_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['IndexInOrder_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['NumProductsFromDep_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['NumProductsFromDep_history']:temp_array})\n",
    "\n",
    "            if False:\n",
    "                temp_array = train_feed_dict[globals()['ProductID1_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID1_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID2_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID2_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID3_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID3_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID4_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID4_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID5_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID5_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID6_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID6_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID7_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID7_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID8_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID8_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID9_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID9_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID10_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID10_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID11_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID11_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID12_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID12_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID13_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID13_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID14_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID14_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID15_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID15_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID16_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID16_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID17_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID17_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID18_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID18_history_a']:temp_array})      \n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID19_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID19_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID20_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID20_history_a']:temp_array})\n",
    "\n",
    "                #temp_array = train_feed_dict[globals()['ProductNameEmbedding_history']]\n",
    "                #temp_array[i,sequence_lengths[i]]=np.zeros(shape=[50],dtype=np.float16)\n",
    "                #train_feed_dict.update({globals()['ProductNameEmbedding_history']:temp_array})\n",
    "            \n",
    "            temp_array = train_feed_dict[globals()['ProductID1_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID1_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID2_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID2_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID3_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID3_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID4_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID4_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID5_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID5_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID6_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID6_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID7_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID7_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID8_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID8_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID9_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID9_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID10_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID10_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID11_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID11_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID12_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID12_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID13_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID13_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID14_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID14_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID15_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID15_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID16_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID16_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID17_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID17_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID18_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID18_history']:temp_array})      \n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID19_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID19_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID20_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID20_history']:temp_array})\n",
    "\n",
    "            \n",
    "            \n",
    "        train_feed_dict.update({learning_rate_var: learning_rate})\n",
    "        train_feed_dict.update({keep_prob: keep_prob_scalar})\n",
    "        train_feed_dict.update({is_training: True})\n",
    "                \n",
    "        debug_dict = {tensor_name: [] for tensor_name in prediction_tensors} #changed\n",
    "        tensor_names, tf_tensors = zip(*prediction_tensors.items()) #changed\n",
    "\n",
    "        train_loss,_,np_tensors = session.run(   #changed\n",
    "            fetches=[loss,step,tf_tensors], #changed\n",
    "            feed_dict=train_feed_dict\n",
    "        )\n",
    "        \n",
    "        for tensor_name, tensor in zip(tensor_names, np_tensors): #changed\n",
    "            debug_dict[tensor_name].append(tensor)#changed \n",
    "        \n",
    "        if debug:\n",
    "            print(debug_dict['history_length'])#changed\n",
    "        \n",
    "        train_loss_history.append(train_loss)\n",
    "        \n",
    "        #BEGIN\n",
    "        prediction_dict = {tensor_name: [] for tensor_name in prediction_tensors}\n",
    "\n",
    "        # test evaluation\n",
    "        \n",
    "        val_feed_dict = next_batch\n",
    "        val_feed_dict.update({learning_rate_var: learning_rate})\n",
    "        val_feed_dict.update({keep_prob: keep_prob_scalar})\n",
    "        val_feed_dict.update({is_training: False})\n",
    "\n",
    "        if debug:\n",
    "            print(val_feed_dict[globals()['history_length']]+1)\n",
    "        \n",
    "        tensor_names, tf_tensors = zip(*prediction_tensors.items())\n",
    "        val_seq_loss,np_tensors = session.run(\n",
    "            fetches=[loss,tf_tensors],\n",
    "            feed_dict=val_feed_dict\n",
    "        )\n",
    "\n",
    "        for tensor_name, tensor in zip(tensor_names, np_tensors):\n",
    "            prediction_dict[tensor_name].append(tensor)\n",
    "      \n",
    "        val_loss = log_loss(prediction_dict['ground_truth'][0],prediction_dict['predictions'][0])\n",
    "        #END\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_seq_loss_history.append(val_seq_loss)\n",
    "        \n",
    "        if debug:\n",
    "            #print(train_loss, val_loss)\n",
    "            #print(prediction_dict['ground_truth'])\n",
    "            print(prediction_dict['history_length'])\n",
    "            break\n",
    "\n",
    "        if step_number % log_interval == 0:\n",
    "            avg_train_loss = sum(train_loss_history)/len(train_loss_history)\n",
    "            avg_val_loss = sum(val_loss_history)/len(val_loss_history)\n",
    "            avg_val_seq_loss = sum(val_seq_loss_history)/len(val_seq_loss_history)\n",
    "            metric_log = (\n",
    "                \"[[step {:>8}]]     \"\n",
    "                \"[[train]]     loss: {:<12}     \"\n",
    "                \"[[val]]     loss: {:<12}     \"\n",
    "                \"[[val-seq]]     loss: {:<12}     \"\n",
    "            ).format(step_number, round(avg_train_loss, 8), round(avg_val_loss, 8), round(avg_val_seq_loss, 8))\n",
    "            print(metric_log)\n",
    "            \n",
    "            if avg_val_loss < best_validation_loss:\n",
    "                best_validation_loss = avg_val_loss\n",
    "                best_validation_tstep = step_number\n",
    "                if step_number > min_steps_to_checkpoint:\n",
    "                    model_path = os.path.join(checkpoint_dir, 'model{}.ckpt'.format(best_validation_tstep))\n",
    "                    saver.save(session, model_path, global_step=best_validation_tstep) \n",
    "                    \n",
    "            if step_number - best_validation_tstep > early_stopping_steps:\n",
    "\n",
    "                if num_restarts is None or restarts >= num_restarts:\n",
    "                    print('best validation loss of {} at training step {}'.format(\n",
    "                        best_validation_loss, best_validation_tstep))\n",
    "                    print('early stopping - ending training.')\n",
    "                    break\n",
    "\n",
    "                if restarts < num_restarts:\n",
    "                    model_path = os.path.join(checkpoint_dir, 'model{}.ckpt-{}'.format(best_validation_tstep,best_validation_tstep))\n",
    "                    saver.restore(session, model_path)\n",
    "                    print('halving learning rate')\n",
    "                    learning_rate /= 2.0\n",
    "                    early_stopping_steps /= 2\n",
    "                    step_number = best_validation_tstep\n",
    "                    restarts += 1\n",
    "\n",
    "        step_number += 1\n",
    "\n",
    "    if step_number <= min_steps_to_checkpoint:\n",
    "        best_validation_tstep = step_number\n",
    "        model_path = os.path.join(checkpoint_dir, 'model{}.ckpt'.format(best_validation_tstep))\n",
    "        saver.save(session, model_path, global_step=best_validation_tstep) \n",
    "                    \n",
    "    print('num_training_steps reached - ending training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3463681830049152, 334000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validation_loss, best_validation_tstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\checkpoints\\\\model3463334000.ckpt-334000'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join(checkpoint_dir, 'model3463{}.ckpt'.format(best_validation_tstep))\n",
    "model_path\n",
    "saver.save(session, model_path, global_step=best_validation_tstep) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "N = 100\n",
    "debug = False\n",
    "batch_size = 128\n",
    "num_training_steps = 300000\n",
    "learning_rate = 0.001\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 35000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 5000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "#dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "#filter_widths=filter_widths+filter_widths\n",
    "skip_channels=64\n",
    "residual_channels=96\n",
    "\n",
    "trainable parameters:\n",
    "[('aisle_embeddings:0', [135, 50]), ('dept_embeddings:0', [22, 10]), ('user_embeddings:0', [207000, 50]), ('lstm1/rnn/lstm_cell/kernel:0', [1754, 1200]), ('lstm1/rnn/lstm_cell/bias:0', [1200]), ('wavenet/x-proj/weights:0', [1454, 96]), ('wavenet/x-proj/biases:0', [96]), ('wavenet/cnn-0/weights:0', [2, 96, 192]), ('wavenet/cnn-0/biases:0', [192]), ('wavenet/cnn-0-proj/weights:0', [96, 160]), ('wavenet/cnn-0-proj/biases:0', [160]), ('wavenet/cnn-1/weights:0', [2, 96, 192]), ('wavenet/cnn-1/biases:0', [192]), ('wavenet/cnn-1-proj/weights:0', [96, 160]), ('wavenet/cnn-1-proj/biases:0', [160]), ('wavenet/cnn-2/weights:0', [2, 96, 192]), ('wavenet/cnn-2/biases:0', [192]), ('wavenet/cnn-2-proj/weights:0', [96, 160]), ('wavenet/cnn-2-proj/biases:0', [160]), ('wavenet/cnn-3/weights:0', [2, 96, 192]), ('wavenet/cnn-3/biases:0', [192]), ('wavenet/cnn-3-proj/weights:0', [96, 160]), ('wavenet/cnn-3-proj/biases:0', [160]), ('wavenet/cnn-4/weights:0', [2, 96, 192]), ('wavenet/cnn-4/biases:0', [192]), ('wavenet/cnn-4-proj/weights:0', [96, 160]), ('wavenet/cnn-4-proj/biases:0', [160]), ('wavenet/cnn-5/weights:0', [2, 96, 192]), ('wavenet/cnn-5/biases:0', [192]), ('wavenet/cnn-5-proj/weights:0', [96, 160]), ('wavenet/cnn-5-proj/biases:0', [160]), ('dense1/weights:0', [794, 50]), ('dense1/biases:0', [50]), ('dense2/weights:0', [50, 1]), ('dense2/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "12957907\n",
    "built graph\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.79283637       [[val]]     loss: 0.60624765       [[val-seq]]     loss: 0.59424144       \n",
    "[[step     1000]]     [[train]]     loss: 0.38630329       [[val]]     loss: 0.43172749       [[val-seq]]     loss: 0.38367621       \n",
    "[[step     2000]]     [[train]]     loss: 0.37829497       [[val]]     loss: 0.42406548       [[val-seq]]     loss: 0.37603256       \n",
    "[[step     3000]]     [[train]]     loss: 0.37626992       [[val]]     loss: 0.42579533       [[val-seq]]     loss: 0.37400462       \n",
    "[[step     4000]]     [[train]]     loss: 0.37561555       [[val]]     loss: 0.42661017       [[val-seq]]     loss: 0.37342759       \n",
    "[[step     5000]]     [[train]]     loss: 0.37550599       [[val]]     loss: 0.42605413       [[val-seq]]     loss: 0.37332096       \n",
    "[[step     6000]]     [[train]]     loss: 0.37529765       [[val]]     loss: 0.42040225       [[val-seq]]     loss: 0.37305216       \n",
    "[[step     7000]]     [[train]]     loss: 0.37498146       [[val]]     loss: 0.42369202       [[val-seq]]     loss: 0.3727545        \n",
    "[[step     8000]]     [[train]]     loss: 0.37255951       [[val]]     loss: 0.42120498       [[val-seq]]     loss: 0.37030773       \n",
    "[[step     9000]]     [[train]]     loss: 0.37495045       [[val]]     loss: 0.42352376       [[val-seq]]     loss: 0.3726997        \n",
    "[[step    10000]]     [[train]]     loss: 0.3718529        [[val]]     loss: 0.42546232       [[val-seq]]     loss: 0.36962371       \n",
    "[[step    11000]]     [[train]]     loss: 0.37165166       [[val]]     loss: 0.42362015       [[val-seq]]     loss: 0.36943411       \n",
    "[[step    12000]]     [[train]]     loss: 0.37223182       [[val]]     loss: 0.42399921       [[val-seq]]     loss: 0.36999945       \n",
    "[[step    13000]]     [[train]]     loss: 0.37197555       [[val]]     loss: 0.42375858       [[val-seq]]     loss: 0.36979175       \n",
    "[[step    14000]]     [[train]]     loss: 0.3705603        [[val]]     loss: 0.42247732       [[val-seq]]     loss: 0.36836915       \n",
    "[[step    15000]]     [[train]]     loss: 0.37174754       [[val]]     loss: 0.42055816       [[val-seq]]     loss: 0.36956906       \n",
    "[[step    16000]]     [[train]]     loss: 0.36986868       [[val]]     loss: 0.4280738        [[val-seq]]     loss: 0.36770084       \n",
    "[[step    17000]]     [[train]]     loss: 0.36993722       [[val]]     loss: 0.42495414       [[val-seq]]     loss: 0.36776946       \n",
    "[[step    18000]]     [[train]]     loss: 0.36878239       [[val]]     loss: 0.42167605       [[val-seq]]     loss: 0.3666584        \n",
    "[[step    19000]]     [[train]]     loss: 0.36844754       [[val]]     loss: 0.42399498       [[val-seq]]     loss: 0.36627883       \n",
    "[[step    20000]]     [[train]]     loss: 0.36800708       [[val]]     loss: 0.42623461       [[val-seq]]     loss: 0.36582198       \n",
    "[[step    21000]]     [[train]]     loss: 0.36885805       [[val]]     loss: 0.42304146       [[val-seq]]     loss: 0.36672158       \n",
    "[[step    22000]]     [[train]]     loss: 0.36758876       [[val]]     loss: 0.42467972       [[val-seq]]     loss: 0.36545116       \n",
    "[[step    23000]]     [[train]]     loss: 0.36837649       [[val]]     loss: 0.42312657       [[val-seq]]     loss: 0.36626064       \n",
    "[[step    24000]]     [[train]]     loss: 0.36790891       [[val]]     loss: 0.42422104       [[val-seq]]     loss: 0.365809         \n",
    "[[step    25000]]     [[train]]     loss: 0.36705284       [[val]]     loss: 0.4248171        [[val-seq]]     loss: 0.36494182       \n",
    "[[step    26000]]     [[train]]     loss: 0.36734557       [[val]]     loss: 0.42304009       [[val-seq]]     loss: 0.36525052       \n",
    "[[step    27000]]     [[train]]     loss: 0.36714028       [[val]]     loss: 0.42480437       [[val-seq]]     loss: 0.36501321       \n",
    "[[step    28000]]     [[train]]     loss: 0.36800031       [[val]]     loss: 0.42523621       [[val-seq]]     loss: 0.36589179       \n",
    "[[step    29000]]     [[train]]     loss: 0.36751857       [[val]]     loss: 0.42673968       [[val-seq]]     loss: 0.36540843       \n",
    "[[step    30000]]     [[train]]     loss: 0.36729939       [[val]]     loss: 0.42805389       [[val-seq]]     loss: 0.36519097       \n",
    "[[step    31000]]     [[train]]     loss: 0.3658448        [[val]]     loss: 0.4232727        [[val-seq]]     loss: 0.36377487       \n",
    "[[step    32000]]     [[train]]     loss: 0.36803557       [[val]]     loss: 0.42449223       [[val-seq]]     loss: 0.36593829       \n",
    "[[step    33000]]     [[train]]     loss: 0.3669557        [[val]]     loss: 0.42459368       [[val-seq]]     loss: 0.36485877       \n",
    "[[step    34000]]     [[train]]     loss: 0.36828809       [[val]]     loss: 0.42800068       [[val-seq]]     loss: 0.36620096       \n",
    "[[step    35000]]     [[train]]     loss: 0.36536645       [[val]]     loss: 0.4269611        [[val-seq]]     loss: 0.36324265       \n",
    "[[step    36000]]     [[train]]     loss: 0.36635399       [[val]]     loss: 0.42869828       [[val-seq]]     loss: 0.36428317       \n",
    "[[step    37000]]     [[train]]     loss: 0.36469545       [[val]]     loss: 0.4243258        [[val-seq]]     loss: 0.36259389       \n",
    "[[step    38000]]     [[train]]     loss: 0.36579357       [[val]]     loss: 0.42807848       [[val-seq]]     loss: 0.36372007       \n",
    "[[step    39000]]     [[train]]     loss: 0.36539431       [[val]]     loss: 0.42909807       [[val-seq]]     loss: 0.36330017       \n",
    "[[step    40000]]     [[train]]     loss: 0.36616532       [[val]]     loss: 0.42908202       [[val-seq]]     loss: 0.36409379       \n",
    "[[step    41000]]     [[train]]     loss: 0.36390483       [[val]]     loss: 0.43058777       [[val-seq]]     loss: 0.36182936       \n",
    "[[step    42000]]     [[train]]     loss: 0.36610457       [[val]]     loss: 0.43106332       [[val-seq]]     loss: 0.36402831       \n",
    "best validation loss of 0.4204022465360795 at training step 6000\n",
    "early stopping - ending training.\n",
    "num_training_steps reached - ending training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_validation_loss, best_validation_tstep\n",
    "(0.4209459807394815, 7000)\n",
    "\n",
    "'C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\checkpoints\\\\model42097000.ckpt-7000'\n",
    "\n",
    "path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\'\n",
    "#path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "N = 100\n",
    "debug = False\n",
    "batch_size = 128\n",
    "num_training_steps = 300000\n",
    "learning_rate = 0.001\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 100000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 35000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = False\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "#dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "#filter_widths=filter_widths+filter_widths\n",
    "skip_channels=64\n",
    "residual_channels=96\n",
    "\n",
    "prediction_dir = path_out + \"predictions\"\n",
    "checkpoint_dir = path_out + \"checkpoints\"\n",
    "\n",
    "if not os.path.isdir(prediction_dir):\n",
    "    os.makedirs(prediction_dir)\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "trainable parameters:\n",
    "[('aisle_embeddings:0', [135, 50]), ('dept_embeddings:0', [22, 10]), ('user_embeddings:0', [207000, 300]), ('lstm1/rnn/lstm_cell/kernel:0', [1004, 1200]), ('lstm1/rnn/lstm_cell/bias:0', [1200]), ('dense1/weights:0', [1004, 50]), ('dense1/biases:0', [50]), ('dense2/weights:0', [50, 1]), ('dense2/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "63363271\n",
    "built graph\n",
    "\n",
    "\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.72577935       [[val]]     loss: 0.65642903       [[val-seq]]     loss: 0.64681882       \n",
    "[[step     1000]]     [[train]]     loss: 0.38667929       [[val]]     loss: 0.43641902       [[val-seq]]     loss: 0.38249544       \n",
    "[[step     2000]]     [[train]]     loss: 0.37966205       [[val]]     loss: 0.42781433       [[val-seq]]     loss: 0.37516639       \n",
    "[[step     3000]]     [[train]]     loss: 0.37733739       [[val]]     loss: 0.4234371        [[val-seq]]     loss: 0.37319629       \n",
    "[[step     4000]]     [[train]]     loss: 0.3772767        [[val]]     loss: 0.42816841       [[val-seq]]     loss: 0.37338137       \n",
    "[[step     5000]]     [[train]]     loss: 0.37526072       [[val]]     loss: 0.42579922       [[val-seq]]     loss: 0.37156339       \n",
    "[[step     6000]]     [[train]]     loss: 0.37493415       [[val]]     loss: 0.42703961       [[val-seq]]     loss: 0.37122898       \n",
    "[[step     7000]]     [[train]]     loss: 0.37254057       [[val]]     loss: 0.42094598       [[val-seq]]     loss: 0.36901362       \n",
    "[[step     8000]]     [[train]]     loss: 0.37242638       [[val]]     loss: 0.42468829       [[val-seq]]     loss: 0.36893474       \n",
    "[[step     9000]]     [[train]]     loss: 0.37198956       [[val]]     loss: 0.42421385       [[val-seq]]     loss: 0.3684996        \n",
    "[[step    10000]]     [[train]]     loss: 0.37027625       [[val]]     loss: 0.42510002       [[val-seq]]     loss: 0.36685655       \n",
    "[[step    11000]]     [[train]]     loss: 0.37190998       [[val]]     loss: 0.42655317       [[val-seq]]     loss: 0.36852493       \n",
    "[[step    12000]]     [[train]]     loss: 0.37150671       [[val]]     loss: 0.42382367       [[val-seq]]     loss: 0.36818472       \n",
    "[[step    13000]]     [[train]]     loss: 0.37044208       [[val]]     loss: 0.42254823       [[val-seq]]     loss: 0.36713486       \n",
    "[[step    14000]]     [[train]]     loss: 0.36938445       [[val]]     loss: 0.42609243       [[val-seq]]     loss: 0.36610656       \n",
    "[[step    15000]]     [[train]]     loss: 0.36897956       [[val]]     loss: 0.42274822       [[val-seq]]     loss: 0.36578495       \n",
    "[[step    16000]]     [[train]]     loss: 0.36960152       [[val]]     loss: 0.42546837       [[val-seq]]     loss: 0.36636248       \n",
    "[[step    17000]]     [[train]]     loss: 0.37039724       [[val]]     loss: 0.42589744       [[val-seq]]     loss: 0.36719882       \n",
    "[[step    18000]]     [[train]]     loss: 0.36984019       [[val]]     loss: 0.42690554       [[val-seq]]     loss: 0.36665125       \n",
    "[[step    19000]]     [[train]]     loss: 0.36908674       [[val]]     loss: 0.42684426       [[val-seq]]     loss: 0.36590971       \n",
    "[[step    20000]]     [[train]]     loss: 0.36816688       [[val]]     loss: 0.42691754       [[val-seq]]     loss: 0.36501191       \n",
    "[[step    21000]]     [[train]]     loss: 0.36886372       [[val]]     loss: 0.42378894       [[val-seq]]     loss: 0.36572918       \n",
    "[[step    22000]]     [[train]]     loss: 0.36979933       [[val]]     loss: 0.42465362       [[val-seq]]     loss: 0.36664639       \n",
    "[[step    23000]]     [[train]]     loss: 0.3685043        [[val]]     loss: 0.42778233       [[val-seq]]     loss: 0.36539656       \n",
    "[[step    24000]]     [[train]]     loss: 0.3676134        [[val]]     loss: 0.42729685       [[val-seq]]     loss: 0.36449743       \n",
    "[[step    25000]]     [[train]]     loss: 0.36701723       [[val]]     loss: 0.42717593       [[val-seq]]     loss: 0.36392438       \n",
    "[[step    26000]]     [[train]]     loss: 0.36569478       [[val]]     loss: 0.42564435       [[val-seq]]     loss: 0.36264328       \n",
    "[[step    27000]]     [[train]]     loss: 0.36755869       [[val]]     loss: 0.43127325       [[val-seq]]     loss: 0.36444162       \n",
    "[[step    28000]]     [[train]]     loss: 0.36827562       [[val]]     loss: 0.43043082       [[val-seq]]     loss: 0.36519178       \n",
    "[[step    29000]]     [[train]]     loss: 0.36570856       [[val]]     loss: 0.42953626       [[val-seq]]     loss: 0.36263079       \n",
    "[[step    30000]]     [[train]]     loss: 0.36546884       [[val]]     loss: 0.43025903       [[val-seq]]     loss: 0.36240199       \n",
    "[[step    31000]]     [[train]]     loss: 0.36596183       [[val]]     loss: 0.43252584       [[val-seq]]     loss: 0.3629174        \n",
    "[[step    32000]]     [[train]]     loss: 0.3668334        [[val]]     loss: 0.4294476        [[val-seq]]     loss: 0.36375651       \n",
    "[[step    33000]]     [[train]]     loss: 0.36659546       [[val]]     loss: 0.42983575       [[val-seq]]     loss: 0.36353752       \n",
    "[[step    34000]]     [[train]]     loss: 0.36507575       [[val]]     loss: 0.43280031       [[val-seq]]     loss: 0.36204965       \n",
    "[[step    35000]]     [[train]]     loss: 0.36463612       [[val]]     loss: 0.43231235       [[val-seq]]     loss: 0.36161446       \n",
    "[[step    36000]]     [[train]]     loss: 0.36568933       [[val]]     loss: 0.42955957       [[val-seq]]     loss: 0.36262748       \n",
    "[[step    37000]]     [[train]]     loss: 0.36467638       [[val]]     loss: 0.43380134       [[val-seq]]     loss: 0.36163716       \n",
    "[[step    38000]]     [[train]]     loss: 0.36381822       [[val]]     loss: 0.43073477       [[val-seq]]     loss: 0.36078022       \n",
    "[[step    39000]]     [[train]]     loss: 0.36461969       [[val]]     loss: 0.43428645       [[val-seq]]     loss: 0.36154758       \n",
    "[[step    40000]]     [[train]]     loss: 0.36492028       [[val]]     loss: 0.43434485       [[val-seq]]     loss: 0.36185018       \n",
    "[[step    41000]]     [[train]]     loss: 0.3641035        [[val]]     loss: 0.42990791       [[val-seq]]     loss: 0.36107208       \n",
    "[[step    42000]]     [[train]]     loss: 0.36516719       [[val]]     loss: 0.43478644       [[val-seq]]     loss: 0.36213557       \n",
    "[[step    43000]]     [[train]]     loss: 0.36548409       [[val]]     loss: 0.4323879        [[val-seq]]     loss: 0.36246711       \n",
    "[[step    44000]]     [[train]]     loss: 0.3638115        [[val]]     loss: 0.43335473       [[val-seq]]     loss: 0.3607743        \n",
    "[[step    45000]]     [[train]]     loss: 0.36366313       [[val]]     loss: 0.43759156       [[val-seq]]     loss: 0.36058636       \n",
    "[[step    46000]]     [[train]]     loss: 0.36283449       [[val]]     loss: 0.43843624       [[val-seq]]     loss: 0.35955424       \n",
    "[[step    47000]]     [[train]]     loss: 0.35964085       [[val]]     loss: 0.44596492       [[val-seq]]     loss: 0.35584711       \n",
    "[[step    48000]]     [[train]]     loss: 0.35961326       [[val]]     loss: 0.45195176       [[val-seq]]     loss: 0.35534527       \n",
    "[[step    49000]]     [[train]]     loss: 0.35746261       [[val]]     loss: 0.45494797       [[val-seq]]     loss: 0.35279568       \n",
    "[[step    50000]]     [[train]]     loss: 0.35759593       [[val]]     loss: 0.46060267       [[val-seq]]     loss: 0.35289372       \n",
    "[[step    51000]]     [[train]]     loss: 0.35635811       [[val]]     loss: 0.45780343       [[val-seq]]     loss: 0.35152714       \n",
    "[[step    52000]]     [[train]]     loss: 0.35471936       [[val]]     loss: 0.46360691       [[val-seq]]     loss: 0.34961522       \n",
    "[[step    53000]]     [[train]]     loss: 0.35465831       [[val]]     loss: 0.45896441       [[val-seq]]     loss: 0.3496902        \n",
    "[[step    54000]]     [[train]]     loss: 0.35495829       [[val]]     loss: 0.46346198       [[val-seq]]     loss: 0.34972423       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\'\n",
    "#path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "N = 100\n",
    "debug = False\n",
    "batch_size = 128\n",
    "num_training_steps = 300000\n",
    "learning_rate = 0.001\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 100000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 35000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 1024\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "#dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "#filter_widths=filter_widths+filter_widths\n",
    "skip_channels=96\n",
    "residual_channels=124\n",
    "\n",
    "trainable parameters:\n",
    "[('aisle_embeddings:0', [135, 50]), ('dept_embeddings:0', [22, 10]), ('user_embeddings:0', [207000, 300]), ('lstm1/rnn/lstm_cell/kernel:0', [2728, 4096]), ('lstm1/rnn/lstm_cell/bias:0', [4096]), ('wavenet/x-proj/weights:0', [1704, 124]), ('wavenet/x-proj/biases:0', [124]), ('wavenet/cnn-0/weights:0', [2, 124, 248]), ('wavenet/cnn-0/biases:0', [248]), ('wavenet/cnn-0-proj/weights:0', [124, 220]), ('wavenet/cnn-0-proj/biases:0', [220]), ('wavenet/cnn-1/weights:0', [2, 124, 248]), ('wavenet/cnn-1/biases:0', [248]), ('wavenet/cnn-1-proj/weights:0', [124, 220]), ('wavenet/cnn-1-proj/biases:0', [220]), ('wavenet/cnn-2/weights:0', [2, 124, 248]), ('wavenet/cnn-2/biases:0', [248]), ('wavenet/cnn-2-proj/weights:0', [124, 220]), ('wavenet/cnn-2-proj/biases:0', [220]), ('wavenet/cnn-3/weights:0', [2, 124, 248]), ('wavenet/cnn-3/biases:0', [248]), ('wavenet/cnn-3-proj/weights:0', [124, 220]), ('wavenet/cnn-3-proj/biases:0', [220]), ('wavenet/cnn-4/weights:0', [2, 124, 248]), ('wavenet/cnn-4/biases:0', [248]), ('wavenet/cnn-4-proj/weights:0', [124, 220]), ('wavenet/cnn-4-proj/biases:0', [220]), ('wavenet/cnn-5/weights:0', [2, 124, 248]), ('wavenet/cnn-5/biases:0', [248]), ('wavenet/cnn-5-proj/weights:0', [124, 220]), ('wavenet/cnn-5-proj/biases:0', [220]), ('dense1/weights:0', [3304, 50]), ('dense1/biases:0', [50]), ('dense2/weights:0', [50, 1]), ('dense2/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "74197187\n",
    "built graph\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.70939094       [[val]]     loss: 0.41096521       \n",
    "[[step     1000]]     [[train]]     loss: 0.38642146       [[val]]     loss: 0.43758895       \n",
    "[[step     2000]]     [[train]]     loss: 0.37751848       [[val]]     loss: 0.42608113       \n",
    "[[step     3000]]     [[train]]     loss: 0.37805695       [[val]]     loss: 0.42838143       \n",
    "[[step     4000]]     [[train]]     loss: 0.3765701        [[val]]     loss: 0.424398         \n",
    "[[step     5000]]     [[train]]     loss: 0.37562265       [[val]]     loss: 0.42579731       \n",
    "[[step     6000]]     [[train]]     loss: 0.37614318       [[val]]     loss: 0.42438959       \n",
    "[[step     7000]]     [[train]]     loss: 0.37442617       [[val]]     loss: 0.42294047       \n",
    "[[step     8000]]     [[train]]     loss: 0.37305277       [[val]]     loss: 0.42525389       \n",
    "[[step     9000]]     [[train]]     loss: 0.37332716       [[val]]     loss: 0.42628442       \n",
    "[[step    10000]]     [[train]]     loss: 0.37205556       [[val]]     loss: 0.42547762       \n",
    "[[step    11000]]     [[train]]     loss: 0.37247341       [[val]]     loss: 0.42411674       \n",
    "[[step    12000]]     [[train]]     loss: 0.371267         [[val]]     loss: 0.42414662       \n",
    "[[step    13000]]     [[train]]     loss: 0.37237499       [[val]]     loss: 0.42269402       \n",
    "[[step    14000]]     [[train]]     loss: 0.37124519       [[val]]     loss: 0.42742674       \n",
    "[[step    15000]]     [[train]]     loss: 0.37024065       [[val]]     loss: 0.42706169       \n",
    "[[step    16000]]     [[train]]     loss: 0.36951248       [[val]]     loss: 0.42313572       \n",
    "[[step    17000]]     [[train]]     loss: 0.36920308       [[val]]     loss: 0.42702655       \n",
    "[[step    18000]]     [[train]]     loss: 0.36890243       [[val]]     loss: 0.42479633       \n",
    "[[step    19000]]     [[train]]     loss: 0.36838289       [[val]]     loss: 0.42760042       \n",
    "[[step    20000]]     [[train]]     loss: 0.36812239       [[val]]     loss: 0.42703578       \n",
    "[[step    21000]]     [[train]]     loss: 0.36878137       [[val]]     loss: 0.42523869       \n",
    "[[step    22000]]     [[train]]     loss: 0.3681644        [[val]]     loss: 0.42360044       \n",
    "[[step    23000]]     [[train]]     loss: 0.36752876       [[val]]     loss: 0.42814147       \n",
    "[[step    24000]]     [[train]]     loss: 0.36667018       [[val]]     loss: 0.42329501       \n",
    "[[step    25000]]     [[train]]     loss: 0.36729296       [[val]]     loss: 0.4259387        \n",
    "[[step    26000]]     [[train]]     loss: 0.36842091       [[val]]     loss: 0.42790254       \n",
    "[[step    27000]]     [[train]]     loss: 0.36803813       [[val]]     loss: 0.427691         \n",
    "[[step    28000]]     [[train]]     loss: 0.36773647       [[val]]     loss: 0.43003596       \n",
    "[[step    29000]]     [[train]]     loss: 0.36623077       [[val]]     loss: 0.42828908       \n",
    "[[step    30000]]     [[train]]     loss: 0.36628201       [[val]]     loss: 0.42338908       \n",
    "[[step    31000]]     [[train]]     loss: 0.36904426       [[val]]     loss: 0.427335         \n",
    "[[step    32000]]     [[train]]     loss: 0.36725425       [[val]]     loss: 0.42920808       \n",
    "[[step    33000]]     [[train]]     loss: 0.367116         [[val]]     loss: 0.42978597       \n",
    "[[step    34000]]     [[train]]     loss: 0.36501117       [[val]]     loss: 0.42692595       \n",
    "[[step    35000]]     [[train]]     loss: 0.36478453       [[val]]     loss: 0.42930666       \n",
    "[[step    36000]]     [[train]]     loss: 0.36557198       [[val]]     loss: 0.42938079       \n",
    "[[step    37000]]     [[train]]     loss: 0.36597545       [[val]]     loss: 0.43095312       \n",
    "[[step    38000]]     [[train]]     loss: 0.36541395       [[val]]     loss: 0.43255321       \n",
    "[[step    39000]]     [[train]]     loss: 0.36410409       [[val]]     loss: 0.43244085       \n",
    "[[step    40000]]     [[train]]     loss: 0.36610092       [[val]]     loss: 0.43255489       \n",
    "[[step    41000]]     [[train]]     loss: 0.36416895       [[val]]     loss: 0.43403675       \n",
    "[[step    42000]]     [[train]]     loss: 0.365641         [[val]]     loss: 0.4310425        \n",
    "[[step    43000]]     [[train]]     loss: 0.3640731        [[val]]     loss: 0.431747         \n",
    "[[step    44000]]     [[train]]     loss: 0.36397421       [[val]]     loss: 0.43372008       \n",
    "[[step    45000]]     [[train]]     loss: 0.36501272       [[val]]     loss: 0.4310498        \n",
    "[[step    46000]]     [[train]]     loss: 0.36092192       [[val]]     loss: 0.43643077       \n",
    "[[step    47000]]     [[train]]     loss: 0.36079551       [[val]]     loss: 0.44190613       \n",
    "[[step    48000]]     [[train]]     loss: 0.35971587       [[val]]     loss: 0.4499071        \n",
    "[[step    49000]]     [[train]]     loss: 0.35865844       [[val]]     loss: 0.45116917       \n",
    "[[step    50000]]     [[train]]     loss: 0.35887549       [[val]]     loss: 0.45157063       \n",
    "[[step    51000]]     [[train]]     loss: 0.35677896       [[val]]     loss: 0.45403695       \n",
    "[[step    52000]]     [[train]]     loss: 0.35802961       [[val]]     loss: 0.45085482       \n",
    "[[step    53000]]     [[train]]     loss: 0.35708232       [[val]]     loss: 0.45648349       \n",
    "[[step    54000]]     [[train]]     loss: 0.35589415       [[val]]     loss: 0.45895067       \n",
    "[[step    55000]]     [[train]]     loss: 0.35592514       [[val]]     loss: 0.45281368       \n",
    "[[step    56000]]     [[train]]     loss: 0.3568274        [[val]]     loss: 0.45942318       \n",
    "[[step    57000]]     [[train]]     loss: 0.35511323       [[val]]     loss: 0.4577161        \n",
    "[[step    58000]]     [[train]]     loss: 0.35569286       [[val]]     loss: 0.46056          \n",
    "[[step    59000]]     [[train]]     loss: 0.35638797       [[val]]     loss: 0.4578978        \n",
    "[[step    60000]]     [[train]]     loss: 0.35509676       [[val]]     loss: 0.46222879       \n",
    "[[step    61000]]     [[train]]     loss: 0.35391866       [[val]]     loss: 0.45974356       \n",
    "[[step    62000]]     [[train]]     loss: 0.35398123       [[val]]     loss: 0.46046257       \n",
    "[[step    63000]]     [[train]]     loss: 0.35339813       [[val]]     loss: 0.46283644       \n",
    "[[step    64000]]     [[train]]     loss: 0.35366576       [[val]]     loss: 0.46394283       \n",
    "[[step    65000]]     [[train]]     loss: 0.3531314        [[val]]     loss: 0.46213683       \n",
    "[[step    66000]]     [[train]]     loss: 0.35327068       [[val]]     loss: 0.46291079       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\'\n",
    "path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "debug = False\n",
    "batch_size = 512\n",
    "num_training_steps = 200000\n",
    "learning_rate = 0.001\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 100000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 100000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "#dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "#filter_widths=filter_widths+filter_widths\n",
    "skip_channels=124\n",
    "residual_channels=256\n",
    "\n",
    "prediction_dir = path_out + \"predictions\"\n",
    "checkpoint_dir = path_out + \"checkpoints\"\n",
    "\n",
    "if not os.path.isdir(prediction_dir):\n",
    "    os.makedirs(prediction_dir)\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "trainable parameters:\n",
    "[(u'aisle_embeddings:0', [135, 10]), (u'dept_embeddings:0', [22, 5]), (u'user_embeddings:0', [207000, 10]), (u'wavenet/x-proj/weights:0', [569, 256]), (u'wavenet/x-proj/biases:0', [256]), (u'wavenet/cnn-0/weights:0', [2, 256, 512]), (u'wavenet/cnn-0/biases:0', [512]), (u'wavenet/cnn-0-proj/weights:0', [256, 380]), (u'wavenet/cnn-0-proj/biases:0', [380]), (u'wavenet/cnn-1/weights:0', [2, 256, 512]), (u'wavenet/cnn-1/biases:0', [512]), (u'wavenet/cnn-1-proj/weights:0', [256, 380]), (u'wavenet/cnn-1-proj/biases:0', [380]), (u'wavenet/cnn-2/weights:0', [2, 256, 512]), (u'wavenet/cnn-2/biases:0', [512]), (u'wavenet/cnn-2-proj/weights:0', [256, 380]), (u'wavenet/cnn-2-proj/biases:0', [380]), (u'wavenet/cnn-3/weights:0', [2, 256, 512]), (u'wavenet/cnn-3/biases:0', [512]), (u'wavenet/cnn-3-proj/weights:0', [256, 380]), (u'wavenet/cnn-3-proj/biases:0', [380]), (u'wavenet/cnn-4/weights:0', [2, 256, 512]), (u'wavenet/cnn-4/biases:0', [512]), (u'wavenet/cnn-4-proj/weights:0', [256, 380]), (u'wavenet/cnn-4-proj/biases:0', [380]), (u'wavenet/cnn-5/weights:0', [2, 256, 512]), (u'wavenet/cnn-5/biases:0', [512]), (u'wavenet/cnn-5-proj/weights:0', [256, 380]), (u'wavenet/cnn-5-proj/biases:0', [380]), (u'dense1/weights:0', [1313, 100]), (u'dense1/biases:0', [100]), (u'dense4/weights:0', [100, 1]), (u'dense4/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "4510777\n",
    "built graph\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.65639955       [[val]]     loss: 0.6460228        \n",
    "[[step     1000]]     [[train]]     loss: 0.39410796       [[val]]     loss: 0.39500415       \n",
    "[[step     2000]]     [[train]]     loss: 0.39043517       [[val]]     loss: 0.38833185       \n",
    "[[step     3000]]     [[train]]     loss: 0.38773643       [[val]]     loss: 0.38830759       \n",
    "[[step     4000]]     [[train]]     loss: 0.38594667       [[val]]     loss: 0.38734226       \n",
    "[[step     5000]]     [[train]]     loss: 0.38534158       [[val]]     loss: 0.38749352       \n",
    "[[step     6000]]     [[train]]     loss: 0.38586397       [[val]]     loss: 0.3863855        \n",
    "[[step     7000]]     [[train]]     loss: 0.38522986       [[val]]     loss: 0.38695843       \n",
    "[[step     8000]]     [[train]]     loss: 0.38583578       [[val]]     loss: 0.38644603       \n",
    "[[step     9000]]     [[train]]     loss: 0.38407461       [[val]]     loss: 0.3873567        \n",
    "[[step    10000]]     [[train]]     loss: 0.38389422       [[val]]     loss: 0.38706803       \n",
    "[[step    11000]]     [[train]]     loss: 0.38081008       [[val]]     loss: 0.38797855       \n",
    "[[step    12000]]     [[train]]     loss: 0.37900639       [[val]]     loss: 0.38802069       \n",
    "[[step    13000]]     [[train]]     loss: 0.37913274       [[val]]     loss: 0.38738997       \n",
    "[[step    14000]]     [[train]]     loss: 0.37896445       [[val]]     loss: 0.38823297       \n",
    "[[step    15000]]     [[train]]     loss: 0.37926663       [[val]]     loss: 0.38674552       \n",
    "[[step    16000]]     [[train]]     loss: 0.37970057       [[val]]     loss: 0.38887942       \n",
    "[[step    17000]]     [[train]]     loss: 0.37902393       [[val]]     loss: 0.388228         \n",
    "[[step    18000]]     [[train]]     loss: 0.38008267       [[val]]     loss: 0.38851277       \n",
    "[[step    19000]]     [[train]]     loss: 0.3773005        [[val]]     loss: 0.38779979       \n",
    "[[step    20000]]     [[train]]     loss: 0.37470627       [[val]]     loss: 0.38891269       \n",
    "[[step    21000]]     [[train]]     loss: 0.3753976        [[val]]     loss: 0.38834771       \n",
    "[[step    22000]]     [[train]]     loss: 0.3749405        [[val]]     loss: 0.3893597        \n",
    "[[step    23000]]     [[train]]     loss: 0.37535476       [[val]]     loss: 0.38784994       \n",
    "[[step    24000]]     [[train]]     loss: 0.37633207       [[val]]     loss: 0.39032121       \n",
    "[[step    25000]]     [[train]]     loss: 0.37576604       [[val]]     loss: 0.38850753       \n",
    "[[step    26000]]     [[train]]     loss: 0.37804546       [[val]]     loss: 0.39029557       \n",
    "[[step    27000]]     [[train]]     loss: 0.37477509       [[val]]     loss: 0.38876165       \n",
    "[[step    28000]]     [[train]]     loss: 0.37353205       [[val]]     loss: 0.38979194       \n",
    "[[step    29000]]     [[train]]     loss: 0.37330536       [[val]]     loss: 0.3893781        \n",
    "[[step    30000]]     [[train]]     loss: 0.37306514       [[val]]     loss: 0.39048891       \n",
    "[[step    31000]]     [[train]]     loss: 0.3734858        [[val]]     loss: 0.38949871       \n",
    "[[step    32000]]     [[train]]     loss: 0.37519194       [[val]]     loss: 0.38989895       \n",
    "[[step    33000]]     [[train]]     loss: 0.37359269       [[val]]     loss: 0.38998747       \n",
    "[[step    34000]]     [[train]]     loss: 0.37674954       [[val]]     loss: 0.39042301       \n",
    "[[step    35000]]     [[train]]     loss: 0.37249341       [[val]]     loss: 0.38951659       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\'\n",
    "path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "debug = False\n",
    "batch_size = 512\n",
    "num_training_steps = 200000\n",
    "learning_rate = 0.0002\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 100000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 100000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "filter_widths=filter_widths+filter_widths\n",
    "skip_channels=96\n",
    "residual_channels=124\n",
    "\n",
    "prediction_dir = path_out + \"predictions\"\n",
    "checkpoint_dir = path_out + \"checkpoints\"\n",
    "\n",
    "if not os.path.isdir(prediction_dir):\n",
    "    os.makedirs(prediction_dir)\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "trainable parameters:\n",
    "[(u'aisle_embeddings:0', [135, 50]), (u'dept_embeddings:0', [22, 10]), (u'wavenet/x-proj/weights:0', [1404, 124]), (u'wavenet/x-proj/biases:0', [124]), (u'wavenet/cnn-0/weights:0', [2, 124, 248]), (u'wavenet/cnn-0/biases:0', [248]), (u'wavenet/cnn-0-proj/weights:0', [124, 220]), (u'wavenet/cnn-0-proj/biases:0', [220]), (u'wavenet/cnn-1/weights:0', [2, 124, 248]), (u'wavenet/cnn-1/biases:0', [248]), (u'wavenet/cnn-1-proj/weights:0', [124, 220]), (u'wavenet/cnn-1-proj/biases:0', [220]), (u'wavenet/cnn-2/weights:0', [2, 124, 248]), (u'wavenet/cnn-2/biases:0', [248]), (u'wavenet/cnn-2-proj/weights:0', [124, 220]), (u'wavenet/cnn-2-proj/biases:0', [220]), (u'wavenet/cnn-3/weights:0', [2, 124, 248]), (u'wavenet/cnn-3/biases:0', [248]), (u'wavenet/cnn-3-proj/weights:0', [124, 220]), (u'wavenet/cnn-3-proj/biases:0', [220]), (u'wavenet/cnn-4/weights:0', [2, 124, 248]), (u'wavenet/cnn-4/biases:0', [248]), (u'wavenet/cnn-4-proj/weights:0', [124, 220]), (u'wavenet/cnn-4-proj/biases:0', [220]), (u'wavenet/cnn-5/weights:0', [2, 124, 248]), (u'wavenet/cnn-5/biases:0', [248]), (u'wavenet/cnn-5-proj/weights:0', [124, 220]), (u'wavenet/cnn-5-proj/biases:0', [220]), (u'wavenet/cnn-6/weights:0', [2, 124, 248]), (u'wavenet/cnn-6/biases:0', [248]), (u'wavenet/cnn-6-proj/weights:0', [124, 220]), (u'wavenet/cnn-6-proj/biases:0', [220]), (u'wavenet/cnn-7/weights:0', [2, 124, 248]), (u'wavenet/cnn-7/biases:0', [248]), (u'wavenet/cnn-7-proj/weights:0', [124, 220]), (u'wavenet/cnn-7-proj/biases:0', [220]), (u'wavenet/cnn-8/weights:0', [2, 124, 248]), (u'wavenet/cnn-8/biases:0', [248]), (u'wavenet/cnn-8-proj/weights:0', [124, 220]), (u'wavenet/cnn-8-proj/biases:0', [220]), (u'wavenet/cnn-9/weights:0', [2, 124, 248]), (u'wavenet/cnn-9/biases:0', [248]), (u'wavenet/cnn-9-proj/weights:0', [124, 220]), (u'wavenet/cnn-9-proj/biases:0', [220]), (u'wavenet/cnn-10/weights:0', [2, 124, 248]), (u'wavenet/cnn-10/biases:0', [248]), (u'wavenet/cnn-10-proj/weights:0', [124, 220]), (u'wavenet/cnn-10-proj/biases:0', [220]), (u'wavenet/cnn-11/weights:0', [2, 124, 248]), (u'wavenet/cnn-11/biases:0', [248]), (u'wavenet/cnn-11-proj/weights:0', [124, 220]), (u'wavenet/cnn-11-proj/biases:0', [220]), (u'dense1/weights:0', [1152, 1]), (u'dense1/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "1253367\n",
    "built graph\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.57300597       [[val]]     loss: 0.54200459       \n",
    "[[step     1000]]     [[train]]     loss: 0.4037229        [[val]]     loss: 0.40151307       \n",
    "[[step     2000]]     [[train]]     loss: 0.39069892       [[val]]     loss: 0.39203746       \n",
    "[[step     3000]]     [[train]]     loss: 0.38875566       [[val]]     loss: 0.38946532       \n",
    "[[step     4000]]     [[train]]     loss: 0.38789024       [[val]]     loss: 0.38929309       \n",
    "[[step     5000]]     [[train]]     loss: 0.38554126       [[val]]     loss: 0.38749247       \n",
    "[[step     6000]]     [[train]]     loss: 0.38616442       [[val]]     loss: 0.38787958       \n",
    "[[step     7000]]     [[train]]     loss: 0.38744169       [[val]]     loss: 0.38627563       \n",
    "[[step     8000]]     [[train]]     loss: 0.38623234       [[val]]     loss: 0.38779106       \n",
    "[[step     9000]]     [[train]]     loss: 0.38921565       [[val]]     loss: 0.38652136       \n",
    "[[step    10000]]     [[train]]     loss: 0.38508331       [[val]]     loss: 0.38733072       \n",
    "[[step    11000]]     [[train]]     loss: 0.38469722       [[val]]     loss: 0.38582431       \n",
    "[[step    12000]]     [[train]]     loss: 0.38549126       [[val]]     loss: 0.38620556       \n",
    "[[step    13000]]     [[train]]     loss: 0.38293964       [[val]]     loss: 0.38595986       \n",
    "[[step    14000]]     [[train]]     loss: 0.38466007       [[val]]     loss: 0.38606849       \n",
    "[[step    15000]]     [[train]]     loss: 0.38619278       [[val]]     loss: 0.38542646       \n",
    "[[step    16000]]     [[train]]     loss: 0.38458682       [[val]]     loss: 0.38630333       \n",
    "[[step    17000]]     [[train]]     loss: 0.38788458       [[val]]     loss: 0.3855057        \n",
    "[[step    18000]]     [[train]]     loss: 0.38331083       [[val]]     loss: 0.3860205        \n",
    "[[step    19000]]     [[train]]     loss: 0.38346381       [[val]]     loss: 0.38480297       \n",
    "[[step    20000]]     [[train]]     loss: 0.38435932       [[val]]     loss: 0.38571543       \n",
    "[[step    21000]]     [[train]]     loss: 0.38265559       [[val]]     loss: 0.38568668       \n",
    "[[step    33000]]     [[train]]     loss: 0.38622362       [[val]]     loss: 0.38602395 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WaveNet ::: Training Loss :: 0.38212136 @ 11500 :: Validation Loss : 0.38483268 @ 8500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\'\n",
    "path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "debug = False\n",
    "batch_size = 512\n",
    "num_training_steps = 100000\n",
    "learning_rate = 0.001\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 100000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 100000\n",
    "log_interval = 500\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "#dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "#filter_widths=filter_widths+filter_widths\n",
    "skip_channels=96\n",
    "residual_channels=124\n",
    "\n",
    "trainable parameters:\n",
    "[(u'aisle_embeddings:0', [135, 50]), (u'dept_embeddings:0', [22, 10]), (u'product_embeddings:0', [50000, 50]), (u'wavenet/x-proj/weights:0', [1404, 124]), (u'wavenet/x-proj/biases:0', [124]), (u'wavenet/cnn-0/weights:0', [2, 124, 248]), (u'wavenet/cnn-0/biases:0', [248]), (u'wavenet/cnn-0-proj/weights:0', [124, 220]), (u'wavenet/cnn-0-proj/biases:0', [220]), (u'wavenet/cnn-1/weights:0', [2, 124, 248]), (u'wavenet/cnn-1/biases:0', [248]), (u'wavenet/cnn-1-proj/weights:0', [124, 220]), (u'wavenet/cnn-1-proj/biases:0', [220]), (u'wavenet/cnn-2/weights:0', [2, 124, 248]), (u'wavenet/cnn-2/biases:0', [248]), (u'wavenet/cnn-2-proj/weights:0', [124, 220]), (u'wavenet/cnn-2-proj/biases:0', [220]), (u'wavenet/cnn-3/weights:0', [2, 124, 248]), (u'wavenet/cnn-3/biases:0', [248]), (u'wavenet/cnn-3-proj/weights:0', [124, 220]), (u'wavenet/cnn-3-proj/biases:0', [220]), (u'wavenet/cnn-4/weights:0', [2, 124, 248]), (u'wavenet/cnn-4/biases:0', [248]), (u'wavenet/cnn-4-proj/weights:0', [124, 220]), (u'wavenet/cnn-4-proj/biases:0', [220]), (u'wavenet/cnn-5/weights:0', [2, 124, 248]), (u'wavenet/cnn-5/biases:0', [248]), (u'wavenet/cnn-5-proj/weights:0', [124, 220]), (u'wavenet/cnn-5-proj/biases:0', [220]), (u'dense1/weights:0', [576, 1]), (u'dense1/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "3217279\n",
    "built graph\n",
    "\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.74986464       [[val]]     loss: 0.76376551       \n",
    "[[step      500]]     [[train]]     loss: 0.39902695       [[val]]     loss: 0.39719228       \n",
    "[[step     1000]]     [[train]]     loss: 0.39419912       [[val]]     loss: 0.39407802       \n",
    "[[step     1500]]     [[train]]     loss: 0.38933741       [[val]]     loss: 0.39025618       \n",
    "[[step     2000]]     [[train]]     loss: 0.38984062       [[val]]     loss: 0.38856346       \n",
    "[[step     2500]]     [[train]]     loss: 0.39052064       [[val]]     loss: 0.38848885       \n",
    "[[step     3000]]     [[train]]     loss: 0.38830204       [[val]]     loss: 0.38782649       \n",
    "[[step     3500]]     [[train]]     loss: 0.38684298       [[val]]     loss: 0.38735862       \n",
    "[[step     4000]]     [[train]]     loss: 0.38547107       [[val]]     loss: 0.38668211       \n",
    "[[step     4500]]     [[train]]     loss: 0.38482107       [[val]]     loss: 0.38653999       \n",
    "[[step     5000]]     [[train]]     loss: 0.38467534       [[val]]     loss: 0.38734918       \n",
    "[[step     5500]]     [[train]]     loss: 0.38469537       [[val]]     loss: 0.38696021       \n",
    "[[step     6000]]     [[train]]     loss: 0.38638392       [[val]]     loss: 0.38610079       \n",
    "[[step     6500]]     [[train]]     loss: 0.38448875       [[val]]     loss: 0.38586745       \n",
    "[[step     7000]]     [[train]]     loss: 0.38478305       [[val]]     loss: 0.38651106       \n",
    "[[step     7500]]     [[train]]     loss: 0.38532901       [[val]]     loss: 0.38626928       \n",
    "[[step     8000]]     [[train]]     loss: 0.38410955       [[val]]     loss: 0.38537588       \n",
    "[[step     8500]]     [[train]]     loss: 0.38513654       [[val]]     loss: 0.38483268       \n",
    "[[step     9000]]     [[train]]     loss: 0.38417677       [[val]]     loss: 0.38610536       \n",
    "[[step     9500]]     [[train]]     loss: 0.38283743       [[val]]     loss: 0.38755284       \n",
    "[[step    10000]]     [[train]]     loss: 0.38427715       [[val]]     loss: 0.38727841       \n",
    "[[step    10500]]     [[train]]     loss: 0.38546348       [[val]]     loss: 0.38697921       \n",
    "[[step    11000]]     [[train]]     loss: 0.38355408       [[val]]     loss: 0.38705426       \n",
    "[[step    11500]]     [[train]]     loss: 0.38212136       [[val]]     loss: 0.3876643     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked LSTM@300 + Wide WaveNet with Fixed Top ::: Training Loss :: 0.38121098 @ 11000 :: Validation Loss : 0.38525658 @ 9500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\'\n",
    "path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "debug = False\n",
    "batch_size = 512\n",
    "num_training_steps = 100000\n",
    "learning_rate = 0.001\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 100000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 100000\n",
    "log_interval = 500\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "#dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "#filter_widths=filter_widths+filter_widths\n",
    "skip_channels=96\n",
    "residual_channels=124*5\n",
    "\n",
    "trainable parameters:\n",
    "[(u'aisle_embeddings:0', [135, 50]), (u'dept_embeddings:0', [22, 10]), (u'product_embeddings:0', [50000, 50]), (u'lstm1/rnn/lstm_cell/kernel:0', [1704, 1200]), (u'lstm1/rnn/lstm_cell/bias:0', [1200]), (u'wavenet/x-proj/weights:0', [1404, 620]), (u'wavenet/x-proj/biases:0', [620]), (u'wavenet/cnn-0/weights:0', [2, 620, 1240]), (u'wavenet/cnn-0/biases:0', [1240]), (u'wavenet/cnn-0-proj/weights:0', [620, 716]), (u'wavenet/cnn-0-proj/biases:0', [716]), (u'wavenet/cnn-1/weights:0', [2, 620, 1240]), (u'wavenet/cnn-1/biases:0', [1240]), (u'wavenet/cnn-1-proj/weights:0', [620, 716]), (u'wavenet/cnn-1-proj/biases:0', [716]), (u'wavenet/cnn-2/weights:0', [2, 620, 1240]), (u'wavenet/cnn-2/biases:0', [1240]), (u'wavenet/cnn-2-proj/weights:0', [620, 716]), (u'wavenet/cnn-2-proj/biases:0', [716]), (u'wavenet/cnn-3/weights:0', [2, 620, 1240]), (u'wavenet/cnn-3/biases:0', [1240]), (u'wavenet/cnn-3-proj/weights:0', [620, 716]), (u'wavenet/cnn-3-proj/biases:0', [716]), (u'wavenet/cnn-4/weights:0', [2, 620, 1240]), (u'wavenet/cnn-4/biases:0', [1240]), (u'wavenet/cnn-4-proj/weights:0', [620, 716]), (u'wavenet/cnn-4-proj/biases:0', [716]), (u'wavenet/cnn-5/weights:0', [2, 620, 1240]), (u'wavenet/cnn-5/biases:0', [1240]), (u'wavenet/cnn-5-proj/weights:0', [620, 716]), (u'wavenet/cnn-5-proj/biases:0', [716]), (u'dense1/weights:0', [2280, 300]), (u'dense1/biases:0', [300]), (u'dense2/weights:0', [300, 200]), (u'dense2/biases:0', [200]), (u'dense3/weights:0', [200, 100]), (u'dense3/biases:0', [100]), (u'dense4/weights:0', [100, 1]), (u'dense4/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "18089627\n",
    "built graph\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.71439296       [[val]]     loss: 0.71579534       \n",
    "[[step      500]]     [[train]]     loss: 0.39536874       [[val]]     loss: 0.39705661       \n",
    "[[step     1000]]     [[train]]     loss: 0.39314378       [[val]]     loss: 0.39347515       \n",
    "[[step     1500]]     [[train]]     loss: 0.38910436       [[val]]     loss: 0.39012438       \n",
    "[[step     2000]]     [[train]]     loss: 0.38624256       [[val]]     loss: 0.38915163       \n",
    "[[step     2500]]     [[train]]     loss: 0.38640541       [[val]]     loss: 0.38788248       \n",
    "[[step     3000]]     [[train]]     loss: 0.38633301       [[val]]     loss: 0.38729562       \n",
    "[[step     3500]]     [[train]]     loss: 0.38628424       [[val]]     loss: 0.38771333       \n",
    "[[step     4000]]     [[train]]     loss: 0.3872626        [[val]]     loss: 0.3872555        \n",
    "[[step     4500]]     [[train]]     loss: 0.3866859        [[val]]     loss: 0.38601295       \n",
    "[[step     5000]]     [[train]]     loss: 0.38571788       [[val]]     loss: 0.38543805       \n",
    "[[step     5500]]     [[train]]     loss: 0.38781096       [[val]]     loss: 0.38662798       \n",
    "[[step     6000]]     [[train]]     loss: 0.38873766       [[val]]     loss: 0.38704623       \n",
    "[[step     6500]]     [[train]]     loss: 0.3863655        [[val]]     loss: 0.38623803       \n",
    "[[step     7000]]     [[train]]     loss: 0.38436648       [[val]]     loss: 0.38577226       \n",
    "[[step     7500]]     [[train]]     loss: 0.38427535       [[val]]     loss: 0.38547944       \n",
    "[[step     8000]]     [[train]]     loss: 0.3842475        [[val]]     loss: 0.38648178       \n",
    "[[step     8500]]     [[train]]     loss: 0.38299651       [[val]]     loss: 0.38612066       \n",
    "[[step     9500]]     [[train]]     loss: 0.38224757       [[val]]     loss: 0.38525658       \n",
    "[[step    10000]]     [[train]]     loss: 0.38008553       [[val]]     loss: 0.38662994       \n",
    "[[step    10500]]     [[train]]     loss: 0.38114074       [[val]]     loss: 0.38728664       \n",
    "[[step    11000]]     [[train]]     loss: 0.38121098       [[val]]     loss: 0.3862937   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\chapanda\\data\\np\\checkpoints\\model3463334000.ckpt-334000\n"
     ]
    }
   ],
   "source": [
    "model_path='C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\checkpoints\\\\model3463334000.ckpt-334000'\n",
    "saver.restore(session, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dict = {tensor_name: [] for tensor_name in prediction_tensors}\n",
    "\n",
    "num_val_batches = int(val.arrays[0].shape[0]/batch_size)\n",
    "for i in range(num_test_batches):\n",
    "    val_dict_list =  next(val.next_batch(batch_size))\n",
    "        \n",
    "    # test evaluation\n",
    "    val_feed_dict = val_dict_list\n",
    "    val_feed_dict.update({learning_rate_var: learning_rate})\n",
    "    val_feed_dict.update({keep_prob: 1.0})\n",
    "    val_feed_dict.update({is_training: False})\n",
    "\n",
    "    tensor_names, tf_tensors = zip(*prediction_tensors.items())\n",
    "    np_tensors = session.run(\n",
    "        fetches=tf_tensors,\n",
    "        feed_dict=val_feed_dict\n",
    "    )\n",
    "    for tensor_name, tensor in zip(tensor_names, np_tensors):\n",
    "        prediction_dict[tensor_name].append(tensor)\n",
    "\n",
    "for tensor_name, tensor in prediction_dict.items():\n",
    "    np_tensor = np.concatenate(tensor, 0)\n",
    "    save_file = os.path.join(prediction_dir, '{}.npy'.format(tensor_name))\n",
    "    print('saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "    np.save(save_file, np_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLift(predics,actual,h_len, a_ids ,verbosity=0):\n",
    "    \n",
    "    aisles = np.unique(a_ids)\n",
    "    cum_lift = 0.0\n",
    "    cnt = 0\n",
    "\n",
    "    for n in range(len(aisles)):\n",
    "\n",
    "        predics_aisle = predics[np.where(a_ids==aisles[n])]\n",
    "        actual_aisle = actual[np.where(a_ids==aisles[n])]\n",
    "        h_len_aisle = h_len[np.where(a_ids==aisles[n])]\n",
    "\n",
    "        i = np.where(predics_aisle>np.percentile(predics_aisle,90))\n",
    "        j = np.where(predics_aisle>0)\n",
    "        num_i = len(i[0])*1.0\n",
    "        num_j = len(j[0])*1.0\n",
    "\n",
    "        up = (np.sum(actual_aisle[i[0],h_len_aisle[i]-1])*1.0)/num_i\n",
    "        down = (np.sum(actual_aisle[j[0],h_len_aisle[j]-1])*1.0)/num_j\n",
    "        lift = round(up/down,3)\n",
    "        if lift>0:\n",
    "            cum_lift += lift\n",
    "            cnt += 1\n",
    "\n",
    "        if verbosity==1:\n",
    "            print  lift , round(num_i/num_j,3), round(num_j/2000.0,2)\n",
    "\n",
    "    return round(cum_lift/cnt,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predics = np.load('np/predictions/predictions.npy')[0:1163264]\n",
    "actual = np.load('np/NextInOrder_history_test.npy')[0:1163264,:]\n",
    "h_len = np.load('np/history_length_test.npy')[0:1163264]\n",
    "a_ids = np.load('np/master_aisle_id_test.npy')[0:1163264]\n",
    "print predics.shape,actual.shape,h_len.shape, a_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(findLift(predics,actual,h_len,a_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "\n",
    "1. Batch Normalization\n",
    "2. 50, 300 User Embeddings\n",
    "3. Department Embeddings\n",
    "4. Product Level Model Stacked with Department Level Model\n",
    "5. wavenet\n",
    "6. dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "lr = 0.00001\n",
    "embedding_size = 10\n",
    "batch_norm = False\n",
    "dropout = 0\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.68945086       [[val]]     loss: 0.68476069       \n",
    "[[step      100]]     [[train]]     loss: 0.52741179       [[val]]     loss: 0.52966699       \n",
    "[[step      200]]     [[train]]     loss: 0.42600287       [[val]]     loss: 0.43532646       \n",
    "[[step      300]]     [[train]]     loss: 0.42100239       [[val]]     loss: 0.42119279       \n",
    "[[step      400]]     [[train]]     loss: 0.40935658       [[val]]     loss: 0.41433323       \n",
    "[[step      500]]     [[train]]     loss: 0.40634079       [[val]]     loss: 0.40343187       \n",
    "[[step      600]]     [[train]]     loss: 0.40356448       [[val]]     loss: 0.40347679       \n",
    "[[step      700]]     [[train]]     loss: 0.39721517       [[val]]     loss: 0.39683794       \n",
    "[[step      800]]     [[train]]     loss: 0.40219059       [[val]]     loss: 0.40324225       \n",
    "[[step      900]]     [[train]]     loss: 0.39875729       [[val]]     loss: 0.39927291       \n",
    "[[step     1000]]     [[train]]     loss: 0.39288183       [[val]]     loss: 0.41149075       \n",
    "[[step     1100]]     [[train]]     loss: 0.4064956        [[val]]     loss: 0.39715582       \n",
    "[[step     1200]]     [[train]]     loss: 0.39463545       [[val]]     loss: 0.39509431       \n",
    "[[step     1300]]     [[train]]     loss: 0.40247756       [[val]]     loss: 0.41126509       \n",
    "[[step     1400]]     [[train]]     loss: 0.39764947       [[val]]     loss: 0.39076713       \n",
    "[[step     1500]]     [[train]]     loss: 0.40276493       [[val]]     loss: 0.38258591       \n",
    "[[step     1600]]     [[train]]     loss: 0.38685263       [[val]]     loss: 0.39597086       \n",
    "[[step     1700]]     [[train]]     loss: 0.39643141       [[val]]     loss: 0.40260221       \n",
    "[[step     1800]]     [[train]]     loss: 0.39657799       [[val]]     loss: 0.39385055       \n",
    "[[step     1900]]     [[train]]     loss: 0.38752659       [[val]]     loss: 0.3947884        \n",
    "[[step     2000]]     [[train]]     loss: 0.39012568       [[val]]     loss: 0.40079436       \n",
    "[[step     2100]]     [[train]]     loss: 0.39138237       [[val]]     loss: 0.39907276    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "lr = 0.00001\n",
    "embedding_size = 10-50\n",
    "batch_norm = False\n",
    "dropout = 0\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.69394004       [[val]]     loss: 0.69585359       \n",
    "[[step      100]]     [[train]]     loss: 0.57589115       [[val]]     loss: 0.57229453       \n",
    "[[step      200]]     [[train]]     loss: 0.46916369       [[val]]     loss: 0.45808681       \n",
    "[[step      300]]     [[train]]     loss: 0.43033853       [[val]]     loss: 0.44753059       \n",
    "[[step      400]]     [[train]]     loss: 0.41173015       [[val]]     loss: 0.40878801       \n",
    "[[step      500]]     [[train]]     loss: 0.40163584       [[val]]     loss: 0.40868014       \n",
    "[[step      600]]     [[train]]     loss: 0.411918         [[val]]     loss: 0.40809412       \n",
    "[[step      700]]     [[train]]     loss: 0.4154951        [[val]]     loss: 0.39677772       \n",
    "[[step      800]]     [[train]]     loss: 0.40657057       [[val]]     loss: 0.3949514        \n",
    "[[step      900]]     [[train]]     loss: 0.40774495       [[val]]     loss: 0.40948893  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.001\n",
    "embedding_size = 10-50\n",
    "batch_norm = False\n",
    "dropout = 0\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.68255645       [[val]]     loss: 0.68390912       \n",
    "[[step      100]]     [[train]]     loss: 0.43326256       [[val]]     loss: 0.41730649       \n",
    "[[step      200]]     [[train]]     loss: 0.42293454       [[val]]     loss: 0.4055799        \n",
    "[[step      300]]     [[train]]     loss: 0.42179572       [[val]]     loss: 0.40204019       \n",
    "[[step      400]]     [[train]]     loss: 0.41070997       [[val]]     loss: 0.39842628       \n",
    "[[step      500]]     [[train]]     loss: 0.42036336       [[val]]     loss: 0.40866262       \n",
    "[[step      600]]     [[train]]     loss: 0.41237596       [[val]]     loss: 0.3960299        \n",
    "[[step      700]]     [[train]]     loss: 0.40602685       [[val]]     loss: 0.39392554       \n",
    "[[step      800]]     [[train]]     loss: 0.41559202       [[val]]     loss: 0.40025059       \n",
    "[[step      900]]     [[train]]     loss: 0.41720033       [[val]]     loss: 0.38500627       \n",
    "[[step     1000]]     [[train]]     loss: 0.40658416       [[val]]     loss: 0.40565673       \n",
    "[[step     1100]]     [[train]]     loss: 0.40783333       [[val]]     loss: 0.39481372       \n",
    "[[step     1200]]     [[train]]     loss: 0.40538956       [[val]]     loss: 0.39182611       \n",
    "[[step     1300]]     [[train]]     loss: 0.41342945       [[val]]     loss: 0.39858596       \n",
    "[[step     1400]]     [[train]]     loss: 0.41209739       [[val]]     loss: 0.39087869       \n",
    "[[step     1500]]     [[train]]     loss: 0.41145673       [[val]]     loss: 0.39485126       \n",
    "[[step     1600]]     [[train]]     loss: 0.42238395       [[val]]     loss: 0.39638283       \n",
    "[[step     1700]]     [[train]]     loss: 0.40937202       [[val]]     loss: 0.39356789       \n",
    "[[step     1800]]     [[train]]     loss: 0.41470608       [[val]]     loss: 0.38789571       \n",
    "[[step     1900]]     [[train]]     loss: 0.41887923       [[val]]     loss: 0.39698425       \n",
    "[[step     2000]]     [[train]]     loss: 0.40212174       [[val]]     loss: 0.38679843       \n",
    "[[step     2100]]     [[train]]     loss: 0.41022544       [[val]]     loss: 0.39420067       \n",
    "[[step     2200]]     [[train]]     loss: 0.41485526       [[val]]     loss: 0.39693822       \n",
    "[[step     2300]]     [[train]]     loss: 0.40914674       [[val]]     loss: 0.39013196       \n",
    "[[step     2400]]     [[train]]     loss: 0.40681492       [[val]]     loss: 0.39067219       \n",
    "[[step     2500]]     [[train]]     loss: 0.4076804        [[val]]     loss: 0.39330774       \n",
    "[[step     2600]]     [[train]]     loss: 0.42123778       [[val]]     loss: 0.38994598       \n",
    "[[step     2700]]     [[train]]     loss: 0.41556455       [[val]]     loss: 0.39031908       \n",
    "[[step     2800]]     [[train]]     loss: 0.39836752       [[val]]     loss: 0.40093884       \n",
    "[[step     2900]]     [[train]]     loss: 0.40989705       [[val]]     loss: 0.38727811       \n",
    "[[step     3000]]     [[train]]     loss: 0.41062794       [[val]]     loss: 0.390566         \n",
    "[[step     3100]]     [[train]]     loss: 0.40107831       [[val]]     loss: 0.3911186        \n",
    "[[step     3200]]     [[train]]     loss: 0.40512848       [[val]]     loss: 0.3826369        \n",
    "[[step     3300]]     [[train]]     loss: 0.40562024       [[val]]     loss: 0.4026496        \n",
    "[[step     3400]]     [[train]]     loss: 0.42069093       [[val]]     loss: 0.38858808       \n",
    "[[step     3500]]     [[train]]     loss: 0.40914303       [[val]]     loss: 0.38865459       \n",
    "[[step     3600]]     [[train]]     loss: 0.41292151       [[val]]     loss: 0.39358043       \n",
    "[[step     3700]]     [[train]]     loss: 0.40138045       [[val]]     loss: 0.38675016       \n",
    "[[step     3800]]     [[train]]     loss: 0.41835118       [[val]]     loss: 0.39161719       \n",
    "[[step     3900]]     [[train]]     loss: 0.40322772       [[val]]     loss: 0.39639044       \n",
    "[[step     4000]]     [[train]]     loss: 0.43074263       [[val]]     loss: 0.3883957        \n",
    "[[step     4100]]     [[train]]     loss: 0.4156726        [[val]]     loss: 0.38980969       \n",
    "[[step     4200]]     [[train]]     loss: 0.40842212       [[val]]     loss: 0.39319402       \n",
    "[[step     4300]]     [[train]]     loss: 0.41640786       [[val]]     loss: 0.38108944       \n",
    "[[step     4400]]     [[train]]     loss: 0.40814732       [[val]]     loss: 0.40034898       \n",
    "[[step     4500]]     [[train]]     loss: 0.41581489       [[val]]     loss: 0.38978179       \n",
    "[[step     4600]]     [[train]]     loss: 0.4131633        [[val]]     loss: 0.38818143       \n",
    "[[step     4700]]     [[train]]     loss: 0.39764337       [[val]]     loss: 0.39419644       \n",
    "[[step     4800]]     [[train]]     loss: 0.40196683       [[val]]     loss: 0.38820853       \n",
    "[[step     4900]]     [[train]]     loss: 0.41307212       [[val]]     loss: 0.39067163       \n",
    "[[step     5000]]     [[train]]     loss: 0.40948561       [[val]]     loss: 0.39197072       \n",
    "[[step     5100]]     [[train]]     loss: 0.41734304       [[val]]     loss: 0.39139198       \n",
    "[[step     5200]]     [[train]]     loss: 0.40337441       [[val]]     loss: 0.38433809       \n",
    "[[step     5300]]     [[train]]     loss: 0.41370722       [[val]]     loss: 0.39362062       \n",
    "[[step     5400]]     [[train]]     loss: 0.41196924       [[val]]     loss: 0.38499878       \n",
    "[[step     5500]]     [[train]]     loss: 0.40916227       [[val]]     loss: 0.39047131       \n",
    "[[step     8400]]     [[train]]     loss: 0.41929639       [[val]]     loss: 0.39204189 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "lr = 0.0001\n",
    "embedding_size = 10-50\n",
    "batch_norm = False\n",
    "dropout = 0\n",
    "final layers = 2 25 - 50\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.69800591       [[val]]     loss: 0.7077204        \n",
    "[[step      100]]     [[train]]     loss: 0.4642874        [[val]]     loss: 0.45294539       \n",
    "[[step      200]]     [[train]]     loss: 0.40891664       [[val]]     loss: 0.39687391       \n",
    "[[step      300]]     [[train]]     loss: 0.41119523       [[val]]     loss: 0.40390418       \n",
    "[[step      400]]     [[train]]     loss: 0.40153559       [[val]]     loss: 0.39589814       \n",
    "[[step      500]]     [[train]]     loss: 0.41193385       [[val]]     loss: 0.39404101       \n",
    "[[step      600]]     [[train]]     loss: 0.40311682       [[val]]     loss: 0.39565056       \n",
    "[[step      700]]     [[train]]     loss: 0.40937866       [[val]]     loss: 0.39157548       \n",
    "[[step      800]]     [[train]]     loss: 0.40068859       [[val]]     loss: 0.39049928       \n",
    "[[step      900]]     [[train]]     loss: 0.40969506       [[val]]     loss: 0.39396498       \n",
    "[[step     1000]]     [[train]]     loss: 0.39421114       [[val]]     loss: 0.3906278        \n",
    "[[step     1100]]     [[train]]     loss: 0.40498208       [[val]]     loss: 0.3875514        \n",
    "[[step     1200]]     [[train]]     loss: 0.40884323       [[val]]     loss: 0.39262184       \n",
    "[[step     1300]]     [[train]]     loss: 0.40473363       [[val]]     loss: 0.38881227       \n",
    "[[step     1400]]     [[train]]     loss: 0.39670036       [[val]]     loss: 0.3909938        \n",
    "[[step     1500]]     [[train]]     loss: 0.39192327       [[val]]     loss: 0.38764418       \n",
    "[[step     1600]]     [[train]]     loss: 0.40501846       [[val]]     loss: 0.38803977       \n",
    "[[step     1700]]     [[train]]     loss: 0.39387804       [[val]]     loss: 0.39030977       \n",
    "[[step     1800]]     [[train]]     loss: 0.39960696       [[val]]     loss: 0.38569073       \n",
    "[[step     1900]]     [[train]]     loss: 0.40731614       [[val]]     loss: 0.38400814       \n",
    "[[step     2000]]     [[train]]     loss: 0.39492307       [[val]]     loss: 0.39189795       \n",
    "[[step     2100]]     [[train]]     loss: 0.40242343       [[val]]     loss: 0.38887628       \n",
    "[[step     2200]]     [[train]]     loss: 0.40079135       [[val]]     loss: 0.38510605       \n",
    "[[step     2300]]     [[train]]     loss: 0.40135673       [[val]]     loss: 0.3891011        \n",
    "[[step     2400]]     [[train]]     loss: 0.40815621       [[val]]     loss: 0.38702249       \n",
    "[[step     2500]]     [[train]]     loss: 0.40195283       [[val]]     loss: 0.38379056       \n",
    "[[step     2600]]     [[train]]     loss: 0.41210029       [[val]]     loss: 0.38936703       \n",
    "[[step     2700]]     [[train]]     loss: 0.39930949       [[val]]     loss: 0.38703894       \n",
    "[[step     2800]]     [[train]]     loss: 0.40331845       [[val]]     loss: 0.38391135       \n",
    "[[step     2900]]     [[train]]     loss: 0.4059205        [[val]]     loss: 0.38919657       \n",
    "[[step     3000]]     [[train]]     loss: 0.39222894       [[val]]     loss: 0.38493212       \n",
    "[[step     3100]]     [[train]]     loss: 0.39660685       [[val]]     loss: 0.3870271        \n",
    "[[step     3200]]     [[train]]     loss: 0.39953087       [[val]]     loss: 0.38373316       \n",
    "[[step     3300]]     [[train]]     loss: 0.40271052       [[val]]     loss: 0.38703135       \n",
    "[[step     3400]]     [[train]]     loss: 0.39658011       [[val]]     loss: 0.38717001       \n",
    "[[step     3500]]     [[train]]     loss: 0.40102986       [[val]]     loss: 0.38259657       \n",
    "[[step     3600]]     [[train]]     loss: 0.40681617       [[val]]     loss: 0.3822414        \n",
    "[[step     3700]]     [[train]]     loss: 0.39806747       [[val]]     loss: 0.38907532      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 10000\n",
    "lr = 0.001\n",
    "embedding_size = 10-50\n",
    "batch_norm = False\n",
    "dropout = 0\n",
    "final layers = 2 100 - 500\n",
    "lstm = 600\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.65691781       [[val]]     loss: 0.65873712       \n",
    "[[step      100]]     [[train]]     loss: 0.40820402       [[val]]     loss: 0.41145359       \n",
    "[[step      200]]     [[train]]     loss: 0.38525348       [[val]]     loss: 0.38974458       \n",
    "[[step      300]]     [[train]]     loss: 0.3921014        [[val]]     loss: 0.38808708       \n",
    "[[step      400]]     [[train]]     loss: 0.3891164        [[val]]     loss: 0.39563058       \n",
    "[[step      500]]     [[train]]     loss: 0.38700464       [[val]]     loss: 0.38803441       \n",
    "[[step      600]]     [[train]]     loss: 0.38530519       [[val]]     loss: 0.38530802       \n",
    "[[step      700]]     [[train]]     loss: 0.3820014        [[val]]     loss: 0.38732941       \n",
    "[[step      800]]     [[train]]     loss: 0.38262708       [[val]]     loss: 0.38449259       \n",
    "[[step      900]]     [[train]]     loss: 0.38381389       [[val]]     loss: 0.38279317       \n",
    "[[step     1000]]     [[train]]     loss: 0.38220652       [[val]]     loss: 0.38621109       \n",
    "[[step     1100]]     [[train]]     loss: 0.38452308       [[val]]     loss: 0.38545803       \n",
    "[[step     1200]]     [[train]]     loss: 0.38348263       [[val]]     loss: 0.38144997       \n",
    "[[step     1300]]     [[train]]     loss: 0.38537586       [[val]]     loss: 0.38501431       \n",
    "[[step     1400]]     [[train]]     loss: 0.38123322       [[val]]     loss: 0.38310651       \n",
    "[[step     1500]]     [[train]]     loss: 0.38254252       [[val]]     loss: 0.38495605       \n",
    "[[step     1600]]     [[train]]     loss: 0.37771166       [[val]]     loss: 0.38185424       \n",
    "[[step     1700]]     [[train]]     loss: 0.38058311       [[val]]     loss: 0.38246381       \n",
    "[[step     1800]]     [[train]]     loss: 0.38418386       [[val]]     loss: 0.38565543       \n",
    "[[step     1900]]     [[train]]     loss: 0.3772252        [[val]]     loss: 0.37936874       \n",
    "[[step     2000]]     [[train]]     loss: 0.38489054       [[val]]     loss: 0.38112491       \n",
    "[[step     2100]]     [[train]]     loss: 0.37931004       [[val]]     loss: 0.38886817       \n",
    "[[step     2200]]     [[train]]     loss: 0.38216422       [[val]]     loss: 0.38252707       \n",
    "[[step     2300]]     [[train]]     loss: 0.38276958       [[val]]     loss: 0.38094901       \n",
    "[[step     2400]]     [[train]]     loss: 0.3831657        [[val]]     loss: 0.38399107       \n",
    "[[step     2500]]     [[train]]     loss: 0.38187274       [[val]]     loss: 0.38105462       \n",
    "[[step     2600]]     [[train]]     loss: 0.38516242       [[val]]     loss: 0.38006142       \n",
    "[[step     2700]]     [[train]]     loss: 0.38071658       [[val]]     loss: 0.3846091        \n",
    "[[step     2800]]     [[train]]     loss: 0.38133765       [[val]]     loss: 0.38212655       \n",
    "[[step     2900]]     [[train]]     loss: 0.37816642       [[val]]     loss: 0.37891927       \n",
    "[[step     3000]]     [[train]]     loss: 0.38419693       [[val]]     loss: 0.38428155       \n",
    "[[step     3100]]     [[train]]     loss: 0.3781649        [[val]]     loss: 0.38119657       \n",
    "[[step     3200]]     [[train]]     loss: 0.38513189       [[val]]     loss: 0.38320454       \n",
    "[[step     3300]]     [[train]]     loss: 0.38730632       [[val]]     loss: 0.37999186       \n",
    "[[step     3400]]     [[train]]     loss: 0.38231762       [[val]]     loss: 0.38117058       \n",
    "[[step     3500]]     [[train]]     loss: 0.3831878        [[val]]     loss: 0.38359242       \n",
    "[[step     3600]]     [[train]]     loss: 0.38420483       [[val]]     loss: 0.37936406       \n",
    "[[step     3700]]     [[train]]     loss: 0.38203219       [[val]]     loss: 0.37749232       \n",
    "[[step     3800]]     [[train]]     loss: 0.37881707       [[val]]     loss: 0.38570269       \n",
    "[[step     3900]]     [[train]]     loss: 0.38056242       [[val]]     loss: 0.38261534       \n",
    "[[step     4000]]     [[train]]     loss: 0.37818603       [[val]]     loss: 0.37869769       \n",
    "[[step     4100]]     [[train]]     loss: 0.37891285       [[val]]     loss: 0.38342259       \n",
    "[[step     4200]]     [[train]]     loss: 0.37394667       [[val]]     loss: 0.38115527       \n",
    "[[step     4300]]     [[train]]     loss: 0.37930212       [[val]]     loss: 0.37814815       \n",
    "[[step     4400]]     [[train]]     loss: 0.37921889       [[val]]     loss: 0.3847813        \n",
    "[[step     4500]]     [[train]]     loss: 0.37409336       [[val]]     loss: 0.38351414       \n",
    "[[step     4600]]     [[train]]     loss: 0.37662914       [[val]]     loss: 0.38074349       \n",
    "[[step     4700]]     [[train]]     loss: 0.37437559       [[val]]     loss: 0.38719823       \n",
    "[[step     4800]]     [[train]]     loss: 0.37528483       [[val]]     loss: 0.3823637        \n",
    "[[step     4900]]     [[train]]     loss: 0.37472385       [[val]]     loss: 0.38422586       \n",
    "[[step     5000]]     [[train]]     loss: 0.37233163       [[val]]     loss: 0.38127384       \n",
    "[[step     5100]]     [[train]]     loss: 0.37854063       [[val]]     loss: 0.38465769       \n",
    "[[step     5200]]     [[train]]     loss: 0.3731164        [[val]]     loss: 0.38532945       \n",
    "[[step     5300]]     [[train]]     loss: 0.37532851       [[val]]     loss: 0.38037309       \n",
    "[[step     5400]]     [[train]]     loss: 0.37103678       [[val]]     loss: 0.38014162       \n",
    "[[step     5500]]     [[train]]     loss: 0.37317753       [[val]]     loss: 0.38685308       \n",
    "[[step     5600]]     [[train]]     loss: 0.36903966       [[val]]     loss: 0.38514764       \n",
    "[[step     5700]]     [[train]]     loss: 0.3736941        [[val]]     loss: 0.38151485       \n",
    "[[step     5800]]     [[train]]     loss: 0.37574548       [[val]]     loss: 0.38607571       \n",
    "[[step     5900]]     [[train]]     loss: 0.36797815       [[val]]     loss: 0.38232391       \n",
    "[[step     6000]]     [[train]]     loss: 0.37378719       [[val]]     loss: 0.37931299       \n",
    "[[step     6100]]     [[train]]     loss: 0.37145472       [[val]]     loss: 0.3854492        \n",
    "[[step     6200]]     [[train]]     loss: 0.37422799       [[val]]     loss: 0.38575146       \n",
    "[[step     6300]]     [[train]]     loss: 0.37489267       [[val]]     loss: 0.3798244        \n",
    "[[step     6400]]     [[train]]     loss: 0.37317946       [[val]]     loss: 0.3865814        \n",
    "[[step     6500]]     [[train]]     loss: 0.37357524       [[val]]     loss: 0.38192061       \n",
    "[[step     6600]]     [[train]]     loss: 0.37510183       [[val]]     loss: 0.38409594       \n",
    "[[step     6700]]     [[train]]     loss: 0.37295555       [[val]]     loss: 0.38225858       \n",
    "[[step     6800]]     [[train]]     loss: 0.37292404       [[val]]     loss: 0.38288011       \n",
    "[[step     6900]]     [[train]]     loss: 0.37217351       [[val]]     loss: 0.38604897       \n",
    "[[step     7000]]     [[train]]     loss: 0.37124587       [[val]]     loss: 0.37983757       \n",
    "[[step     7100]]     [[train]]     loss: 0.3734486        [[val]]     loss: 0.38058637       \n",
    "[[step     7200]]     [[train]]     loss: 0.37438173       [[val]]     loss: 0.38803061       \n",
    "[[step     7300]]     [[train]]     loss: 0.37942499       [[val]]     loss: 0.38332724       \n",
    "[[step     7400]]     [[train]]     loss: 0.37363714       [[val]]     loss: 0.38279361       \n",
    "[[step     7500]]     [[train]]     loss: 0.37495664       [[val]]     loss: 0.38467916       \n",
    "[[step     7600]]     [[train]]     loss: 0.37496262       [[val]]     loss: 0.38225796       \n",
    "[[step     7700]]     [[train]]     loss: 0.37258991       [[val]]     loss: 0.37846636       \n",
    "[[step     7800]]     [[train]]     loss: 0.37120649       [[val]]     loss: 0.38686409       \n",
    "[[step     7900]]     [[train]]     loss: 0.37128173       [[val]]     loss: 0.38571515       \n",
    "[[step     8000]]     [[train]]     loss: 0.37181711       [[val]]     loss: 0.37960169       \n",
    "[[step     8100]]     [[train]]     loss: 0.36826489       [[val]]     loss: 0.38574902       \n",
    "[[step     8200]]     [[train]]     loss: 0.37130566       [[val]]     loss: 0.38235267       \n",
    "[[step     8300]]     [[train]]     loss: 0.37132396       [[val]]     loss: 0.3838576        \n",
    "[[step     8400]]     [[train]]     loss: 0.36927456       [[val]]     loss: 0.38404872       \n",
    "[[step     8500]]     [[train]]     loss: 0.36493352       [[val]]     loss: 0.3866663        \n",
    "[[step     8600]]     [[train]]     loss: 0.36904615       [[val]]     loss: 0.38660518       \n",
    "[[step     8700]]     [[train]]     loss: 0.36574878       [[val]]     loss: 0.382791         \n",
    "[[step     8800]]     [[train]]     loss: 0.36782915       [[val]]     loss: 0.38435617       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     8900]]     [[train]]     loss: 0.36883847       [[val]]     loss: 0.38830326       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9000]]     [[train]]     loss: 0.36762432       [[val]]     loss: 0.38375869       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9100]]     [[train]]     loss: 0.372115         [[val]]     loss: 0.38439275       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9200]]     [[train]]     loss: 0.36688577       [[val]]     loss: 0.38790736       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9300]]     [[train]]     loss: 0.36813599       [[val]]     loss: 0.38178434       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9400]]     [[train]]     loss: 0.36671974       [[val]]     loss: 0.37995047       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9500]]     [[train]]     loss: 0.36658081       [[val]]     loss: 0.38970799       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9600]]     [[train]]     loss: 0.36533744       [[val]]     loss: 0.38604101       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9700]]     [[train]]     loss: 0.36672812       [[val]]     loss: 0.38222775       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9800]]     [[train]]     loss: 0.36806024       [[val]]     loss: 0.38704535       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9900]]     [[train]]     loss: 0.3634537        [[val]]     loss: 0.38424529       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step    10000]]     [[train]]     loss: 0.36658145       [[val]]     loss: 0.38508767       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
