{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\chapanda\\\\OneDrive - Epsilon\\\\cp\\\\ACG\\\\03_Practise\\\\RnD\\\\crosssell'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import csv\n",
    "from collections import deque\n",
    "from glob import glob\n",
    "import bcolz\n",
    "from sklearn.metrics import log_loss\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True,gpu_options=gpu_options))\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\'\n",
    "#path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "N = 100\n",
    "debug = False\n",
    "batch_size = 128\n",
    "num_training_steps = 1000000\n",
    "learning_rate = 0.0003\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 500000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 50000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = False\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "filter_widths=filter_widths+filter_widths\n",
    "skip_channels=124\n",
    "residual_channels=256\n",
    "\n",
    "prediction_dir = path_out + \"predictions\"\n",
    "checkpoint_dir = path_out + \"checkpoints\"\n",
    "\n",
    "if not os.path.isdir(prediction_dir):\n",
    "    os.makedirs(prediction_dir)\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "def tic():\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "def toc():\n",
    "    global start_time\n",
    "    elapsed_time = (time.time() - start_time)\n",
    "    print(\"took me \" + str(round(elapsed_time, 3))+\" seconds to do this..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(tensor, dim=None):\n",
    "    \"\"\"Get tensor shape/dimension as list/int\"\"\"\n",
    "    if dim is None:\n",
    "        return tensor.shape.as_list()\n",
    "    else:\n",
    "        return tensor.shape.as_list()[dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(object):\n",
    "\n",
    "    def __init__(self, path_out, suffix):\n",
    "        \n",
    "        self.filenames = [\n",
    "            'user_id',\n",
    "            'master_aisle_id',\n",
    "            'master_department_id',\n",
    "            'IsInOrder_history',\n",
    "            'NextInOrder_history',\n",
    "            'NumProductsFromDep_history',\n",
    "            'OrderSize_history',\n",
    "            'IndexInOrder_history',\n",
    "            'order_number_history',\n",
    "            'order_hour_of_day_history',\n",
    "            'order_dow_history',\n",
    "            'days_since_prior_order_history',\n",
    "            'history_length',\n",
    "            'ProductID1_history_a',\n",
    "            'ProductID2_history_a',\n",
    "            'ProductID3_history_a',\n",
    "            'ProductID4_history_a',\n",
    "            'ProductID5_history_a',\n",
    "            'ProductID6_history_a',\n",
    "            'ProductID7_history_a',\n",
    "            'ProductID8_history_a',\n",
    "            'ProductID9_history_a',\n",
    "            'ProductID10_history_a',\n",
    "            'ProductID11_history_a',\n",
    "            'ProductID12_history_a',\n",
    "            'ProductID13_history_a',\n",
    "            'ProductID14_history_a',\n",
    "            'ProductID15_history_a',\n",
    "            'ProductID16_history_a',\n",
    "            'ProductID17_history_a',\n",
    "            'ProductID18_history_a',   \n",
    "            'ProductID19_history_a',\n",
    "            'ProductID20_history_a',\n",
    "            'ProductID1_history',\n",
    "            'ProductID2_history',\n",
    "            'ProductID3_history',\n",
    "            'ProductID4_history',\n",
    "            'ProductID5_history',\n",
    "            'ProductID6_history',\n",
    "            'ProductID7_history',\n",
    "            'ProductID8_history',\n",
    "            'ProductID9_history',\n",
    "            'ProductID10_history',\n",
    "            'ProductID11_history',\n",
    "            'ProductID12_history',\n",
    "            'ProductID13_history',\n",
    "            'ProductID14_history',\n",
    "            'ProductID15_history',\n",
    "            'ProductID16_history',\n",
    "            'ProductID17_history',\n",
    "            'ProductID18_history',   \n",
    "            'ProductID19_history',\n",
    "            'ProductID20_history',\n",
    "            #'ProductNameEmbedding_history',\n",
    "            'label'\n",
    "        ]\n",
    "        \n",
    "        self.ids = 0\n",
    "        self.start = 0\n",
    "        self.dataFiles = glob(path_out+'user_id*.dat') \n",
    "        self.num_chunks = len(self.dataFiles)\n",
    "        self.chunks_index = 0\n",
    "        \n",
    "        self.start = 0 \n",
    "        self.arrays = [(bcolz.open(self.dataFiles[0].replace('user_id',i))[:]) for i in self.filenames]\n",
    "        self.ids = np.arange(0,len(self.arrays[0]))\n",
    "\n",
    "    def next_batch(self, batch_size, shuffle=True, is_test=False):\n",
    "        while(1):\n",
    "            if (self.arrays[0].shape[0] - self.start) >= batch_size:\n",
    "                batch_ids = self.ids[self.start: (self.start + batch_size)]\n",
    "                self.start += batch_size\n",
    "                yield {globals()[placeholder] : copy.copy(array[batch_ids]) for placeholder, array in zip(self.filenames,self.arrays)} \n",
    "            else:\n",
    "                self.start = 0 \n",
    "                self.chunks_index += 1\n",
    "                if self.chunks_index >= self.num_chunks:\n",
    "                    self.chunks_index = 0\n",
    "                self.arrays = [(bcolz.open(self.dataFiles[self.chunks_index].replace('user_id',i))[:]) for i in self.filenames]\n",
    "                self.ids = np.arange(0,len(self.arrays[0]))\n",
    "                if shuffle:\n",
    "                    random.shuffle(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset(path_out,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_log_loss(y, y_hat, sequence_lengths, max_sequence_length, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Calculates average log loss on variable length sequences.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor of shape [batch size, max_sequence_length, input units].\n",
    "        y_hat: Prediction tensor, same shape as y.\n",
    "        sequence_lengths: Sequence lengths.  Tensor of shape [batch_size].\n",
    "        max_sequence_length: maximum length of padded sequence tensor.\n",
    "\n",
    "    Returns:\n",
    "        Log loss. 0-dimensional tensor.\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.minimum(tf.maximum(y_hat, eps), 1.0 - eps)\n",
    "    log_losses = y*tf.log(y_hat) + (1.0 - y)*tf.log(1.0 - y_hat)\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n",
    "    avg_log_loss = -tf.reduce_sum(log_losses*sequence_mask) / tf.cast(tf.reduce_sum(sequence_lengths), tf.float32)\n",
    "    return avg_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_convolution_layer(inputs, output_units, convolution_width, causal=False, dilation_rate=[1], bias=True,\n",
    "                               activation=None, dropout=None, scope='temporal-convolution-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Convolution over the temporal axis of sequence data.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, input_units].\n",
    "        output_units: Output channels for convolution.\n",
    "        convolution_width: Number of timesteps to use in convolution.\n",
    "        causal: Output at timestep t is a function of inputs at or before timestep t.\n",
    "        dilation_rate:  Dilation rate along temporal axis.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        if causal:\n",
    "            shift = int((convolution_width / 2) + (int(dilation_rate[0] - 1) / 2)) #FIXED THIS\n",
    "            pad = tf.zeros([tf.shape(inputs)[0], shift, inputs.shape.as_list()[2]])\n",
    "            inputs = tf.concat([pad, inputs], axis=1)\n",
    "\n",
    "        W = tf.get_variable(\n",
    "            name='weights',\n",
    "            initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            shape=[convolution_width, shape(inputs, 2), output_units]\n",
    "        )\n",
    "\n",
    "        z = tf.nn.convolution(inputs, W, padding='SAME', dilation_rate=dilation_rate)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                name='biases',\n",
    "                initializer=tf.constant_initializer(),\n",
    "                shape=[output_units]\n",
    "            )\n",
    "            z = z + b\n",
    "        z = activation(z) if activation else z\n",
    "        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "        z = z[:, :-shift, :] if causal else z\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavenet(x, dilations, filter_widths, skip_channels, residual_channels, scope='wavenet', reuse=False):\n",
    "    \"\"\"\n",
    "    A stack of causal dilated convolutions with paramaterized residual and skip connections as described\n",
    "    in the WaveNet paper (with some minor differences).\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape [batch size, max sequence length, input units].\n",
    "        dilations: List of dilations for each layer.  len(dilations) is the number of layers\n",
    "        filter_widths: List of filter widths.  Same length as dilations.\n",
    "        skip_channels: Number of channels to use for skip connections.\n",
    "        residual_channels: Number of channels to use for residual connections.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, len(dilations)*skip_channels].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        # wavenet uses 2x1 conv here\n",
    "        inputs = time_distributed_dense_layer(x, residual_channels, activation=tf.nn.tanh, scope='x-proj')\n",
    "\n",
    "        skip_outputs = []\n",
    "        for i, (dilation, filter_width) in enumerate(zip(dilations, filter_widths)):\n",
    "            dilated_conv = temporal_convolution_layer(\n",
    "                inputs=inputs,\n",
    "                output_units=2*residual_channels,\n",
    "                convolution_width=filter_width,\n",
    "                causal=True,# CHECK THIS OUT\n",
    "                dilation_rate=[dilation],\n",
    "                scope='cnn-{}'.format(i) \n",
    "            )\n",
    "            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "            output_units = skip_channels + residual_channels\n",
    "            outputs = time_distributed_dense_layer(dilated_conv, output_units, scope='cnn-{}-proj'.format(i))\n",
    "            skips, residuals = tf.split(outputs, [skip_channels, residual_channels], axis=2)\n",
    "\n",
    "            inputs += residuals\n",
    "            skip_outputs.append(skips)\n",
    "\n",
    "        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "        return skip_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_layer(inputs, lengths, state_size, keep_prob=1.0, scope='lstm-layer', reuse=False, return_final_state=False):\n",
    "    \"\"\"\n",
    "    LSTM layer.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, ...].\n",
    "        lengths: Tensor of shape [batch size].\n",
    "        state_size: LSTM state size.\n",
    "        keep_prob: 1 - p, where p is the dropout probability.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, state_size] containing the lstm\n",
    "        outputs at each timestep.\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        cell_fw = tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.contrib.rnn.LSTMCell(\n",
    "                state_size,\n",
    "                reuse=reuse\n",
    "            ),\n",
    "            output_keep_prob=keep_prob\n",
    "        )\n",
    "        outputs, output_state = tf.nn.dynamic_rnn(\n",
    "            inputs=inputs,\n",
    "            cell=cell_fw,\n",
    "            sequence_length=lengths,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        if return_final_state:\n",
    "            return outputs, output_state\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_distributed_dense_layer(inputs, output_units, bias=True, activation=None, batch_norm=None,\n",
    "                                 dropout=None, scope='time-distributed-dense-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Applies a shared dense layer to each timestep of a tensor of shape [batch_size, max_seq_len, input_units]\n",
    "    to produce a tensor of shape [batch_size, max_seq_len, output_units].\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, ...].\n",
    "        output_units: Number of output units.\n",
    "        activation: activation function.\n",
    "        dropout: dropout keep prob.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            name='weights',\n",
    "            initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            shape=[shape(inputs, -1), output_units]\n",
    "        )\n",
    "        z = tf.einsum('ijk,kl->ijl', inputs, W)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                name='biases',\n",
    "                initializer=tf.constant_initializer(),\n",
    "                shape=[output_units]\n",
    "            )\n",
    "            z = z + b\n",
    "\n",
    "        if batch_norm is not None:\n",
    "            z = tf.layers.batch_normalization(z, training=batch_norm, reuse=reuse)\n",
    "\n",
    "        z = activation(z) if activation else z\n",
    "        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_log_loss(y, y_hat, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Calculates log loss between two tensors.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor.\n",
    "        y_hat: Prediction tensor\n",
    "\n",
    "    Returns:\n",
    "        Log loss. 0-dimensional tensor.\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.minimum(tf.maximum(y_hat, eps), 1.0 - eps)\n",
    "    log_loss = -tf.reduce_mean(y*tf.log(y_hat) + (1.0 - y)*tf.log(1.0 - y_hat))\n",
    "    return log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable parameters:\n",
      "[('aisle_embeddings:0', [135, 50]), ('dept_embeddings:0', [22, 10]), ('user_embeddings:0', [207000, 50]), ('product_embeddings:0', [50000, 50]), ('lstm1/rnn/lstm_cell/kernel:0', [2754, 1200]), ('lstm1/rnn/lstm_cell/bias:0', [1200]), ('dense1/weights:0', [2754, 50]), ('dense1/biases:0', [50]), ('dense-2/weights:0', [50, 2]), ('dense-2/biases:0', [2])]\n",
      "trainable parameter count:\n",
      "16300822\n",
      "built graph\n"
     ]
    }
   ],
   "source": [
    " with tf.Graph().as_default() as graph:\n",
    "        \n",
    "    user_id = tf.placeholder(tf.int32, [None])\n",
    "    master_aisle_id = tf.placeholder(tf.int32, [None])\n",
    "    master_department_id = tf.placeholder(tf.int32, [None])\n",
    "    history_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    IsInOrder_history = tf.placeholder(tf.int32, [None, N]) #\n",
    "    order_dow_history = tf.placeholder(tf.int32, [None, N])\n",
    "    order_hour_of_day_history = tf.placeholder(tf.int32, [None, N])\n",
    "    days_since_prior_order_history = tf.placeholder(tf.int32, [None, N])\n",
    "    OrderSize_history = tf.placeholder(tf.int32, [None, N]) #\n",
    "    IndexInOrder_history = tf.placeholder(tf.int32, [None, N]) #\n",
    "    order_number_history = tf.placeholder(tf.int32, [None, N])\n",
    "    NumProductsFromDep_history = tf.placeholder(tf.int32, [None, N]) #\n",
    "    NextInOrder_history = tf.placeholder(tf.int32, [None, N])\n",
    "    \n",
    "    label = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    \n",
    "    ProductID1_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID2_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID3_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID4_history_a = tf.placeholder(tf.int32, [None, N])   \n",
    "    ProductID5_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID6_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID7_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID8_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID9_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID11_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID10_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID12_history_a = tf.placeholder(tf.int32, [None, N])   \n",
    "    ProductID13_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID14_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID15_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID16_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID17_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID18_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID19_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID20_history_a = tf.placeholder(tf.int32, [None, N])\n",
    "    \n",
    "    #ProductNameEmbedding_history = tf.placeholder(tf.float32, [None, N, 50])\n",
    "\n",
    "    ProductID1_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID2_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID3_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID4_history = tf.placeholder(tf.int32, [None, N])   \n",
    "    ProductID5_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID6_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID7_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID8_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID9_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID11_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID10_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID12_history = tf.placeholder(tf.int32, [None, N])   \n",
    "    ProductID13_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID14_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID15_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID16_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID17_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID18_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID19_history = tf.placeholder(tf.int32, [None, N])\n",
    "    ProductID20_history = tf.placeholder(tf.int32, [None, N])\n",
    "  \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    # aisle data\n",
    "    aisle_embeddings = tf.get_variable(\n",
    "        name='aisle_embeddings',\n",
    "        shape=[135, 50],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "  \n",
    "    # department data\n",
    "    dept_embeddings = tf.get_variable(\n",
    "        name='dept_embeddings',\n",
    "        shape=[22, 10],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "  \n",
    "    x_aisle = tf.concat([\n",
    "        tf.nn.embedding_lookup(aisle_embeddings, master_aisle_id),\n",
    "        tf.nn.embedding_lookup(dept_embeddings, master_department_id),\n",
    "    ], axis=1)\n",
    "    x_aisle = tf.tile(tf.expand_dims(x_aisle, 1), (1, N, 1))\n",
    "\n",
    "    # user data\n",
    "    user_embeddings = tf.get_variable(\n",
    "        name='user_embeddings',\n",
    "        shape=[207000, min(lstm_size,50)],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    x_user = tf.nn.embedding_lookup(user_embeddings, user_id)\n",
    "    x_user = tf.tile(tf.expand_dims(x_user, 1), (1, N, 1))\n",
    "\n",
    "    # product data\n",
    "    product_embeddings = tf.get_variable(\n",
    "        name='product_embeddings',\n",
    "        shape=[50000, 50],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "       \n",
    "    x_product_1a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID1_history_a))\n",
    "    x_product_2a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID2_history_a))\n",
    "    x_product_3a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID3_history_a))\n",
    "    x_product_4a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID4_history_a))\n",
    "    x_product_5a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID5_history_a))\n",
    "    x_product_6a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID6_history_a))\n",
    "    x_product_7a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID7_history_a))\n",
    "    x_product_8a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID8_history_a))\n",
    "    x_product_9a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID9_history_a))\n",
    "    x_product_10a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID10_history_a))\n",
    "    x_product_11a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID11_history_a))\n",
    "    x_product_12a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID12_history_a))\n",
    "    x_product_13a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID13_history_a))\n",
    "    x_product_14a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID14_history_a))\n",
    "    x_product_15a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID15_history_a))\n",
    "    x_product_16a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID16_history_a))\n",
    "    x_product_17a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID17_history_a))\n",
    "    x_product_18a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID18_history_a))\n",
    "    x_product_19a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID19_history_a))\n",
    "    x_product_20a = tf.nn.embedding_lookup(aisle_embeddings, (ProductID20_history_a))\n",
    "    \n",
    "    x_product_1 = tf.nn.embedding_lookup(product_embeddings, (ProductID1_history))\n",
    "    x_product_2 = tf.nn.embedding_lookup(product_embeddings, (ProductID2_history))\n",
    "    x_product_3 = tf.nn.embedding_lookup(product_embeddings, (ProductID3_history))\n",
    "    x_product_4 = tf.nn.embedding_lookup(product_embeddings, (ProductID4_history))\n",
    "    x_product_5 = tf.nn.embedding_lookup(product_embeddings, (ProductID5_history))\n",
    "    x_product_6 = tf.nn.embedding_lookup(product_embeddings, (ProductID6_history))\n",
    "    x_product_7 = tf.nn.embedding_lookup(product_embeddings, (ProductID7_history))\n",
    "    x_product_8 = tf.nn.embedding_lookup(product_embeddings, (ProductID8_history))\n",
    "    x_product_9 = tf.nn.embedding_lookup(product_embeddings, (ProductID9_history))\n",
    "    x_product_10 = tf.nn.embedding_lookup(product_embeddings, (ProductID10_history))\n",
    "    x_product_11 = tf.nn.embedding_lookup(product_embeddings, (ProductID11_history))\n",
    "    x_product_12 = tf.nn.embedding_lookup(product_embeddings, (ProductID12_history))\n",
    "    x_product_13 = tf.nn.embedding_lookup(product_embeddings, (ProductID13_history))\n",
    "    x_product_14 = tf.nn.embedding_lookup(product_embeddings, (ProductID14_history))\n",
    "    x_product_15 = tf.nn.embedding_lookup(product_embeddings, (ProductID15_history))\n",
    "    x_product_16 = tf.nn.embedding_lookup(product_embeddings, (ProductID16_history))\n",
    "    x_product_17 = tf.nn.embedding_lookup(product_embeddings, (ProductID17_history))\n",
    "    x_product_18 = tf.nn.embedding_lookup(product_embeddings, (ProductID18_history))\n",
    "    x_product_19 = tf.nn.embedding_lookup(product_embeddings, (ProductID19_history))\n",
    "    x_product_20 = tf.nn.embedding_lookup(product_embeddings, (ProductID20_history))\n",
    "    \n",
    "   \n",
    "    # sequence data\n",
    "    IsInOrder_history_oh = tf.one_hot(IsInOrder_history, 2)\n",
    "    order_dow_history_oh = tf.one_hot(order_dow_history, 8)\n",
    "    order_hour_of_day_history_oh = tf.one_hot(order_hour_of_day_history, 25)\n",
    "    days_since_prior_order_history_oh = tf.one_hot(days_since_prior_order_history, 31)\n",
    "    OrderSize_history_oh = tf.one_hot(OrderSize_history, 60)\n",
    "    IndexInOrder_history_oh = tf.one_hot(OrderSize_history, 60)\n",
    "    order_number_history_oh = tf.one_hot(order_number_history, 101)\n",
    "    NumProductsFromDep_history_oh = tf.one_hot(NumProductsFromDep_history, 50)\n",
    "    \n",
    "    order_dow_history_scalar = tf.expand_dims(tf.cast(order_dow_history, tf.float32) / 8.0, 2)\n",
    "    order_hour_of_day_history_scalar = tf.expand_dims(tf.cast(order_hour_of_day_history, tf.float32) / 25.0, 2)\n",
    "    days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "    OrderSize_history_scalar = tf.expand_dims(tf.cast(OrderSize_history, tf.float32) / 60.0, 2)\n",
    "    IndexInOrder_history_scalar = tf.expand_dims(tf.cast(IndexInOrder_history, tf.float32) / 60.0, 2)\n",
    "    order_number_history_scalar = tf.expand_dims(tf.cast(order_number_history, tf.float32) / 100.0, 2)\n",
    "    NumProductsFromDep_history_scalar = tf.expand_dims(tf.cast(NumProductsFromDep_history, tf.float32) / 50.0, 2)\n",
    "\n",
    "\n",
    "\n",
    "    x_history = tf.concat([IsInOrder_history_oh,order_dow_history_oh,order_hour_of_day_history_oh,days_since_prior_order_history_oh,\\\n",
    "        OrderSize_history_oh,IndexInOrder_history_oh,NumProductsFromDep_history_oh,order_number_history_oh,order_dow_history_scalar,\\\n",
    "        order_hour_of_day_history_scalar,days_since_prior_order_history_scalar,OrderSize_history_scalar,IndexInOrder_history_scalar,\\\n",
    "        order_number_history_scalar,NumProductsFromDep_history_scalar,x_product_1,x_product_2,x_product_3,\\\n",
    "        x_product_4,x_product_5,x_product_6,x_product_7,x_product_8,x_product_9\\\n",
    "        ,x_product_10,x_product_11,x_product_12,x_product_13,\\\n",
    "        x_product_14,x_product_15,x_product_16,x_product_17,x_product_18,\\\n",
    "        x_product_19,x_product_20,x_product_1a,x_product_2a,x_product_3a,\\\n",
    "        x_product_4a,x_product_5a,x_product_6a,x_product_7a,x_product_8a,x_product_9a\\\n",
    "        ,x_product_10a,x_product_11a,x_product_12a,x_product_13a,\\\n",
    "        x_product_14a,x_product_15a,x_product_16a,x_product_17a,x_product_18a,\\\n",
    "        x_product_19a,x_product_20a], axis=2) #ProductNameEmbedding_history\n",
    "        #], axis=2)\n",
    "    \n",
    "    x = tf.concat([x_history, x_aisle, x_user], axis=2)\n",
    "    #h = lstm_layer(x, history_length, lstm_size, scope='lstm1')\n",
    "\n",
    "    if WaveNet:\n",
    "        c = wavenet(x, dilations, filter_widths, skip_channels, residual_channels)\n",
    "        h = tf.concat([c , h], axis=2)\n",
    "    else:\n",
    "        h = lstm_layer(x, history_length, lstm_size, scope='lstm1')\n",
    "        h = tf.concat([h , x], axis=2)\n",
    "    \n",
    "\n",
    "    h_final = time_distributed_dense_layer(h, 50 , activation=tf.nn.relu, scope='dense1') \n",
    "    #y_hat = tf.squeeze(time_distributed_dense_layer(h_final, 1, activation=tf.nn.sigmoid, scope='dense3'), 2)\n",
    "\n",
    "    n_components = 1\n",
    "    params = time_distributed_dense_layer(h_final, n_components*2, scope='dense-2', activation=None)\n",
    "    ps, mixing_coefs = tf.split(params, 2, axis=2)\n",
    "\n",
    "    # this is implemented incorrectly, but it still helped...\n",
    "    mixing_coefs = tf.nn.softmax(mixing_coefs - tf.reduce_min(mixing_coefs, 2, keep_dims=True))\n",
    "    ps = tf.nn.sigmoid(ps)\n",
    "\n",
    "    #loss = sequence_log_loss(NextInOrder_history, preds, history_length, N)\n",
    "    labels = tf.tile(tf.expand_dims(NextInOrder_history, 2), (1, 1, n_components))\n",
    "    losses = tf.reduce_sum(mixing_coefs*tensor_log_loss(labels, ps), axis=2)\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(history_length, maxlen=N), tf.float32)\n",
    "    loss = tf.reduce_sum(losses*sequence_mask) / tf.cast(tf.reduce_sum(history_length), tf.float32)\n",
    "\n",
    "    \n",
    "    final_temporal_idx = tf.stack([tf.range(tf.shape(history_length)[0]), history_length - 1], axis=1)\n",
    "    final_states = tf.gather_nd(h_final, final_temporal_idx)\n",
    "    #final_predictions = tf.gather_nd(y_hat, final_temporal_idx)\n",
    "    final_predictions = tf.gather_nd(ps, final_temporal_idx)\n",
    "    ground_truth = tf.gather_nd(NextInOrder_history, final_temporal_idx)\n",
    "    \n",
    "    prediction_tensors = {\n",
    "        'user_ids': user_id,\n",
    "        'aisle_ids': master_aisle_id,\n",
    "        'final_states': final_states,\n",
    "        'predictions': final_predictions,\n",
    "        'ground_truth': label, #BUG FIX\n",
    "        'history_length':history_length\n",
    "    }\n",
    "    #preds = y_hat\n",
    "    preds = ps\n",
    "        \n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate_var = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "    if regularization_constant != 0:\n",
    "        l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()])\n",
    "        loss = loss + regularization_constant*l2_norm\n",
    "\n",
    "    if opt == 'adam':\n",
    "        optimizer =  tf.train.AdamOptimizer(learning_rate_var)\n",
    "    elif opt == 'gd':\n",
    "        optimizer =  tf.train.GradientDescentOptimizer(learning_rate_var)\n",
    "    elif opt == 'rms':\n",
    "        optimizer =  tf.train.RMSPropOptimizer(learning_rate_var, decay=0.95, momentum=0.9)\n",
    "    else:\n",
    "        assert False, 'optimizer must be adam, gd, or rms'\n",
    "            \n",
    "    grads = optimizer.compute_gradients(loss)\n",
    "    clipped = [(tf.clip_by_value(g, -grad_clip, grad_clip), v_) for g, v_ in grads]\n",
    "\n",
    "    step = optimizer.apply_gradients(clipped, global_step=global_step)\n",
    "\n",
    "    print('trainable parameters:')\n",
    "    print([(var.name, shape(var)) for var in tf.trainable_variables()])\n",
    "\n",
    "    print('trainable parameter count:')\n",
    "    print(str(np.sum(np.prod(shape(var)) for var in tf.trainable_variables())))\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "session = tf.Session(graph=graph)\n",
    "print('built graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[step        0]]     [[train]]     loss: 0.71334159       [[val]]     loss: 0.34576926       [[val-seq]]     loss: 0.65176511       \n",
      "[[step     1000]]     [[train]]     loss: 0.09504382       [[val]]     loss: 0.44454406       [[val-seq]]     loss: 0.09420364       \n",
      "[[step     2000]]     [[train]]     loss: 0.08172654       [[val]]     loss: 0.43615381       [[val-seq]]     loss: 0.08138814       \n",
      "[[step     3000]]     [[train]]     loss: 0.08020399       [[val]]     loss: 0.43064464       [[val-seq]]     loss: 0.07983331       \n",
      "[[step     4000]]     [[train]]     loss: 0.08006256       [[val]]     loss: 0.42614842       [[val-seq]]     loss: 0.07967393       \n",
      "[[step     5000]]     [[train]]     loss: 0.08001398       [[val]]     loss: 0.42456471       [[val-seq]]     loss: 0.07961224       \n",
      "[[step     6000]]     [[train]]     loss: 0.07935135       [[val]]     loss: 0.42444403       [[val-seq]]     loss: 0.07894602       \n",
      "[[step     7000]]     [[train]]     loss: 0.07960653       [[val]]     loss: 0.42023837       [[val-seq]]     loss: 0.07919458       \n",
      "[[step     8000]]     [[train]]     loss: 0.0796378        [[val]]     loss: 0.42037848       [[val-seq]]     loss: 0.07922103       \n",
      "[[step     9000]]     [[train]]     loss: 0.07963806       [[val]]     loss: 0.42187326       [[val-seq]]     loss: 0.07922563       \n",
      "[[step    10000]]     [[train]]     loss: 0.0788629        [[val]]     loss: 0.42438231       [[val-seq]]     loss: 0.07845371       \n",
      "[[step    11000]]     [[train]]     loss: 0.07862586       [[val]]     loss: 0.42192358       [[val-seq]]     loss: 0.07821091       \n",
      "[[step    12000]]     [[train]]     loss: 0.07874734       [[val]]     loss: 0.4221826        [[val-seq]]     loss: 0.07833616       \n",
      "[[step    13000]]     [[train]]     loss: 0.07877503       [[val]]     loss: 0.42093561       [[val-seq]]     loss: 0.07837122       \n",
      "[[step    14000]]     [[train]]     loss: 0.0787603        [[val]]     loss: 0.41932996       [[val-seq]]     loss: 0.07835104       \n",
      "[[step    15000]]     [[train]]     loss: 0.07856092       [[val]]     loss: 0.41958562       [[val-seq]]     loss: 0.07815312       \n",
      "[[step    16000]]     [[train]]     loss: 0.07867923       [[val]]     loss: 0.42442146       [[val-seq]]     loss: 0.07827508       \n",
      "[[step    17000]]     [[train]]     loss: 0.07823178       [[val]]     loss: 0.41935918       [[val-seq]]     loss: 0.07783313       \n",
      "[[step    18000]]     [[train]]     loss: 0.07783984       [[val]]     loss: 0.42184813       [[val-seq]]     loss: 0.07744181       \n",
      "[[step    19000]]     [[train]]     loss: 0.07788386       [[val]]     loss: 0.42251171       [[val-seq]]     loss: 0.07748958       \n",
      "[[step    20000]]     [[train]]     loss: 0.07799802       [[val]]     loss: 0.42168921       [[val-seq]]     loss: 0.07760165       \n",
      "[[step    21000]]     [[train]]     loss: 0.07790401       [[val]]     loss: 0.42275472       [[val-seq]]     loss: 0.07751111       \n",
      "[[step    22000]]     [[train]]     loss: 0.07772372       [[val]]     loss: 0.42114883       [[val-seq]]     loss: 0.07733175       \n",
      "[[step    23000]]     [[train]]     loss: 0.07837459       [[val]]     loss: 0.42146789       [[val-seq]]     loss: 0.07798391       \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-88e43dbf2e95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    253\u001b[0m         train_loss,_,np_tensors = session.run(   #changed\n\u001b[0;32m    254\u001b[0m             \u001b[0mfetches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf_tensors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#changed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m         )\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with session.as_default():\n",
    "    debug = False\n",
    "\n",
    "    if warm_start_init_step:\n",
    "        restore(warm_start_init_step)\n",
    "        step_number = warm_start_init_step\n",
    "    else:\n",
    "        session.run(init)\n",
    "        step_number = 0\n",
    "\n",
    "    best_validation_loss, best_validation_tstep = float('inf'), 0\n",
    "    restarts = 0\n",
    "    \n",
    "    train_loss_history = deque(maxlen=loss_averaging_window)\n",
    "    val_loss_history = deque(maxlen=loss_averaging_window)\n",
    "    #val_loss_history.append(0)\n",
    "    val_seq_loss_history = deque(maxlen=loss_averaging_window)\n",
    "    \n",
    "    while step_number < num_training_steps:\n",
    "\n",
    "        next_batch =  next(train.next_batch(batch_size))\n",
    "                    \n",
    "        # train step\n",
    "        train_feed_dict = next_batch\n",
    "\n",
    "        # Use n-2 for training so that n-1 can be used for validation\n",
    "\n",
    "        if debug:\n",
    "            print(next_batch[globals()['history_length']])\n",
    "        train_feed_dict.update({globals()['history_length']:train_feed_dict[globals()['history_length']]-1})\n",
    "        \n",
    "        if debug:\n",
    "            print(train_feed_dict[globals()['history_length']])\n",
    "\n",
    "        sequence_lengths = train_feed_dict[globals()['history_length']]\n",
    "        if debug:\n",
    "            print(sequence_lengths)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "        \n",
    "        # Remove the input data for n-1 step for training to avoid leakage\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['NextInOrder_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['NextInOrder_history']:temp_array})\n",
    "            \n",
    "            temp_array = train_feed_dict[globals()['order_number_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['order_number_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['order_hour_of_day_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['order_hour_of_day_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['order_dow_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['order_dow_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['days_since_prior_order_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['days_since_prior_order_history']:temp_array})\n",
    "                       \n",
    "            temp_array = train_feed_dict[globals()['IsInOrder_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['IsInOrder_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['OrderSize_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['OrderSize_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['IndexInOrder_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['IndexInOrder_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['NumProductsFromDep_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['NumProductsFromDep_history']:temp_array})\n",
    "\n",
    "            if True:\n",
    "                temp_array = train_feed_dict[globals()['ProductID1_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID1_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID2_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID2_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID3_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID3_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID4_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID4_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID5_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID5_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID6_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID6_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID7_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID7_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID8_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID8_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID9_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID9_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID10_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID10_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID11_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID11_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID12_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID12_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID13_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID13_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID14_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID14_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID15_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID15_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID16_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID16_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID17_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID17_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID18_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID18_history_a']:temp_array})      \n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID19_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID19_history_a']:temp_array})\n",
    "\n",
    "                temp_array = train_feed_dict[globals()['ProductID20_history_a']]\n",
    "                temp_array[i,sequence_lengths[i]]=0\n",
    "                train_feed_dict.update({globals()['ProductID20_history_a']:temp_array})\n",
    "\n",
    "            #temp_array = train_feed_dict[globals()['ProductNameEmbedding_history']]\n",
    "            #temp_array[i,sequence_lengths[i]]=np.zeros(shape=[50],dtype=np.float16)\n",
    "            #train_feed_dict.update({globals()['ProductNameEmbedding_history']:temp_array})\n",
    "            \n",
    "            temp_array = train_feed_dict[globals()['ProductID1_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID1_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID2_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID2_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID3_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID3_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID4_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID4_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID5_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID5_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID6_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID6_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID7_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID7_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID8_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID8_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID9_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID9_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID10_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID10_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID11_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID11_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID12_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID12_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID13_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID13_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID14_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID14_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID15_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID15_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID16_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID16_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID17_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID17_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID18_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID18_history']:temp_array})      \n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID19_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID19_history']:temp_array})\n",
    "\n",
    "            temp_array = train_feed_dict[globals()['ProductID20_history']]\n",
    "            temp_array[i,sequence_lengths[i]]=0\n",
    "            train_feed_dict.update({globals()['ProductID20_history']:temp_array})\n",
    "\n",
    "            \n",
    "            \n",
    "        train_feed_dict.update({learning_rate_var: learning_rate})\n",
    "        train_feed_dict.update({keep_prob: keep_prob_scalar})\n",
    "        train_feed_dict.update({is_training: True})\n",
    "                \n",
    "        debug_dict = {tensor_name: [] for tensor_name in prediction_tensors} #changed\n",
    "        tensor_names, tf_tensors = zip(*prediction_tensors.items()) #changed\n",
    "\n",
    "        train_loss,_,np_tensors = session.run(   #changed\n",
    "            fetches=[loss,step,tf_tensors], #changed\n",
    "            feed_dict=train_feed_dict\n",
    "        )\n",
    "        \n",
    "        for tensor_name, tensor in zip(tensor_names, np_tensors): #changed\n",
    "            debug_dict[tensor_name].append(tensor)#changed \n",
    "        \n",
    "        if debug:\n",
    "            print(debug_dict['history_length'])#changed\n",
    "        \n",
    "        train_loss_history.append(train_loss)\n",
    "        \n",
    "        #BEGIN\n",
    "        prediction_dict = {tensor_name: [] for tensor_name in prediction_tensors}\n",
    "\n",
    "        # test evaluation\n",
    "        \n",
    "        val_feed_dict = next_batch\n",
    "        val_feed_dict.update({learning_rate_var: learning_rate})\n",
    "        val_feed_dict.update({keep_prob: keep_prob_scalar})\n",
    "        val_feed_dict.update({is_training: False})\n",
    "\n",
    "        if debug:\n",
    "            print(val_feed_dict[globals()['history_length']]+1)\n",
    "        \n",
    "        tensor_names, tf_tensors = zip(*prediction_tensors.items())\n",
    "        val_seq_loss,np_tensors = session.run(\n",
    "            fetches=[loss,tf_tensors],\n",
    "            feed_dict=val_feed_dict\n",
    "        )\n",
    "\n",
    "        for tensor_name, tensor in zip(tensor_names, np_tensors):\n",
    "            prediction_dict[tensor_name].append(tensor)\n",
    "        #print(prediction_dict['ground_truth'])\n",
    "        #print(prediction_dict['predictions'])\n",
    "        if(np.sum(prediction_dict['ground_truth'][0])>0):\n",
    "            val_loss = log_loss(prediction_dict['ground_truth'][0],prediction_dict['predictions'][0])\n",
    "        #END\n",
    "            val_loss_history.append(val_loss)\n",
    "        val_seq_loss_history.append(val_seq_loss)\n",
    "        \n",
    "        if debug:\n",
    "            #print(train_loss, val_loss)\n",
    "            #print(prediction_dict['ground_truth'])\n",
    "            print(prediction_dict['history_length'])\n",
    "            break\n",
    "        \n",
    "        if step_number % log_interval == 0:\n",
    "            avg_train_loss = sum(train_loss_history)/len(train_loss_history)\n",
    "            denominator = len(val_loss_history)\n",
    "            if denominator ==0:\n",
    "                denominator=1\n",
    "            avg_val_loss = sum(val_loss_history)/denominator\n",
    "            avg_val_seq_loss = sum(val_seq_loss_history)/len(val_seq_loss_history)\n",
    "            metric_log = (\n",
    "                \"[[step {:>8}]]     \"\n",
    "                \"[[train]]     loss: {:<12}     \"\n",
    "                \"[[val]]     loss: {:<12}     \"\n",
    "                \"[[val-seq]]     loss: {:<12}     \"\n",
    "            ).format(step_number, round(avg_train_loss, 8), round(avg_val_loss, 8), round(avg_val_seq_loss, 8))\n",
    "            print(metric_log)\n",
    "            \n",
    "            if avg_val_loss < best_validation_loss:\n",
    "                best_validation_loss = avg_val_loss\n",
    "                best_validation_tstep = step_number\n",
    "                if step_number > min_steps_to_checkpoint:\n",
    "                    model_path = os.path.join(checkpoint_dir, 'model{}.ckpt'.format(best_validation_tstep))\n",
    "                    saver.save(session, model_path, global_step=best_validation_tstep) \n",
    "                    \n",
    "            if step_number - best_validation_tstep > early_stopping_steps:\n",
    "\n",
    "                if num_restarts is None or restarts >= num_restarts:\n",
    "                    print('best validation loss of {} at training step {}'.format(\n",
    "                        best_validation_loss, best_validation_tstep))\n",
    "                    print('early stopping - ending training.')\n",
    "                    break\n",
    "\n",
    "                if restarts < num_restarts:\n",
    "                    model_path = os.path.join(checkpoint_dir, 'model{}.ckpt-{}'.format(best_validation_tstep,best_validation_tstep))\n",
    "                    saver.restore(session, model_path)\n",
    "                    print('halving learning rate')\n",
    "                    learning_rate /= 2.0\n",
    "                    early_stopping_steps /= 2\n",
    "                    step_number = best_validation_tstep\n",
    "                    restarts += 1\n",
    "\n",
    "        step_number += 1\n",
    "\n",
    "    if step_number <= min_steps_to_checkpoint:\n",
    "        best_validation_tstep = step_number\n",
    "        model_path = os.path.join(checkpoint_dir, 'model{}.ckpt'.format(best_validation_tstep))\n",
    "        saver.save(session, model_path, global_step=best_validation_tstep) \n",
    "                    \n",
    "    print('num_training_steps reached - ending training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3457692607771605, 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validation_loss, best_validation_tstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_validation_tstep = 14000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\checkpoints\\\\model14000.ckpt-14000'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join(checkpoint_dir, 'model{}.ckpt'.format(best_validation_tstep))\n",
    "model_path\n",
    "saver.save(session, model_path, global_step=best_validation_tstep) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = dataset(path_out,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\chapanda\\data\\np\\checkpoints\\model14000.ckpt-14000\n"
     ]
    }
   ],
   "source": [
    "model_path='C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\checkpoints\\\\model14000.ckpt-14000'\n",
    "saver.restore(session, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39062\n"
     ]
    }
   ],
   "source": [
    "prediction_dict = {tensor_name: [] for tensor_name in prediction_tensors}\n",
    "\n",
    "num_val_batches = int(val.arrays[0].shape[0]/batch_size)\n",
    "num_val_batches = int(5000000/batch_size)\n",
    "print(num_val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic()\n",
    "for i in range(num_val_batches):\n",
    "    val_dict_list =  next(val.next_batch(batch_size))\n",
    "        \n",
    "    # test evaluation\n",
    "    val_feed_dict = val_dict_list\n",
    "    val_feed_dict.update({learning_rate_var: learning_rate})\n",
    "    val_feed_dict.update({keep_prob: 1.0})\n",
    "    val_feed_dict.update({is_training: False})\n",
    "\n",
    "    tensor_names, tf_tensors = zip(*prediction_tensors.items())\n",
    "    np_tensors = session.run(\n",
    "        fetches=tf_tensors,\n",
    "        feed_dict=val_feed_dict\n",
    "    )\n",
    "    for tensor_name, tensor in zip(tensor_names, np_tensors):\n",
    "        prediction_dict[tensor_name].append(tensor)\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tensor_name, tensor in prediction_dict.items():\n",
    "    np_tensor = np.concatenate(tensor, 0)\n",
    "    save_file = os.path.join(prediction_dir, '{}.npy'.format(tensor_name))\n",
    "    print('saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "    np.save(save_file, np_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLift(predics,actual, user_ids , a_ids ,verbosity=0):\n",
    "    \n",
    "    aisles = np.unique(a_ids)\n",
    "    users = np.unique(user_ids)\n",
    "    cum_lift = 0.0\n",
    "    cnt = 0\n",
    "\n",
    "    for n in range(len(aisles)):\n",
    "\n",
    "        predics_aisle = predics[np.where(a_ids==aisles[n])]\n",
    "        actual_aisle = actual[np.where(a_ids==aisles[n])]\n",
    "        users_aisle = user_ids[np.where(a_ids==aisles[n])]\n",
    "\n",
    "        i = np.where(predics_aisle>np.percentile(predics_aisle,90))\n",
    "        j = np.where(predics_aisle>0)\n",
    "        num_i = len(i[0])*1.0\n",
    "        num_j = len(j[0])*1.0\n",
    "        num_users = users.shape[0]\n",
    "\n",
    "        up = (np.sum(actual_aisle[i[0]])*1.0)/num_i\n",
    "        down = (np.sum(actual_aisle[j[0]])*1.0)/num_j\n",
    "        lift = round(up/down,3)\n",
    "        if lift>0:\n",
    "            cum_lift += lift\n",
    "            cnt += 1\n",
    "\n",
    "        if verbosity==1:\n",
    "            print(lift , round(num_i/num_j,3), round((num_j*100.0)/num_users,2))\n",
    "\n",
    "    return round(cum_lift/cnt,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predics = np.load(prediction_dir+'\\\\predictions.npy')\n",
    "actual = np.load(prediction_dir+'\\\\ground_truth.npy')\n",
    "a_ids = np.load(prediction_dir+'\\\\aisle_ids.npy')\n",
    "u_ids = np.load(prediction_dir+'\\\\user_ids.npy')\n",
    "print(predics.shape,actual.shape, a_ids.shape, u_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(findLift(predics,actual,u_ids, a_ids,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "N = 100\n",
    "debug = False\n",
    "batch_size = 128\n",
    "num_training_steps = 300000\n",
    "learning_rate = 0.001\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 35000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 5000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "#dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "#filter_widths=filter_widths+filter_widths\n",
    "skip_channels=64\n",
    "residual_channels=96\n",
    "\n",
    "trainable parameters:\n",
    "[('aisle_embeddings:0', [135, 50]), ('dept_embeddings:0', [22, 10]), ('user_embeddings:0', [207000, 50]), ('lstm1/rnn/lstm_cell/kernel:0', [1754, 1200]), ('lstm1/rnn/lstm_cell/bias:0', [1200]), ('wavenet/x-proj/weights:0', [1454, 96]), ('wavenet/x-proj/biases:0', [96]), ('wavenet/cnn-0/weights:0', [2, 96, 192]), ('wavenet/cnn-0/biases:0', [192]), ('wavenet/cnn-0-proj/weights:0', [96, 160]), ('wavenet/cnn-0-proj/biases:0', [160]), ('wavenet/cnn-1/weights:0', [2, 96, 192]), ('wavenet/cnn-1/biases:0', [192]), ('wavenet/cnn-1-proj/weights:0', [96, 160]), ('wavenet/cnn-1-proj/biases:0', [160]), ('wavenet/cnn-2/weights:0', [2, 96, 192]), ('wavenet/cnn-2/biases:0', [192]), ('wavenet/cnn-2-proj/weights:0', [96, 160]), ('wavenet/cnn-2-proj/biases:0', [160]), ('wavenet/cnn-3/weights:0', [2, 96, 192]), ('wavenet/cnn-3/biases:0', [192]), ('wavenet/cnn-3-proj/weights:0', [96, 160]), ('wavenet/cnn-3-proj/biases:0', [160]), ('wavenet/cnn-4/weights:0', [2, 96, 192]), ('wavenet/cnn-4/biases:0', [192]), ('wavenet/cnn-4-proj/weights:0', [96, 160]), ('wavenet/cnn-4-proj/biases:0', [160]), ('wavenet/cnn-5/weights:0', [2, 96, 192]), ('wavenet/cnn-5/biases:0', [192]), ('wavenet/cnn-5-proj/weights:0', [96, 160]), ('wavenet/cnn-5-proj/biases:0', [160]), ('dense1/weights:0', [794, 50]), ('dense1/biases:0', [50]), ('dense2/weights:0', [50, 1]), ('dense2/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "12957907\n",
    "built graph\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.79283637       [[val]]     loss: 0.60624765       [[val-seq]]     loss: 0.59424144       \n",
    "[[step     1000]]     [[train]]     loss: 0.38630329       [[val]]     loss: 0.43172749       [[val-seq]]     loss: 0.38367621       \n",
    "[[step     2000]]     [[train]]     loss: 0.37829497       [[val]]     loss: 0.42406548       [[val-seq]]     loss: 0.37603256       \n",
    "[[step     3000]]     [[train]]     loss: 0.37626992       [[val]]     loss: 0.42579533       [[val-seq]]     loss: 0.37400462       \n",
    "[[step     4000]]     [[train]]     loss: 0.37561555       [[val]]     loss: 0.42661017       [[val-seq]]     loss: 0.37342759       \n",
    "[[step     5000]]     [[train]]     loss: 0.37550599       [[val]]     loss: 0.42605413       [[val-seq]]     loss: 0.37332096       \n",
    "[[step     6000]]     [[train]]     loss: 0.37529765       [[val]]     loss: 0.42040225       [[val-seq]]     loss: 0.37305216       \n",
    "[[step     7000]]     [[train]]     loss: 0.37498146       [[val]]     loss: 0.42369202       [[val-seq]]     loss: 0.3727545        \n",
    "[[step     8000]]     [[train]]     loss: 0.37255951       [[val]]     loss: 0.42120498       [[val-seq]]     loss: 0.37030773       \n",
    "[[step     9000]]     [[train]]     loss: 0.37495045       [[val]]     loss: 0.42352376       [[val-seq]]     loss: 0.3726997        \n",
    "[[step    10000]]     [[train]]     loss: 0.3718529        [[val]]     loss: 0.42546232       [[val-seq]]     loss: 0.36962371       \n",
    "[[step    11000]]     [[train]]     loss: 0.37165166       [[val]]     loss: 0.42362015       [[val-seq]]     loss: 0.36943411       \n",
    "[[step    12000]]     [[train]]     loss: 0.37223182       [[val]]     loss: 0.42399921       [[val-seq]]     loss: 0.36999945       \n",
    "[[step    13000]]     [[train]]     loss: 0.37197555       [[val]]     loss: 0.42375858       [[val-seq]]     loss: 0.36979175       \n",
    "[[step    14000]]     [[train]]     loss: 0.3705603        [[val]]     loss: 0.42247732       [[val-seq]]     loss: 0.36836915       \n",
    "[[step    15000]]     [[train]]     loss: 0.37174754       [[val]]     loss: 0.42055816       [[val-seq]]     loss: 0.36956906       \n",
    "[[step    16000]]     [[train]]     loss: 0.36986868       [[val]]     loss: 0.4280738        [[val-seq]]     loss: 0.36770084       \n",
    "[[step    17000]]     [[train]]     loss: 0.36993722       [[val]]     loss: 0.42495414       [[val-seq]]     loss: 0.36776946       \n",
    "[[step    18000]]     [[train]]     loss: 0.36878239       [[val]]     loss: 0.42167605       [[val-seq]]     loss: 0.3666584        \n",
    "[[step    19000]]     [[train]]     loss: 0.36844754       [[val]]     loss: 0.42399498       [[val-seq]]     loss: 0.36627883       \n",
    "[[step    20000]]     [[train]]     loss: 0.36800708       [[val]]     loss: 0.42623461       [[val-seq]]     loss: 0.36582198       \n",
    "[[step    21000]]     [[train]]     loss: 0.36885805       [[val]]     loss: 0.42304146       [[val-seq]]     loss: 0.36672158       \n",
    "[[step    22000]]     [[train]]     loss: 0.36758876       [[val]]     loss: 0.42467972       [[val-seq]]     loss: 0.36545116       \n",
    "[[step    23000]]     [[train]]     loss: 0.36837649       [[val]]     loss: 0.42312657       [[val-seq]]     loss: 0.36626064       \n",
    "[[step    24000]]     [[train]]     loss: 0.36790891       [[val]]     loss: 0.42422104       [[val-seq]]     loss: 0.365809         \n",
    "[[step    25000]]     [[train]]     loss: 0.36705284       [[val]]     loss: 0.4248171        [[val-seq]]     loss: 0.36494182       \n",
    "[[step    26000]]     [[train]]     loss: 0.36734557       [[val]]     loss: 0.42304009       [[val-seq]]     loss: 0.36525052       \n",
    "[[step    27000]]     [[train]]     loss: 0.36714028       [[val]]     loss: 0.42480437       [[val-seq]]     loss: 0.36501321       \n",
    "[[step    28000]]     [[train]]     loss: 0.36800031       [[val]]     loss: 0.42523621       [[val-seq]]     loss: 0.36589179       \n",
    "[[step    29000]]     [[train]]     loss: 0.36751857       [[val]]     loss: 0.42673968       [[val-seq]]     loss: 0.36540843       \n",
    "[[step    30000]]     [[train]]     loss: 0.36729939       [[val]]     loss: 0.42805389       [[val-seq]]     loss: 0.36519097       \n",
    "[[step    31000]]     [[train]]     loss: 0.3658448        [[val]]     loss: 0.4232727        [[val-seq]]     loss: 0.36377487       \n",
    "[[step    32000]]     [[train]]     loss: 0.36803557       [[val]]     loss: 0.42449223       [[val-seq]]     loss: 0.36593829       \n",
    "[[step    33000]]     [[train]]     loss: 0.3669557        [[val]]     loss: 0.42459368       [[val-seq]]     loss: 0.36485877       \n",
    "[[step    34000]]     [[train]]     loss: 0.36828809       [[val]]     loss: 0.42800068       [[val-seq]]     loss: 0.36620096       \n",
    "[[step    35000]]     [[train]]     loss: 0.36536645       [[val]]     loss: 0.4269611        [[val-seq]]     loss: 0.36324265       \n",
    "[[step    36000]]     [[train]]     loss: 0.36635399       [[val]]     loss: 0.42869828       [[val-seq]]     loss: 0.36428317       \n",
    "[[step    37000]]     [[train]]     loss: 0.36469545       [[val]]     loss: 0.4243258        [[val-seq]]     loss: 0.36259389       \n",
    "[[step    38000]]     [[train]]     loss: 0.36579357       [[val]]     loss: 0.42807848       [[val-seq]]     loss: 0.36372007       \n",
    "[[step    39000]]     [[train]]     loss: 0.36539431       [[val]]     loss: 0.42909807       [[val-seq]]     loss: 0.36330017       \n",
    "[[step    40000]]     [[train]]     loss: 0.36616532       [[val]]     loss: 0.42908202       [[val-seq]]     loss: 0.36409379       \n",
    "[[step    41000]]     [[train]]     loss: 0.36390483       [[val]]     loss: 0.43058777       [[val-seq]]     loss: 0.36182936       \n",
    "[[step    42000]]     [[train]]     loss: 0.36610457       [[val]]     loss: 0.43106332       [[val-seq]]     loss: 0.36402831       \n",
    "best validation loss of 0.4204022465360795 at training step 6000\n",
    "early stopping - ending training.\n",
    "num_training_steps reached - ending training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_validation_loss, best_validation_tstep\n",
    "(0.4209459807394815, 7000)\n",
    "\n",
    "'C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\checkpoints\\\\model42097000.ckpt-7000'\n",
    "\n",
    "path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\'\n",
    "#path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "N = 100\n",
    "debug = False\n",
    "batch_size = 128\n",
    "num_training_steps = 300000\n",
    "learning_rate = 0.001\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 100000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 35000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = False\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "#dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "#filter_widths=filter_widths+filter_widths\n",
    "skip_channels=64\n",
    "residual_channels=96\n",
    "\n",
    "prediction_dir = path_out + \"predictions\"\n",
    "checkpoint_dir = path_out + \"checkpoints\"\n",
    "\n",
    "if not os.path.isdir(prediction_dir):\n",
    "    os.makedirs(prediction_dir)\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "trainable parameters:\n",
    "[('aisle_embeddings:0', [135, 50]), ('dept_embeddings:0', [22, 10]), ('user_embeddings:0', [207000, 300]), ('lstm1/rnn/lstm_cell/kernel:0', [1004, 1200]), ('lstm1/rnn/lstm_cell/bias:0', [1200]), ('dense1/weights:0', [1004, 50]), ('dense1/biases:0', [50]), ('dense2/weights:0', [50, 1]), ('dense2/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "63363271\n",
    "built graph\n",
    "\n",
    "\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.72577935       [[val]]     loss: 0.65642903       [[val-seq]]     loss: 0.64681882       \n",
    "[[step     1000]]     [[train]]     loss: 0.38667929       [[val]]     loss: 0.43641902       [[val-seq]]     loss: 0.38249544       \n",
    "[[step     2000]]     [[train]]     loss: 0.37966205       [[val]]     loss: 0.42781433       [[val-seq]]     loss: 0.37516639       \n",
    "[[step     3000]]     [[train]]     loss: 0.37733739       [[val]]     loss: 0.4234371        [[val-seq]]     loss: 0.37319629       \n",
    "[[step     4000]]     [[train]]     loss: 0.3772767        [[val]]     loss: 0.42816841       [[val-seq]]     loss: 0.37338137       \n",
    "[[step     5000]]     [[train]]     loss: 0.37526072       [[val]]     loss: 0.42579922       [[val-seq]]     loss: 0.37156339       \n",
    "[[step     6000]]     [[train]]     loss: 0.37493415       [[val]]     loss: 0.42703961       [[val-seq]]     loss: 0.37122898       \n",
    "[[step     7000]]     [[train]]     loss: 0.37254057       [[val]]     loss: 0.42094598       [[val-seq]]     loss: 0.36901362       \n",
    "[[step     8000]]     [[train]]     loss: 0.37242638       [[val]]     loss: 0.42468829       [[val-seq]]     loss: 0.36893474       \n",
    "[[step     9000]]     [[train]]     loss: 0.37198956       [[val]]     loss: 0.42421385       [[val-seq]]     loss: 0.3684996        \n",
    "[[step    10000]]     [[train]]     loss: 0.37027625       [[val]]     loss: 0.42510002       [[val-seq]]     loss: 0.36685655       \n",
    "[[step    11000]]     [[train]]     loss: 0.37190998       [[val]]     loss: 0.42655317       [[val-seq]]     loss: 0.36852493       \n",
    "[[step    12000]]     [[train]]     loss: 0.37150671       [[val]]     loss: 0.42382367       [[val-seq]]     loss: 0.36818472       \n",
    "[[step    13000]]     [[train]]     loss: 0.37044208       [[val]]     loss: 0.42254823       [[val-seq]]     loss: 0.36713486       \n",
    "[[step    14000]]     [[train]]     loss: 0.36938445       [[val]]     loss: 0.42609243       [[val-seq]]     loss: 0.36610656       \n",
    "[[step    15000]]     [[train]]     loss: 0.36897956       [[val]]     loss: 0.42274822       [[val-seq]]     loss: 0.36578495       \n",
    "[[step    16000]]     [[train]]     loss: 0.36960152       [[val]]     loss: 0.42546837       [[val-seq]]     loss: 0.36636248       \n",
    "[[step    17000]]     [[train]]     loss: 0.37039724       [[val]]     loss: 0.42589744       [[val-seq]]     loss: 0.36719882       \n",
    "[[step    18000]]     [[train]]     loss: 0.36984019       [[val]]     loss: 0.42690554       [[val-seq]]     loss: 0.36665125       \n",
    "[[step    19000]]     [[train]]     loss: 0.36908674       [[val]]     loss: 0.42684426       [[val-seq]]     loss: 0.36590971       \n",
    "[[step    20000]]     [[train]]     loss: 0.36816688       [[val]]     loss: 0.42691754       [[val-seq]]     loss: 0.36501191       \n",
    "[[step    21000]]     [[train]]     loss: 0.36886372       [[val]]     loss: 0.42378894       [[val-seq]]     loss: 0.36572918       \n",
    "[[step    22000]]     [[train]]     loss: 0.36979933       [[val]]     loss: 0.42465362       [[val-seq]]     loss: 0.36664639       \n",
    "[[step    23000]]     [[train]]     loss: 0.3685043        [[val]]     loss: 0.42778233       [[val-seq]]     loss: 0.36539656       \n",
    "[[step    24000]]     [[train]]     loss: 0.3676134        [[val]]     loss: 0.42729685       [[val-seq]]     loss: 0.36449743       \n",
    "[[step    25000]]     [[train]]     loss: 0.36701723       [[val]]     loss: 0.42717593       [[val-seq]]     loss: 0.36392438       \n",
    "[[step    26000]]     [[train]]     loss: 0.36569478       [[val]]     loss: 0.42564435       [[val-seq]]     loss: 0.36264328       \n",
    "[[step    27000]]     [[train]]     loss: 0.36755869       [[val]]     loss: 0.43127325       [[val-seq]]     loss: 0.36444162       \n",
    "[[step    28000]]     [[train]]     loss: 0.36827562       [[val]]     loss: 0.43043082       [[val-seq]]     loss: 0.36519178       \n",
    "[[step    29000]]     [[train]]     loss: 0.36570856       [[val]]     loss: 0.42953626       [[val-seq]]     loss: 0.36263079       \n",
    "[[step    30000]]     [[train]]     loss: 0.36546884       [[val]]     loss: 0.43025903       [[val-seq]]     loss: 0.36240199       \n",
    "[[step    31000]]     [[train]]     loss: 0.36596183       [[val]]     loss: 0.43252584       [[val-seq]]     loss: 0.3629174        \n",
    "[[step    32000]]     [[train]]     loss: 0.3668334        [[val]]     loss: 0.4294476        [[val-seq]]     loss: 0.36375651       \n",
    "[[step    33000]]     [[train]]     loss: 0.36659546       [[val]]     loss: 0.42983575       [[val-seq]]     loss: 0.36353752       \n",
    "[[step    34000]]     [[train]]     loss: 0.36507575       [[val]]     loss: 0.43280031       [[val-seq]]     loss: 0.36204965       \n",
    "[[step    35000]]     [[train]]     loss: 0.36463612       [[val]]     loss: 0.43231235       [[val-seq]]     loss: 0.36161446       \n",
    "[[step    36000]]     [[train]]     loss: 0.36568933       [[val]]     loss: 0.42955957       [[val-seq]]     loss: 0.36262748       \n",
    "[[step    37000]]     [[train]]     loss: 0.36467638       [[val]]     loss: 0.43380134       [[val-seq]]     loss: 0.36163716       \n",
    "[[step    38000]]     [[train]]     loss: 0.36381822       [[val]]     loss: 0.43073477       [[val-seq]]     loss: 0.36078022       \n",
    "[[step    39000]]     [[train]]     loss: 0.36461969       [[val]]     loss: 0.43428645       [[val-seq]]     loss: 0.36154758       \n",
    "[[step    40000]]     [[train]]     loss: 0.36492028       [[val]]     loss: 0.43434485       [[val-seq]]     loss: 0.36185018       \n",
    "[[step    41000]]     [[train]]     loss: 0.3641035        [[val]]     loss: 0.42990791       [[val-seq]]     loss: 0.36107208       \n",
    "[[step    42000]]     [[train]]     loss: 0.36516719       [[val]]     loss: 0.43478644       [[val-seq]]     loss: 0.36213557       \n",
    "[[step    43000]]     [[train]]     loss: 0.36548409       [[val]]     loss: 0.4323879        [[val-seq]]     loss: 0.36246711       \n",
    "[[step    44000]]     [[train]]     loss: 0.3638115        [[val]]     loss: 0.43335473       [[val-seq]]     loss: 0.3607743        \n",
    "[[step    45000]]     [[train]]     loss: 0.36366313       [[val]]     loss: 0.43759156       [[val-seq]]     loss: 0.36058636       \n",
    "[[step    46000]]     [[train]]     loss: 0.36283449       [[val]]     loss: 0.43843624       [[val-seq]]     loss: 0.35955424       \n",
    "[[step    47000]]     [[train]]     loss: 0.35964085       [[val]]     loss: 0.44596492       [[val-seq]]     loss: 0.35584711       \n",
    "[[step    48000]]     [[train]]     loss: 0.35961326       [[val]]     loss: 0.45195176       [[val-seq]]     loss: 0.35534527       \n",
    "[[step    49000]]     [[train]]     loss: 0.35746261       [[val]]     loss: 0.45494797       [[val-seq]]     loss: 0.35279568       \n",
    "[[step    50000]]     [[train]]     loss: 0.35759593       [[val]]     loss: 0.46060267       [[val-seq]]     loss: 0.35289372       \n",
    "[[step    51000]]     [[train]]     loss: 0.35635811       [[val]]     loss: 0.45780343       [[val-seq]]     loss: 0.35152714       \n",
    "[[step    52000]]     [[train]]     loss: 0.35471936       [[val]]     loss: 0.46360691       [[val-seq]]     loss: 0.34961522       \n",
    "[[step    53000]]     [[train]]     loss: 0.35465831       [[val]]     loss: 0.45896441       [[val-seq]]     loss: 0.3496902        \n",
    "[[step    54000]]     [[train]]     loss: 0.35495829       [[val]]     loss: 0.46346198       [[val-seq]]     loss: 0.34972423       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\np\\\\'\n",
    "#path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "N = 100\n",
    "debug = False\n",
    "batch_size = 128\n",
    "num_training_steps = 300000\n",
    "learning_rate = 0.001\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 100000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 35000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 1024\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "#dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "#filter_widths=filter_widths+filter_widths\n",
    "skip_channels=96\n",
    "residual_channels=124\n",
    "\n",
    "trainable parameters:\n",
    "[('aisle_embeddings:0', [135, 50]), ('dept_embeddings:0', [22, 10]), ('user_embeddings:0', [207000, 300]), ('lstm1/rnn/lstm_cell/kernel:0', [2728, 4096]), ('lstm1/rnn/lstm_cell/bias:0', [4096]), ('wavenet/x-proj/weights:0', [1704, 124]), ('wavenet/x-proj/biases:0', [124]), ('wavenet/cnn-0/weights:0', [2, 124, 248]), ('wavenet/cnn-0/biases:0', [248]), ('wavenet/cnn-0-proj/weights:0', [124, 220]), ('wavenet/cnn-0-proj/biases:0', [220]), ('wavenet/cnn-1/weights:0', [2, 124, 248]), ('wavenet/cnn-1/biases:0', [248]), ('wavenet/cnn-1-proj/weights:0', [124, 220]), ('wavenet/cnn-1-proj/biases:0', [220]), ('wavenet/cnn-2/weights:0', [2, 124, 248]), ('wavenet/cnn-2/biases:0', [248]), ('wavenet/cnn-2-proj/weights:0', [124, 220]), ('wavenet/cnn-2-proj/biases:0', [220]), ('wavenet/cnn-3/weights:0', [2, 124, 248]), ('wavenet/cnn-3/biases:0', [248]), ('wavenet/cnn-3-proj/weights:0', [124, 220]), ('wavenet/cnn-3-proj/biases:0', [220]), ('wavenet/cnn-4/weights:0', [2, 124, 248]), ('wavenet/cnn-4/biases:0', [248]), ('wavenet/cnn-4-proj/weights:0', [124, 220]), ('wavenet/cnn-4-proj/biases:0', [220]), ('wavenet/cnn-5/weights:0', [2, 124, 248]), ('wavenet/cnn-5/biases:0', [248]), ('wavenet/cnn-5-proj/weights:0', [124, 220]), ('wavenet/cnn-5-proj/biases:0', [220]), ('dense1/weights:0', [3304, 50]), ('dense1/biases:0', [50]), ('dense2/weights:0', [50, 1]), ('dense2/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "74197187\n",
    "built graph\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.70939094       [[val]]     loss: 0.41096521       \n",
    "[[step     1000]]     [[train]]     loss: 0.38642146       [[val]]     loss: 0.43758895       \n",
    "[[step     2000]]     [[train]]     loss: 0.37751848       [[val]]     loss: 0.42608113       \n",
    "[[step     3000]]     [[train]]     loss: 0.37805695       [[val]]     loss: 0.42838143       \n",
    "[[step     4000]]     [[train]]     loss: 0.3765701        [[val]]     loss: 0.424398         \n",
    "[[step     5000]]     [[train]]     loss: 0.37562265       [[val]]     loss: 0.42579731       \n",
    "[[step     6000]]     [[train]]     loss: 0.37614318       [[val]]     loss: 0.42438959       \n",
    "[[step     7000]]     [[train]]     loss: 0.37442617       [[val]]     loss: 0.42294047       \n",
    "[[step     8000]]     [[train]]     loss: 0.37305277       [[val]]     loss: 0.42525389       \n",
    "[[step     9000]]     [[train]]     loss: 0.37332716       [[val]]     loss: 0.42628442       \n",
    "[[step    10000]]     [[train]]     loss: 0.37205556       [[val]]     loss: 0.42547762       \n",
    "[[step    11000]]     [[train]]     loss: 0.37247341       [[val]]     loss: 0.42411674       \n",
    "[[step    12000]]     [[train]]     loss: 0.371267         [[val]]     loss: 0.42414662       \n",
    "[[step    13000]]     [[train]]     loss: 0.37237499       [[val]]     loss: 0.42269402       \n",
    "[[step    14000]]     [[train]]     loss: 0.37124519       [[val]]     loss: 0.42742674       \n",
    "[[step    15000]]     [[train]]     loss: 0.37024065       [[val]]     loss: 0.42706169       \n",
    "[[step    16000]]     [[train]]     loss: 0.36951248       [[val]]     loss: 0.42313572       \n",
    "[[step    17000]]     [[train]]     loss: 0.36920308       [[val]]     loss: 0.42702655       \n",
    "[[step    18000]]     [[train]]     loss: 0.36890243       [[val]]     loss: 0.42479633       \n",
    "[[step    19000]]     [[train]]     loss: 0.36838289       [[val]]     loss: 0.42760042       \n",
    "[[step    20000]]     [[train]]     loss: 0.36812239       [[val]]     loss: 0.42703578       \n",
    "[[step    21000]]     [[train]]     loss: 0.36878137       [[val]]     loss: 0.42523869       \n",
    "[[step    22000]]     [[train]]     loss: 0.3681644        [[val]]     loss: 0.42360044       \n",
    "[[step    23000]]     [[train]]     loss: 0.36752876       [[val]]     loss: 0.42814147       \n",
    "[[step    24000]]     [[train]]     loss: 0.36667018       [[val]]     loss: 0.42329501       \n",
    "[[step    25000]]     [[train]]     loss: 0.36729296       [[val]]     loss: 0.4259387        \n",
    "[[step    26000]]     [[train]]     loss: 0.36842091       [[val]]     loss: 0.42790254       \n",
    "[[step    27000]]     [[train]]     loss: 0.36803813       [[val]]     loss: 0.427691         \n",
    "[[step    28000]]     [[train]]     loss: 0.36773647       [[val]]     loss: 0.43003596       \n",
    "[[step    29000]]     [[train]]     loss: 0.36623077       [[val]]     loss: 0.42828908       \n",
    "[[step    30000]]     [[train]]     loss: 0.36628201       [[val]]     loss: 0.42338908       \n",
    "[[step    31000]]     [[train]]     loss: 0.36904426       [[val]]     loss: 0.427335         \n",
    "[[step    32000]]     [[train]]     loss: 0.36725425       [[val]]     loss: 0.42920808       \n",
    "[[step    33000]]     [[train]]     loss: 0.367116         [[val]]     loss: 0.42978597       \n",
    "[[step    34000]]     [[train]]     loss: 0.36501117       [[val]]     loss: 0.42692595       \n",
    "[[step    35000]]     [[train]]     loss: 0.36478453       [[val]]     loss: 0.42930666       \n",
    "[[step    36000]]     [[train]]     loss: 0.36557198       [[val]]     loss: 0.42938079       \n",
    "[[step    37000]]     [[train]]     loss: 0.36597545       [[val]]     loss: 0.43095312       \n",
    "[[step    38000]]     [[train]]     loss: 0.36541395       [[val]]     loss: 0.43255321       \n",
    "[[step    39000]]     [[train]]     loss: 0.36410409       [[val]]     loss: 0.43244085       \n",
    "[[step    40000]]     [[train]]     loss: 0.36610092       [[val]]     loss: 0.43255489       \n",
    "[[step    41000]]     [[train]]     loss: 0.36416895       [[val]]     loss: 0.43403675       \n",
    "[[step    42000]]     [[train]]     loss: 0.365641         [[val]]     loss: 0.4310425        \n",
    "[[step    43000]]     [[train]]     loss: 0.3640731        [[val]]     loss: 0.431747         \n",
    "[[step    44000]]     [[train]]     loss: 0.36397421       [[val]]     loss: 0.43372008       \n",
    "[[step    45000]]     [[train]]     loss: 0.36501272       [[val]]     loss: 0.4310498        \n",
    "[[step    46000]]     [[train]]     loss: 0.36092192       [[val]]     loss: 0.43643077       \n",
    "[[step    47000]]     [[train]]     loss: 0.36079551       [[val]]     loss: 0.44190613       \n",
    "[[step    48000]]     [[train]]     loss: 0.35971587       [[val]]     loss: 0.4499071        \n",
    "[[step    49000]]     [[train]]     loss: 0.35865844       [[val]]     loss: 0.45116917       \n",
    "[[step    50000]]     [[train]]     loss: 0.35887549       [[val]]     loss: 0.45157063       \n",
    "[[step    51000]]     [[train]]     loss: 0.35677896       [[val]]     loss: 0.45403695       \n",
    "[[step    52000]]     [[train]]     loss: 0.35802961       [[val]]     loss: 0.45085482       \n",
    "[[step    53000]]     [[train]]     loss: 0.35708232       [[val]]     loss: 0.45648349       \n",
    "[[step    54000]]     [[train]]     loss: 0.35589415       [[val]]     loss: 0.45895067       \n",
    "[[step    55000]]     [[train]]     loss: 0.35592514       [[val]]     loss: 0.45281368       \n",
    "[[step    56000]]     [[train]]     loss: 0.3568274        [[val]]     loss: 0.45942318       \n",
    "[[step    57000]]     [[train]]     loss: 0.35511323       [[val]]     loss: 0.4577161        \n",
    "[[step    58000]]     [[train]]     loss: 0.35569286       [[val]]     loss: 0.46056          \n",
    "[[step    59000]]     [[train]]     loss: 0.35638797       [[val]]     loss: 0.4578978        \n",
    "[[step    60000]]     [[train]]     loss: 0.35509676       [[val]]     loss: 0.46222879       \n",
    "[[step    61000]]     [[train]]     loss: 0.35391866       [[val]]     loss: 0.45974356       \n",
    "[[step    62000]]     [[train]]     loss: 0.35398123       [[val]]     loss: 0.46046257       \n",
    "[[step    63000]]     [[train]]     loss: 0.35339813       [[val]]     loss: 0.46283644       \n",
    "[[step    64000]]     [[train]]     loss: 0.35366576       [[val]]     loss: 0.46394283       \n",
    "[[step    65000]]     [[train]]     loss: 0.3531314        [[val]]     loss: 0.46213683       \n",
    "[[step    66000]]     [[train]]     loss: 0.35327068       [[val]]     loss: 0.46291079       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\'\n",
    "path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "debug = False\n",
    "batch_size = 512\n",
    "num_training_steps = 200000\n",
    "learning_rate = 0.001\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 100000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 100000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "#dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "#filter_widths=filter_widths+filter_widths\n",
    "skip_channels=124\n",
    "residual_channels=256\n",
    "\n",
    "prediction_dir = path_out + \"predictions\"\n",
    "checkpoint_dir = path_out + \"checkpoints\"\n",
    "\n",
    "if not os.path.isdir(prediction_dir):\n",
    "    os.makedirs(prediction_dir)\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "trainable parameters:\n",
    "[(u'aisle_embeddings:0', [135, 10]), (u'dept_embeddings:0', [22, 5]), (u'user_embeddings:0', [207000, 10]), (u'wavenet/x-proj/weights:0', [569, 256]), (u'wavenet/x-proj/biases:0', [256]), (u'wavenet/cnn-0/weights:0', [2, 256, 512]), (u'wavenet/cnn-0/biases:0', [512]), (u'wavenet/cnn-0-proj/weights:0', [256, 380]), (u'wavenet/cnn-0-proj/biases:0', [380]), (u'wavenet/cnn-1/weights:0', [2, 256, 512]), (u'wavenet/cnn-1/biases:0', [512]), (u'wavenet/cnn-1-proj/weights:0', [256, 380]), (u'wavenet/cnn-1-proj/biases:0', [380]), (u'wavenet/cnn-2/weights:0', [2, 256, 512]), (u'wavenet/cnn-2/biases:0', [512]), (u'wavenet/cnn-2-proj/weights:0', [256, 380]), (u'wavenet/cnn-2-proj/biases:0', [380]), (u'wavenet/cnn-3/weights:0', [2, 256, 512]), (u'wavenet/cnn-3/biases:0', [512]), (u'wavenet/cnn-3-proj/weights:0', [256, 380]), (u'wavenet/cnn-3-proj/biases:0', [380]), (u'wavenet/cnn-4/weights:0', [2, 256, 512]), (u'wavenet/cnn-4/biases:0', [512]), (u'wavenet/cnn-4-proj/weights:0', [256, 380]), (u'wavenet/cnn-4-proj/biases:0', [380]), (u'wavenet/cnn-5/weights:0', [2, 256, 512]), (u'wavenet/cnn-5/biases:0', [512]), (u'wavenet/cnn-5-proj/weights:0', [256, 380]), (u'wavenet/cnn-5-proj/biases:0', [380]), (u'dense1/weights:0', [1313, 100]), (u'dense1/biases:0', [100]), (u'dense4/weights:0', [100, 1]), (u'dense4/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "4510777\n",
    "built graph\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.65639955       [[val]]     loss: 0.6460228        \n",
    "[[step     1000]]     [[train]]     loss: 0.39410796       [[val]]     loss: 0.39500415       \n",
    "[[step     2000]]     [[train]]     loss: 0.39043517       [[val]]     loss: 0.38833185       \n",
    "[[step     3000]]     [[train]]     loss: 0.38773643       [[val]]     loss: 0.38830759       \n",
    "[[step     4000]]     [[train]]     loss: 0.38594667       [[val]]     loss: 0.38734226       \n",
    "[[step     5000]]     [[train]]     loss: 0.38534158       [[val]]     loss: 0.38749352       \n",
    "[[step     6000]]     [[train]]     loss: 0.38586397       [[val]]     loss: 0.3863855        \n",
    "[[step     7000]]     [[train]]     loss: 0.38522986       [[val]]     loss: 0.38695843       \n",
    "[[step     8000]]     [[train]]     loss: 0.38583578       [[val]]     loss: 0.38644603       \n",
    "[[step     9000]]     [[train]]     loss: 0.38407461       [[val]]     loss: 0.3873567        \n",
    "[[step    10000]]     [[train]]     loss: 0.38389422       [[val]]     loss: 0.38706803       \n",
    "[[step    11000]]     [[train]]     loss: 0.38081008       [[val]]     loss: 0.38797855       \n",
    "[[step    12000]]     [[train]]     loss: 0.37900639       [[val]]     loss: 0.38802069       \n",
    "[[step    13000]]     [[train]]     loss: 0.37913274       [[val]]     loss: 0.38738997       \n",
    "[[step    14000]]     [[train]]     loss: 0.37896445       [[val]]     loss: 0.38823297       \n",
    "[[step    15000]]     [[train]]     loss: 0.37926663       [[val]]     loss: 0.38674552       \n",
    "[[step    16000]]     [[train]]     loss: 0.37970057       [[val]]     loss: 0.38887942       \n",
    "[[step    17000]]     [[train]]     loss: 0.37902393       [[val]]     loss: 0.388228         \n",
    "[[step    18000]]     [[train]]     loss: 0.38008267       [[val]]     loss: 0.38851277       \n",
    "[[step    19000]]     [[train]]     loss: 0.3773005        [[val]]     loss: 0.38779979       \n",
    "[[step    20000]]     [[train]]     loss: 0.37470627       [[val]]     loss: 0.38891269       \n",
    "[[step    21000]]     [[train]]     loss: 0.3753976        [[val]]     loss: 0.38834771       \n",
    "[[step    22000]]     [[train]]     loss: 0.3749405        [[val]]     loss: 0.3893597        \n",
    "[[step    23000]]     [[train]]     loss: 0.37535476       [[val]]     loss: 0.38784994       \n",
    "[[step    24000]]     [[train]]     loss: 0.37633207       [[val]]     loss: 0.39032121       \n",
    "[[step    25000]]     [[train]]     loss: 0.37576604       [[val]]     loss: 0.38850753       \n",
    "[[step    26000]]     [[train]]     loss: 0.37804546       [[val]]     loss: 0.39029557       \n",
    "[[step    27000]]     [[train]]     loss: 0.37477509       [[val]]     loss: 0.38876165       \n",
    "[[step    28000]]     [[train]]     loss: 0.37353205       [[val]]     loss: 0.38979194       \n",
    "[[step    29000]]     [[train]]     loss: 0.37330536       [[val]]     loss: 0.3893781        \n",
    "[[step    30000]]     [[train]]     loss: 0.37306514       [[val]]     loss: 0.39048891       \n",
    "[[step    31000]]     [[train]]     loss: 0.3734858        [[val]]     loss: 0.38949871       \n",
    "[[step    32000]]     [[train]]     loss: 0.37519194       [[val]]     loss: 0.38989895       \n",
    "[[step    33000]]     [[train]]     loss: 0.37359269       [[val]]     loss: 0.38998747       \n",
    "[[step    34000]]     [[train]]     loss: 0.37674954       [[val]]     loss: 0.39042301       \n",
    "[[step    35000]]     [[train]]     loss: 0.37249341       [[val]]     loss: 0.38951659       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\'\n",
    "path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "debug = False\n",
    "batch_size = 512\n",
    "num_training_steps = 200000\n",
    "learning_rate = 0.0002\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 100000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 100000\n",
    "log_interval = 1000\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "filter_widths=filter_widths+filter_widths\n",
    "skip_channels=96\n",
    "residual_channels=124\n",
    "\n",
    "prediction_dir = path_out + \"predictions\"\n",
    "checkpoint_dir = path_out + \"checkpoints\"\n",
    "\n",
    "if not os.path.isdir(prediction_dir):\n",
    "    os.makedirs(prediction_dir)\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "trainable parameters:\n",
    "[(u'aisle_embeddings:0', [135, 50]), (u'dept_embeddings:0', [22, 10]), (u'wavenet/x-proj/weights:0', [1404, 124]), (u'wavenet/x-proj/biases:0', [124]), (u'wavenet/cnn-0/weights:0', [2, 124, 248]), (u'wavenet/cnn-0/biases:0', [248]), (u'wavenet/cnn-0-proj/weights:0', [124, 220]), (u'wavenet/cnn-0-proj/biases:0', [220]), (u'wavenet/cnn-1/weights:0', [2, 124, 248]), (u'wavenet/cnn-1/biases:0', [248]), (u'wavenet/cnn-1-proj/weights:0', [124, 220]), (u'wavenet/cnn-1-proj/biases:0', [220]), (u'wavenet/cnn-2/weights:0', [2, 124, 248]), (u'wavenet/cnn-2/biases:0', [248]), (u'wavenet/cnn-2-proj/weights:0', [124, 220]), (u'wavenet/cnn-2-proj/biases:0', [220]), (u'wavenet/cnn-3/weights:0', [2, 124, 248]), (u'wavenet/cnn-3/biases:0', [248]), (u'wavenet/cnn-3-proj/weights:0', [124, 220]), (u'wavenet/cnn-3-proj/biases:0', [220]), (u'wavenet/cnn-4/weights:0', [2, 124, 248]), (u'wavenet/cnn-4/biases:0', [248]), (u'wavenet/cnn-4-proj/weights:0', [124, 220]), (u'wavenet/cnn-4-proj/biases:0', [220]), (u'wavenet/cnn-5/weights:0', [2, 124, 248]), (u'wavenet/cnn-5/biases:0', [248]), (u'wavenet/cnn-5-proj/weights:0', [124, 220]), (u'wavenet/cnn-5-proj/biases:0', [220]), (u'wavenet/cnn-6/weights:0', [2, 124, 248]), (u'wavenet/cnn-6/biases:0', [248]), (u'wavenet/cnn-6-proj/weights:0', [124, 220]), (u'wavenet/cnn-6-proj/biases:0', [220]), (u'wavenet/cnn-7/weights:0', [2, 124, 248]), (u'wavenet/cnn-7/biases:0', [248]), (u'wavenet/cnn-7-proj/weights:0', [124, 220]), (u'wavenet/cnn-7-proj/biases:0', [220]), (u'wavenet/cnn-8/weights:0', [2, 124, 248]), (u'wavenet/cnn-8/biases:0', [248]), (u'wavenet/cnn-8-proj/weights:0', [124, 220]), (u'wavenet/cnn-8-proj/biases:0', [220]), (u'wavenet/cnn-9/weights:0', [2, 124, 248]), (u'wavenet/cnn-9/biases:0', [248]), (u'wavenet/cnn-9-proj/weights:0', [124, 220]), (u'wavenet/cnn-9-proj/biases:0', [220]), (u'wavenet/cnn-10/weights:0', [2, 124, 248]), (u'wavenet/cnn-10/biases:0', [248]), (u'wavenet/cnn-10-proj/weights:0', [124, 220]), (u'wavenet/cnn-10-proj/biases:0', [220]), (u'wavenet/cnn-11/weights:0', [2, 124, 248]), (u'wavenet/cnn-11/biases:0', [248]), (u'wavenet/cnn-11-proj/weights:0', [124, 220]), (u'wavenet/cnn-11-proj/biases:0', [220]), (u'dense1/weights:0', [1152, 1]), (u'dense1/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "1253367\n",
    "built graph\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.57300597       [[val]]     loss: 0.54200459       \n",
    "[[step     1000]]     [[train]]     loss: 0.4037229        [[val]]     loss: 0.40151307       \n",
    "[[step     2000]]     [[train]]     loss: 0.39069892       [[val]]     loss: 0.39203746       \n",
    "[[step     3000]]     [[train]]     loss: 0.38875566       [[val]]     loss: 0.38946532       \n",
    "[[step     4000]]     [[train]]     loss: 0.38789024       [[val]]     loss: 0.38929309       \n",
    "[[step     5000]]     [[train]]     loss: 0.38554126       [[val]]     loss: 0.38749247       \n",
    "[[step     6000]]     [[train]]     loss: 0.38616442       [[val]]     loss: 0.38787958       \n",
    "[[step     7000]]     [[train]]     loss: 0.38744169       [[val]]     loss: 0.38627563       \n",
    "[[step     8000]]     [[train]]     loss: 0.38623234       [[val]]     loss: 0.38779106       \n",
    "[[step     9000]]     [[train]]     loss: 0.38921565       [[val]]     loss: 0.38652136       \n",
    "[[step    10000]]     [[train]]     loss: 0.38508331       [[val]]     loss: 0.38733072       \n",
    "[[step    11000]]     [[train]]     loss: 0.38469722       [[val]]     loss: 0.38582431       \n",
    "[[step    12000]]     [[train]]     loss: 0.38549126       [[val]]     loss: 0.38620556       \n",
    "[[step    13000]]     [[train]]     loss: 0.38293964       [[val]]     loss: 0.38595986       \n",
    "[[step    14000]]     [[train]]     loss: 0.38466007       [[val]]     loss: 0.38606849       \n",
    "[[step    15000]]     [[train]]     loss: 0.38619278       [[val]]     loss: 0.38542646       \n",
    "[[step    16000]]     [[train]]     loss: 0.38458682       [[val]]     loss: 0.38630333       \n",
    "[[step    17000]]     [[train]]     loss: 0.38788458       [[val]]     loss: 0.3855057        \n",
    "[[step    18000]]     [[train]]     loss: 0.38331083       [[val]]     loss: 0.3860205        \n",
    "[[step    19000]]     [[train]]     loss: 0.38346381       [[val]]     loss: 0.38480297       \n",
    "[[step    20000]]     [[train]]     loss: 0.38435932       [[val]]     loss: 0.38571543       \n",
    "[[step    21000]]     [[train]]     loss: 0.38265559       [[val]]     loss: 0.38568668       \n",
    "[[step    33000]]     [[train]]     loss: 0.38622362       [[val]]     loss: 0.38602395 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WaveNet ::: Training Loss :: 0.38212136 @ 11500 :: Validation Loss : 0.38483268 @ 8500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\'\n",
    "path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "debug = False\n",
    "batch_size = 512\n",
    "num_training_steps = 100000\n",
    "learning_rate = 0.001\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 100000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 100000\n",
    "log_interval = 500\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "#dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "#filter_widths=filter_widths+filter_widths\n",
    "skip_channels=96\n",
    "residual_channels=124\n",
    "\n",
    "trainable parameters:\n",
    "[(u'aisle_embeddings:0', [135, 50]), (u'dept_embeddings:0', [22, 10]), (u'product_embeddings:0', [50000, 50]), (u'wavenet/x-proj/weights:0', [1404, 124]), (u'wavenet/x-proj/biases:0', [124]), (u'wavenet/cnn-0/weights:0', [2, 124, 248]), (u'wavenet/cnn-0/biases:0', [248]), (u'wavenet/cnn-0-proj/weights:0', [124, 220]), (u'wavenet/cnn-0-proj/biases:0', [220]), (u'wavenet/cnn-1/weights:0', [2, 124, 248]), (u'wavenet/cnn-1/biases:0', [248]), (u'wavenet/cnn-1-proj/weights:0', [124, 220]), (u'wavenet/cnn-1-proj/biases:0', [220]), (u'wavenet/cnn-2/weights:0', [2, 124, 248]), (u'wavenet/cnn-2/biases:0', [248]), (u'wavenet/cnn-2-proj/weights:0', [124, 220]), (u'wavenet/cnn-2-proj/biases:0', [220]), (u'wavenet/cnn-3/weights:0', [2, 124, 248]), (u'wavenet/cnn-3/biases:0', [248]), (u'wavenet/cnn-3-proj/weights:0', [124, 220]), (u'wavenet/cnn-3-proj/biases:0', [220]), (u'wavenet/cnn-4/weights:0', [2, 124, 248]), (u'wavenet/cnn-4/biases:0', [248]), (u'wavenet/cnn-4-proj/weights:0', [124, 220]), (u'wavenet/cnn-4-proj/biases:0', [220]), (u'wavenet/cnn-5/weights:0', [2, 124, 248]), (u'wavenet/cnn-5/biases:0', [248]), (u'wavenet/cnn-5-proj/weights:0', [124, 220]), (u'wavenet/cnn-5-proj/biases:0', [220]), (u'dense1/weights:0', [576, 1]), (u'dense1/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "3217279\n",
    "built graph\n",
    "\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.74986464       [[val]]     loss: 0.76376551       \n",
    "[[step      500]]     [[train]]     loss: 0.39902695       [[val]]     loss: 0.39719228       \n",
    "[[step     1000]]     [[train]]     loss: 0.39419912       [[val]]     loss: 0.39407802       \n",
    "[[step     1500]]     [[train]]     loss: 0.38933741       [[val]]     loss: 0.39025618       \n",
    "[[step     2000]]     [[train]]     loss: 0.38984062       [[val]]     loss: 0.38856346       \n",
    "[[step     2500]]     [[train]]     loss: 0.39052064       [[val]]     loss: 0.38848885       \n",
    "[[step     3000]]     [[train]]     loss: 0.38830204       [[val]]     loss: 0.38782649       \n",
    "[[step     3500]]     [[train]]     loss: 0.38684298       [[val]]     loss: 0.38735862       \n",
    "[[step     4000]]     [[train]]     loss: 0.38547107       [[val]]     loss: 0.38668211       \n",
    "[[step     4500]]     [[train]]     loss: 0.38482107       [[val]]     loss: 0.38653999       \n",
    "[[step     5000]]     [[train]]     loss: 0.38467534       [[val]]     loss: 0.38734918       \n",
    "[[step     5500]]     [[train]]     loss: 0.38469537       [[val]]     loss: 0.38696021       \n",
    "[[step     6000]]     [[train]]     loss: 0.38638392       [[val]]     loss: 0.38610079       \n",
    "[[step     6500]]     [[train]]     loss: 0.38448875       [[val]]     loss: 0.38586745       \n",
    "[[step     7000]]     [[train]]     loss: 0.38478305       [[val]]     loss: 0.38651106       \n",
    "[[step     7500]]     [[train]]     loss: 0.38532901       [[val]]     loss: 0.38626928       \n",
    "[[step     8000]]     [[train]]     loss: 0.38410955       [[val]]     loss: 0.38537588       \n",
    "[[step     8500]]     [[train]]     loss: 0.38513654       [[val]]     loss: 0.38483268       \n",
    "[[step     9000]]     [[train]]     loss: 0.38417677       [[val]]     loss: 0.38610536       \n",
    "[[step     9500]]     [[train]]     loss: 0.38283743       [[val]]     loss: 0.38755284       \n",
    "[[step    10000]]     [[train]]     loss: 0.38427715       [[val]]     loss: 0.38727841       \n",
    "[[step    10500]]     [[train]]     loss: 0.38546348       [[val]]     loss: 0.38697921       \n",
    "[[step    11000]]     [[train]]     loss: 0.38355408       [[val]]     loss: 0.38705426       \n",
    "[[step    11500]]     [[train]]     loss: 0.38212136       [[val]]     loss: 0.3876643     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked LSTM@300 + Wide WaveNet with Fixed Top ::: Training Loss :: 0.38121098 @ 11000 :: Validation Loss : 0.38525658 @ 9500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#path_out = 'C:\\\\Users\\\\chapanda\\\\data\\\\'\n",
    "path_out = u'/cat/home/ubuntu/ikrt/mycode/np/'\n",
    "debug = False\n",
    "batch_size = 512\n",
    "num_training_steps = 100000\n",
    "learning_rate = 0.001\n",
    "opt = 'adam'\n",
    "grad_clip = 5\n",
    "regularization_constant = 0.0000\n",
    "warm_start_init_step = 0\n",
    "early_stopping_steps = 100000\n",
    "keep_prob_scalar = 1.0\n",
    "enable_parameter_averaging = False\n",
    "num_restarts = 0\n",
    "min_steps_to_checkpoint = 100000\n",
    "log_interval = 500\n",
    "loss_averaging_window = 1000\n",
    "lstm_size = 300\n",
    "val_multiplier = 1\n",
    "WaveNet = True\n",
    "dropout = 0\n",
    "\n",
    "dilations=[2**i for i in range(6)]\n",
    "#dilations=dilations+dilations\n",
    "filter_widths=[2]*6\n",
    "#filter_widths=filter_widths+filter_widths\n",
    "skip_channels=96\n",
    "residual_channels=124*5\n",
    "\n",
    "trainable parameters:\n",
    "[(u'aisle_embeddings:0', [135, 50]), (u'dept_embeddings:0', [22, 10]), (u'product_embeddings:0', [50000, 50]), (u'lstm1/rnn/lstm_cell/kernel:0', [1704, 1200]), (u'lstm1/rnn/lstm_cell/bias:0', [1200]), (u'wavenet/x-proj/weights:0', [1404, 620]), (u'wavenet/x-proj/biases:0', [620]), (u'wavenet/cnn-0/weights:0', [2, 620, 1240]), (u'wavenet/cnn-0/biases:0', [1240]), (u'wavenet/cnn-0-proj/weights:0', [620, 716]), (u'wavenet/cnn-0-proj/biases:0', [716]), (u'wavenet/cnn-1/weights:0', [2, 620, 1240]), (u'wavenet/cnn-1/biases:0', [1240]), (u'wavenet/cnn-1-proj/weights:0', [620, 716]), (u'wavenet/cnn-1-proj/biases:0', [716]), (u'wavenet/cnn-2/weights:0', [2, 620, 1240]), (u'wavenet/cnn-2/biases:0', [1240]), (u'wavenet/cnn-2-proj/weights:0', [620, 716]), (u'wavenet/cnn-2-proj/biases:0', [716]), (u'wavenet/cnn-3/weights:0', [2, 620, 1240]), (u'wavenet/cnn-3/biases:0', [1240]), (u'wavenet/cnn-3-proj/weights:0', [620, 716]), (u'wavenet/cnn-3-proj/biases:0', [716]), (u'wavenet/cnn-4/weights:0', [2, 620, 1240]), (u'wavenet/cnn-4/biases:0', [1240]), (u'wavenet/cnn-4-proj/weights:0', [620, 716]), (u'wavenet/cnn-4-proj/biases:0', [716]), (u'wavenet/cnn-5/weights:0', [2, 620, 1240]), (u'wavenet/cnn-5/biases:0', [1240]), (u'wavenet/cnn-5-proj/weights:0', [620, 716]), (u'wavenet/cnn-5-proj/biases:0', [716]), (u'dense1/weights:0', [2280, 300]), (u'dense1/biases:0', [300]), (u'dense2/weights:0', [300, 200]), (u'dense2/biases:0', [200]), (u'dense3/weights:0', [200, 100]), (u'dense3/biases:0', [100]), (u'dense4/weights:0', [100, 1]), (u'dense4/biases:0', [1])]\n",
    "trainable parameter count:\n",
    "18089627\n",
    "built graph\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.71439296       [[val]]     loss: 0.71579534       \n",
    "[[step      500]]     [[train]]     loss: 0.39536874       [[val]]     loss: 0.39705661       \n",
    "[[step     1000]]     [[train]]     loss: 0.39314378       [[val]]     loss: 0.39347515       \n",
    "[[step     1500]]     [[train]]     loss: 0.38910436       [[val]]     loss: 0.39012438       \n",
    "[[step     2000]]     [[train]]     loss: 0.38624256       [[val]]     loss: 0.38915163       \n",
    "[[step     2500]]     [[train]]     loss: 0.38640541       [[val]]     loss: 0.38788248       \n",
    "[[step     3000]]     [[train]]     loss: 0.38633301       [[val]]     loss: 0.38729562       \n",
    "[[step     3500]]     [[train]]     loss: 0.38628424       [[val]]     loss: 0.38771333       \n",
    "[[step     4000]]     [[train]]     loss: 0.3872626        [[val]]     loss: 0.3872555        \n",
    "[[step     4500]]     [[train]]     loss: 0.3866859        [[val]]     loss: 0.38601295       \n",
    "[[step     5000]]     [[train]]     loss: 0.38571788       [[val]]     loss: 0.38543805       \n",
    "[[step     5500]]     [[train]]     loss: 0.38781096       [[val]]     loss: 0.38662798       \n",
    "[[step     6000]]     [[train]]     loss: 0.38873766       [[val]]     loss: 0.38704623       \n",
    "[[step     6500]]     [[train]]     loss: 0.3863655        [[val]]     loss: 0.38623803       \n",
    "[[step     7000]]     [[train]]     loss: 0.38436648       [[val]]     loss: 0.38577226       \n",
    "[[step     7500]]     [[train]]     loss: 0.38427535       [[val]]     loss: 0.38547944       \n",
    "[[step     8000]]     [[train]]     loss: 0.3842475        [[val]]     loss: 0.38648178       \n",
    "[[step     8500]]     [[train]]     loss: 0.38299651       [[val]]     loss: 0.38612066       \n",
    "[[step     9500]]     [[train]]     loss: 0.38224757       [[val]]     loss: 0.38525658       \n",
    "[[step    10000]]     [[train]]     loss: 0.38008553       [[val]]     loss: 0.38662994       \n",
    "[[step    10500]]     [[train]]     loss: 0.38114074       [[val]]     loss: 0.38728664       \n",
    "[[step    11000]]     [[train]]     loss: 0.38121098       [[val]]     loss: 0.3862937   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "\n",
    "1. Batch Normalization\n",
    "2. 50, 300 User Embeddings\n",
    "3. Department Embeddings\n",
    "4. Product Level Model Stacked with Department Level Model\n",
    "5. wavenet\n",
    "6. dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "lr = 0.00001\n",
    "embedding_size = 10\n",
    "batch_norm = False\n",
    "dropout = 0\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.68945086       [[val]]     loss: 0.68476069       \n",
    "[[step      100]]     [[train]]     loss: 0.52741179       [[val]]     loss: 0.52966699       \n",
    "[[step      200]]     [[train]]     loss: 0.42600287       [[val]]     loss: 0.43532646       \n",
    "[[step      300]]     [[train]]     loss: 0.42100239       [[val]]     loss: 0.42119279       \n",
    "[[step      400]]     [[train]]     loss: 0.40935658       [[val]]     loss: 0.41433323       \n",
    "[[step      500]]     [[train]]     loss: 0.40634079       [[val]]     loss: 0.40343187       \n",
    "[[step      600]]     [[train]]     loss: 0.40356448       [[val]]     loss: 0.40347679       \n",
    "[[step      700]]     [[train]]     loss: 0.39721517       [[val]]     loss: 0.39683794       \n",
    "[[step      800]]     [[train]]     loss: 0.40219059       [[val]]     loss: 0.40324225       \n",
    "[[step      900]]     [[train]]     loss: 0.39875729       [[val]]     loss: 0.39927291       \n",
    "[[step     1000]]     [[train]]     loss: 0.39288183       [[val]]     loss: 0.41149075       \n",
    "[[step     1100]]     [[train]]     loss: 0.4064956        [[val]]     loss: 0.39715582       \n",
    "[[step     1200]]     [[train]]     loss: 0.39463545       [[val]]     loss: 0.39509431       \n",
    "[[step     1300]]     [[train]]     loss: 0.40247756       [[val]]     loss: 0.41126509       \n",
    "[[step     1400]]     [[train]]     loss: 0.39764947       [[val]]     loss: 0.39076713       \n",
    "[[step     1500]]     [[train]]     loss: 0.40276493       [[val]]     loss: 0.38258591       \n",
    "[[step     1600]]     [[train]]     loss: 0.38685263       [[val]]     loss: 0.39597086       \n",
    "[[step     1700]]     [[train]]     loss: 0.39643141       [[val]]     loss: 0.40260221       \n",
    "[[step     1800]]     [[train]]     loss: 0.39657799       [[val]]     loss: 0.39385055       \n",
    "[[step     1900]]     [[train]]     loss: 0.38752659       [[val]]     loss: 0.3947884        \n",
    "[[step     2000]]     [[train]]     loss: 0.39012568       [[val]]     loss: 0.40079436       \n",
    "[[step     2100]]     [[train]]     loss: 0.39138237       [[val]]     loss: 0.39907276    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "lr = 0.00001\n",
    "embedding_size = 10-50\n",
    "batch_norm = False\n",
    "dropout = 0\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.69394004       [[val]]     loss: 0.69585359       \n",
    "[[step      100]]     [[train]]     loss: 0.57589115       [[val]]     loss: 0.57229453       \n",
    "[[step      200]]     [[train]]     loss: 0.46916369       [[val]]     loss: 0.45808681       \n",
    "[[step      300]]     [[train]]     loss: 0.43033853       [[val]]     loss: 0.44753059       \n",
    "[[step      400]]     [[train]]     loss: 0.41173015       [[val]]     loss: 0.40878801       \n",
    "[[step      500]]     [[train]]     loss: 0.40163584       [[val]]     loss: 0.40868014       \n",
    "[[step      600]]     [[train]]     loss: 0.411918         [[val]]     loss: 0.40809412       \n",
    "[[step      700]]     [[train]]     loss: 0.4154951        [[val]]     loss: 0.39677772       \n",
    "[[step      800]]     [[train]]     loss: 0.40657057       [[val]]     loss: 0.3949514        \n",
    "[[step      900]]     [[train]]     loss: 0.40774495       [[val]]     loss: 0.40948893  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.001\n",
    "embedding_size = 10-50\n",
    "batch_norm = False\n",
    "dropout = 0\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.68255645       [[val]]     loss: 0.68390912       \n",
    "[[step      100]]     [[train]]     loss: 0.43326256       [[val]]     loss: 0.41730649       \n",
    "[[step      200]]     [[train]]     loss: 0.42293454       [[val]]     loss: 0.4055799        \n",
    "[[step      300]]     [[train]]     loss: 0.42179572       [[val]]     loss: 0.40204019       \n",
    "[[step      400]]     [[train]]     loss: 0.41070997       [[val]]     loss: 0.39842628       \n",
    "[[step      500]]     [[train]]     loss: 0.42036336       [[val]]     loss: 0.40866262       \n",
    "[[step      600]]     [[train]]     loss: 0.41237596       [[val]]     loss: 0.3960299        \n",
    "[[step      700]]     [[train]]     loss: 0.40602685       [[val]]     loss: 0.39392554       \n",
    "[[step      800]]     [[train]]     loss: 0.41559202       [[val]]     loss: 0.40025059       \n",
    "[[step      900]]     [[train]]     loss: 0.41720033       [[val]]     loss: 0.38500627       \n",
    "[[step     1000]]     [[train]]     loss: 0.40658416       [[val]]     loss: 0.40565673       \n",
    "[[step     1100]]     [[train]]     loss: 0.40783333       [[val]]     loss: 0.39481372       \n",
    "[[step     1200]]     [[train]]     loss: 0.40538956       [[val]]     loss: 0.39182611       \n",
    "[[step     1300]]     [[train]]     loss: 0.41342945       [[val]]     loss: 0.39858596       \n",
    "[[step     1400]]     [[train]]     loss: 0.41209739       [[val]]     loss: 0.39087869       \n",
    "[[step     1500]]     [[train]]     loss: 0.41145673       [[val]]     loss: 0.39485126       \n",
    "[[step     1600]]     [[train]]     loss: 0.42238395       [[val]]     loss: 0.39638283       \n",
    "[[step     1700]]     [[train]]     loss: 0.40937202       [[val]]     loss: 0.39356789       \n",
    "[[step     1800]]     [[train]]     loss: 0.41470608       [[val]]     loss: 0.38789571       \n",
    "[[step     1900]]     [[train]]     loss: 0.41887923       [[val]]     loss: 0.39698425       \n",
    "[[step     2000]]     [[train]]     loss: 0.40212174       [[val]]     loss: 0.38679843       \n",
    "[[step     2100]]     [[train]]     loss: 0.41022544       [[val]]     loss: 0.39420067       \n",
    "[[step     2200]]     [[train]]     loss: 0.41485526       [[val]]     loss: 0.39693822       \n",
    "[[step     2300]]     [[train]]     loss: 0.40914674       [[val]]     loss: 0.39013196       \n",
    "[[step     2400]]     [[train]]     loss: 0.40681492       [[val]]     loss: 0.39067219       \n",
    "[[step     2500]]     [[train]]     loss: 0.4076804        [[val]]     loss: 0.39330774       \n",
    "[[step     2600]]     [[train]]     loss: 0.42123778       [[val]]     loss: 0.38994598       \n",
    "[[step     2700]]     [[train]]     loss: 0.41556455       [[val]]     loss: 0.39031908       \n",
    "[[step     2800]]     [[train]]     loss: 0.39836752       [[val]]     loss: 0.40093884       \n",
    "[[step     2900]]     [[train]]     loss: 0.40989705       [[val]]     loss: 0.38727811       \n",
    "[[step     3000]]     [[train]]     loss: 0.41062794       [[val]]     loss: 0.390566         \n",
    "[[step     3100]]     [[train]]     loss: 0.40107831       [[val]]     loss: 0.3911186        \n",
    "[[step     3200]]     [[train]]     loss: 0.40512848       [[val]]     loss: 0.3826369        \n",
    "[[step     3300]]     [[train]]     loss: 0.40562024       [[val]]     loss: 0.4026496        \n",
    "[[step     3400]]     [[train]]     loss: 0.42069093       [[val]]     loss: 0.38858808       \n",
    "[[step     3500]]     [[train]]     loss: 0.40914303       [[val]]     loss: 0.38865459       \n",
    "[[step     3600]]     [[train]]     loss: 0.41292151       [[val]]     loss: 0.39358043       \n",
    "[[step     3700]]     [[train]]     loss: 0.40138045       [[val]]     loss: 0.38675016       \n",
    "[[step     3800]]     [[train]]     loss: 0.41835118       [[val]]     loss: 0.39161719       \n",
    "[[step     3900]]     [[train]]     loss: 0.40322772       [[val]]     loss: 0.39639044       \n",
    "[[step     4000]]     [[train]]     loss: 0.43074263       [[val]]     loss: 0.3883957        \n",
    "[[step     4100]]     [[train]]     loss: 0.4156726        [[val]]     loss: 0.38980969       \n",
    "[[step     4200]]     [[train]]     loss: 0.40842212       [[val]]     loss: 0.39319402       \n",
    "[[step     4300]]     [[train]]     loss: 0.41640786       [[val]]     loss: 0.38108944       \n",
    "[[step     4400]]     [[train]]     loss: 0.40814732       [[val]]     loss: 0.40034898       \n",
    "[[step     4500]]     [[train]]     loss: 0.41581489       [[val]]     loss: 0.38978179       \n",
    "[[step     4600]]     [[train]]     loss: 0.4131633        [[val]]     loss: 0.38818143       \n",
    "[[step     4700]]     [[train]]     loss: 0.39764337       [[val]]     loss: 0.39419644       \n",
    "[[step     4800]]     [[train]]     loss: 0.40196683       [[val]]     loss: 0.38820853       \n",
    "[[step     4900]]     [[train]]     loss: 0.41307212       [[val]]     loss: 0.39067163       \n",
    "[[step     5000]]     [[train]]     loss: 0.40948561       [[val]]     loss: 0.39197072       \n",
    "[[step     5100]]     [[train]]     loss: 0.41734304       [[val]]     loss: 0.39139198       \n",
    "[[step     5200]]     [[train]]     loss: 0.40337441       [[val]]     loss: 0.38433809       \n",
    "[[step     5300]]     [[train]]     loss: 0.41370722       [[val]]     loss: 0.39362062       \n",
    "[[step     5400]]     [[train]]     loss: 0.41196924       [[val]]     loss: 0.38499878       \n",
    "[[step     5500]]     [[train]]     loss: 0.40916227       [[val]]     loss: 0.39047131       \n",
    "[[step     8400]]     [[train]]     loss: 0.41929639       [[val]]     loss: 0.39204189 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "lr = 0.0001\n",
    "embedding_size = 10-50\n",
    "batch_norm = False\n",
    "dropout = 0\n",
    "final layers = 2 25 - 50\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.69800591       [[val]]     loss: 0.7077204        \n",
    "[[step      100]]     [[train]]     loss: 0.4642874        [[val]]     loss: 0.45294539       \n",
    "[[step      200]]     [[train]]     loss: 0.40891664       [[val]]     loss: 0.39687391       \n",
    "[[step      300]]     [[train]]     loss: 0.41119523       [[val]]     loss: 0.40390418       \n",
    "[[step      400]]     [[train]]     loss: 0.40153559       [[val]]     loss: 0.39589814       \n",
    "[[step      500]]     [[train]]     loss: 0.41193385       [[val]]     loss: 0.39404101       \n",
    "[[step      600]]     [[train]]     loss: 0.40311682       [[val]]     loss: 0.39565056       \n",
    "[[step      700]]     [[train]]     loss: 0.40937866       [[val]]     loss: 0.39157548       \n",
    "[[step      800]]     [[train]]     loss: 0.40068859       [[val]]     loss: 0.39049928       \n",
    "[[step      900]]     [[train]]     loss: 0.40969506       [[val]]     loss: 0.39396498       \n",
    "[[step     1000]]     [[train]]     loss: 0.39421114       [[val]]     loss: 0.3906278        \n",
    "[[step     1100]]     [[train]]     loss: 0.40498208       [[val]]     loss: 0.3875514        \n",
    "[[step     1200]]     [[train]]     loss: 0.40884323       [[val]]     loss: 0.39262184       \n",
    "[[step     1300]]     [[train]]     loss: 0.40473363       [[val]]     loss: 0.38881227       \n",
    "[[step     1400]]     [[train]]     loss: 0.39670036       [[val]]     loss: 0.3909938        \n",
    "[[step     1500]]     [[train]]     loss: 0.39192327       [[val]]     loss: 0.38764418       \n",
    "[[step     1600]]     [[train]]     loss: 0.40501846       [[val]]     loss: 0.38803977       \n",
    "[[step     1700]]     [[train]]     loss: 0.39387804       [[val]]     loss: 0.39030977       \n",
    "[[step     1800]]     [[train]]     loss: 0.39960696       [[val]]     loss: 0.38569073       \n",
    "[[step     1900]]     [[train]]     loss: 0.40731614       [[val]]     loss: 0.38400814       \n",
    "[[step     2000]]     [[train]]     loss: 0.39492307       [[val]]     loss: 0.39189795       \n",
    "[[step     2100]]     [[train]]     loss: 0.40242343       [[val]]     loss: 0.38887628       \n",
    "[[step     2200]]     [[train]]     loss: 0.40079135       [[val]]     loss: 0.38510605       \n",
    "[[step     2300]]     [[train]]     loss: 0.40135673       [[val]]     loss: 0.3891011        \n",
    "[[step     2400]]     [[train]]     loss: 0.40815621       [[val]]     loss: 0.38702249       \n",
    "[[step     2500]]     [[train]]     loss: 0.40195283       [[val]]     loss: 0.38379056       \n",
    "[[step     2600]]     [[train]]     loss: 0.41210029       [[val]]     loss: 0.38936703       \n",
    "[[step     2700]]     [[train]]     loss: 0.39930949       [[val]]     loss: 0.38703894       \n",
    "[[step     2800]]     [[train]]     loss: 0.40331845       [[val]]     loss: 0.38391135       \n",
    "[[step     2900]]     [[train]]     loss: 0.4059205        [[val]]     loss: 0.38919657       \n",
    "[[step     3000]]     [[train]]     loss: 0.39222894       [[val]]     loss: 0.38493212       \n",
    "[[step     3100]]     [[train]]     loss: 0.39660685       [[val]]     loss: 0.3870271        \n",
    "[[step     3200]]     [[train]]     loss: 0.39953087       [[val]]     loss: 0.38373316       \n",
    "[[step     3300]]     [[train]]     loss: 0.40271052       [[val]]     loss: 0.38703135       \n",
    "[[step     3400]]     [[train]]     loss: 0.39658011       [[val]]     loss: 0.38717001       \n",
    "[[step     3500]]     [[train]]     loss: 0.40102986       [[val]]     loss: 0.38259657       \n",
    "[[step     3600]]     [[train]]     loss: 0.40681617       [[val]]     loss: 0.3822414        \n",
    "[[step     3700]]     [[train]]     loss: 0.39806747       [[val]]     loss: 0.38907532      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 10000\n",
    "lr = 0.001\n",
    "embedding_size = 10-50\n",
    "batch_norm = False\n",
    "dropout = 0\n",
    "final layers = 2 100 - 500\n",
    "lstm = 600\n",
    "\n",
    "[[step        0]]     [[train]]     loss: 0.65691781       [[val]]     loss: 0.65873712       \n",
    "[[step      100]]     [[train]]     loss: 0.40820402       [[val]]     loss: 0.41145359       \n",
    "[[step      200]]     [[train]]     loss: 0.38525348       [[val]]     loss: 0.38974458       \n",
    "[[step      300]]     [[train]]     loss: 0.3921014        [[val]]     loss: 0.38808708       \n",
    "[[step      400]]     [[train]]     loss: 0.3891164        [[val]]     loss: 0.39563058       \n",
    "[[step      500]]     [[train]]     loss: 0.38700464       [[val]]     loss: 0.38803441       \n",
    "[[step      600]]     [[train]]     loss: 0.38530519       [[val]]     loss: 0.38530802       \n",
    "[[step      700]]     [[train]]     loss: 0.3820014        [[val]]     loss: 0.38732941       \n",
    "[[step      800]]     [[train]]     loss: 0.38262708       [[val]]     loss: 0.38449259       \n",
    "[[step      900]]     [[train]]     loss: 0.38381389       [[val]]     loss: 0.38279317       \n",
    "[[step     1000]]     [[train]]     loss: 0.38220652       [[val]]     loss: 0.38621109       \n",
    "[[step     1100]]     [[train]]     loss: 0.38452308       [[val]]     loss: 0.38545803       \n",
    "[[step     1200]]     [[train]]     loss: 0.38348263       [[val]]     loss: 0.38144997       \n",
    "[[step     1300]]     [[train]]     loss: 0.38537586       [[val]]     loss: 0.38501431       \n",
    "[[step     1400]]     [[train]]     loss: 0.38123322       [[val]]     loss: 0.38310651       \n",
    "[[step     1500]]     [[train]]     loss: 0.38254252       [[val]]     loss: 0.38495605       \n",
    "[[step     1600]]     [[train]]     loss: 0.37771166       [[val]]     loss: 0.38185424       \n",
    "[[step     1700]]     [[train]]     loss: 0.38058311       [[val]]     loss: 0.38246381       \n",
    "[[step     1800]]     [[train]]     loss: 0.38418386       [[val]]     loss: 0.38565543       \n",
    "[[step     1900]]     [[train]]     loss: 0.3772252        [[val]]     loss: 0.37936874       \n",
    "[[step     2000]]     [[train]]     loss: 0.38489054       [[val]]     loss: 0.38112491       \n",
    "[[step     2100]]     [[train]]     loss: 0.37931004       [[val]]     loss: 0.38886817       \n",
    "[[step     2200]]     [[train]]     loss: 0.38216422       [[val]]     loss: 0.38252707       \n",
    "[[step     2300]]     [[train]]     loss: 0.38276958       [[val]]     loss: 0.38094901       \n",
    "[[step     2400]]     [[train]]     loss: 0.3831657        [[val]]     loss: 0.38399107       \n",
    "[[step     2500]]     [[train]]     loss: 0.38187274       [[val]]     loss: 0.38105462       \n",
    "[[step     2600]]     [[train]]     loss: 0.38516242       [[val]]     loss: 0.38006142       \n",
    "[[step     2700]]     [[train]]     loss: 0.38071658       [[val]]     loss: 0.3846091        \n",
    "[[step     2800]]     [[train]]     loss: 0.38133765       [[val]]     loss: 0.38212655       \n",
    "[[step     2900]]     [[train]]     loss: 0.37816642       [[val]]     loss: 0.37891927       \n",
    "[[step     3000]]     [[train]]     loss: 0.38419693       [[val]]     loss: 0.38428155       \n",
    "[[step     3100]]     [[train]]     loss: 0.3781649        [[val]]     loss: 0.38119657       \n",
    "[[step     3200]]     [[train]]     loss: 0.38513189       [[val]]     loss: 0.38320454       \n",
    "[[step     3300]]     [[train]]     loss: 0.38730632       [[val]]     loss: 0.37999186       \n",
    "[[step     3400]]     [[train]]     loss: 0.38231762       [[val]]     loss: 0.38117058       \n",
    "[[step     3500]]     [[train]]     loss: 0.3831878        [[val]]     loss: 0.38359242       \n",
    "[[step     3600]]     [[train]]     loss: 0.38420483       [[val]]     loss: 0.37936406       \n",
    "[[step     3700]]     [[train]]     loss: 0.38203219       [[val]]     loss: 0.37749232       \n",
    "[[step     3800]]     [[train]]     loss: 0.37881707       [[val]]     loss: 0.38570269       \n",
    "[[step     3900]]     [[train]]     loss: 0.38056242       [[val]]     loss: 0.38261534       \n",
    "[[step     4000]]     [[train]]     loss: 0.37818603       [[val]]     loss: 0.37869769       \n",
    "[[step     4100]]     [[train]]     loss: 0.37891285       [[val]]     loss: 0.38342259       \n",
    "[[step     4200]]     [[train]]     loss: 0.37394667       [[val]]     loss: 0.38115527       \n",
    "[[step     4300]]     [[train]]     loss: 0.37930212       [[val]]     loss: 0.37814815       \n",
    "[[step     4400]]     [[train]]     loss: 0.37921889       [[val]]     loss: 0.3847813        \n",
    "[[step     4500]]     [[train]]     loss: 0.37409336       [[val]]     loss: 0.38351414       \n",
    "[[step     4600]]     [[train]]     loss: 0.37662914       [[val]]     loss: 0.38074349       \n",
    "[[step     4700]]     [[train]]     loss: 0.37437559       [[val]]     loss: 0.38719823       \n",
    "[[step     4800]]     [[train]]     loss: 0.37528483       [[val]]     loss: 0.3823637        \n",
    "[[step     4900]]     [[train]]     loss: 0.37472385       [[val]]     loss: 0.38422586       \n",
    "[[step     5000]]     [[train]]     loss: 0.37233163       [[val]]     loss: 0.38127384       \n",
    "[[step     5100]]     [[train]]     loss: 0.37854063       [[val]]     loss: 0.38465769       \n",
    "[[step     5200]]     [[train]]     loss: 0.3731164        [[val]]     loss: 0.38532945       \n",
    "[[step     5300]]     [[train]]     loss: 0.37532851       [[val]]     loss: 0.38037309       \n",
    "[[step     5400]]     [[train]]     loss: 0.37103678       [[val]]     loss: 0.38014162       \n",
    "[[step     5500]]     [[train]]     loss: 0.37317753       [[val]]     loss: 0.38685308       \n",
    "[[step     5600]]     [[train]]     loss: 0.36903966       [[val]]     loss: 0.38514764       \n",
    "[[step     5700]]     [[train]]     loss: 0.3736941        [[val]]     loss: 0.38151485       \n",
    "[[step     5800]]     [[train]]     loss: 0.37574548       [[val]]     loss: 0.38607571       \n",
    "[[step     5900]]     [[train]]     loss: 0.36797815       [[val]]     loss: 0.38232391       \n",
    "[[step     6000]]     [[train]]     loss: 0.37378719       [[val]]     loss: 0.37931299       \n",
    "[[step     6100]]     [[train]]     loss: 0.37145472       [[val]]     loss: 0.3854492        \n",
    "[[step     6200]]     [[train]]     loss: 0.37422799       [[val]]     loss: 0.38575146       \n",
    "[[step     6300]]     [[train]]     loss: 0.37489267       [[val]]     loss: 0.3798244        \n",
    "[[step     6400]]     [[train]]     loss: 0.37317946       [[val]]     loss: 0.3865814        \n",
    "[[step     6500]]     [[train]]     loss: 0.37357524       [[val]]     loss: 0.38192061       \n",
    "[[step     6600]]     [[train]]     loss: 0.37510183       [[val]]     loss: 0.38409594       \n",
    "[[step     6700]]     [[train]]     loss: 0.37295555       [[val]]     loss: 0.38225858       \n",
    "[[step     6800]]     [[train]]     loss: 0.37292404       [[val]]     loss: 0.38288011       \n",
    "[[step     6900]]     [[train]]     loss: 0.37217351       [[val]]     loss: 0.38604897       \n",
    "[[step     7000]]     [[train]]     loss: 0.37124587       [[val]]     loss: 0.37983757       \n",
    "[[step     7100]]     [[train]]     loss: 0.3734486        [[val]]     loss: 0.38058637       \n",
    "[[step     7200]]     [[train]]     loss: 0.37438173       [[val]]     loss: 0.38803061       \n",
    "[[step     7300]]     [[train]]     loss: 0.37942499       [[val]]     loss: 0.38332724       \n",
    "[[step     7400]]     [[train]]     loss: 0.37363714       [[val]]     loss: 0.38279361       \n",
    "[[step     7500]]     [[train]]     loss: 0.37495664       [[val]]     loss: 0.38467916       \n",
    "[[step     7600]]     [[train]]     loss: 0.37496262       [[val]]     loss: 0.38225796       \n",
    "[[step     7700]]     [[train]]     loss: 0.37258991       [[val]]     loss: 0.37846636       \n",
    "[[step     7800]]     [[train]]     loss: 0.37120649       [[val]]     loss: 0.38686409       \n",
    "[[step     7900]]     [[train]]     loss: 0.37128173       [[val]]     loss: 0.38571515       \n",
    "[[step     8000]]     [[train]]     loss: 0.37181711       [[val]]     loss: 0.37960169       \n",
    "[[step     8100]]     [[train]]     loss: 0.36826489       [[val]]     loss: 0.38574902       \n",
    "[[step     8200]]     [[train]]     loss: 0.37130566       [[val]]     loss: 0.38235267       \n",
    "[[step     8300]]     [[train]]     loss: 0.37132396       [[val]]     loss: 0.3838576        \n",
    "[[step     8400]]     [[train]]     loss: 0.36927456       [[val]]     loss: 0.38404872       \n",
    "[[step     8500]]     [[train]]     loss: 0.36493352       [[val]]     loss: 0.3866663        \n",
    "[[step     8600]]     [[train]]     loss: 0.36904615       [[val]]     loss: 0.38660518       \n",
    "[[step     8700]]     [[train]]     loss: 0.36574878       [[val]]     loss: 0.382791         \n",
    "[[step     8800]]     [[train]]     loss: 0.36782915       [[val]]     loss: 0.38435617       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     8900]]     [[train]]     loss: 0.36883847       [[val]]     loss: 0.38830326       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9000]]     [[train]]     loss: 0.36762432       [[val]]     loss: 0.38375869       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9100]]     [[train]]     loss: 0.372115         [[val]]     loss: 0.38439275       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9200]]     [[train]]     loss: 0.36688577       [[val]]     loss: 0.38790736       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9300]]     [[train]]     loss: 0.36813599       [[val]]     loss: 0.38178434       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9400]]     [[train]]     loss: 0.36671974       [[val]]     loss: 0.37995047       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9500]]     [[train]]     loss: 0.36658081       [[val]]     loss: 0.38970799       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9600]]     [[train]]     loss: 0.36533744       [[val]]     loss: 0.38604101       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9700]]     [[train]]     loss: 0.36672812       [[val]]     loss: 0.38222775       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9800]]     [[train]]     loss: 0.36806024       [[val]]     loss: 0.38704535       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step     9900]]     [[train]]     loss: 0.3634537        [[val]]     loss: 0.38424529       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training.\n",
    "[[step    10000]]     [[train]]     loss: 0.36658145       [[val]]     loss: 0.38508767       \n",
    "best validation loss of 0.377492317557 at training step 3700\n",
    "early stopping - ending training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLift(predics,actual,h_len, a_ids ,verbosity=0):\n",
    "    \n",
    "    aisles = np.unique(a_ids)\n",
    "    cum_lift = 0.0\n",
    "    cnt = 0\n",
    "\n",
    "    for n in range(len(aisles)):\n",
    "\n",
    "        predics_aisle = predics[np.where(a_ids==aisles[n])]\n",
    "        actual_aisle = actual[np.where(a_ids==aisles[n])]\n",
    "        h_len_aisle = h_len[np.where(a_ids==aisles[n])]\n",
    "\n",
    "        i = np.where(predics_aisle>np.percentile(predics_aisle,90))\n",
    "        j = np.where(predics_aisle>0)\n",
    "        num_i = len(i[0])*1.0\n",
    "        num_j = len(j[0])*1.0\n",
    "\n",
    "        up = (np.sum(actual_aisle[i[0],h_len_aisle[i]-1])*1.0)/num_i\n",
    "        down = (np.sum(actual_aisle[j[0],h_len_aisle[j]-1])*1.0)/num_j\n",
    "        lift = round(up/down,3)\n",
    "        if lift>0:\n",
    "            cum_lift += lift\n",
    "            cnt += 1\n",
    "\n",
    "        if verbosity==1:\n",
    "            print(lift , round(num_i/num_j,3), round(num_j/2000.0,2))\n",
    "\n",
    "    return round(cum_lift/cnt,2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
