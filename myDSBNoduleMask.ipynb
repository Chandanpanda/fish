{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use pretrained unet network - DONE\n",
    "#Add information about slice - TODAY\n",
    "#Are all lungs facing the same way = TOMMOROW?\n",
    "#Add information about nodule - TOMMOROW\n",
    "#Lung mask as per simple thresholding? Maybe No. But do measure jaccard/dice for thresholding Vs deep learning\n",
    "#Add information about patient\n",
    "#Try 3D Unet\n",
    "#Process data to have 2 classes - lung, Nodule\n",
    "#Use Jacckard index instead of dice coefficient\n",
    "#initialise using tutorial weights\n",
    "#preprocess to make 0 or 1 in training data\n",
    "#train on segmented lungs data using unet \n",
    "#test on validation set\n",
    "#Expand data from 160 to zoom on lung\n",
    "#does every CT scan have patient oriented in same way? TO BE VERIFIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from random import uniform\n",
    "import bcolz\n",
    "import time\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers import Input, Dense\n",
    "import math\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils as u\n",
    "\n",
    "numGPUs = 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/luna/home/ubuntu/data/masks\n",
      "/dsb/home/ubuntu/data/lungs\n",
      "/luna/home/ubuntu/data/lungs\n",
      "/luna/home/ubuntu/data\n"
     ]
    }
   ],
   "source": [
    "%cd '/luna/home/ubuntu/data/masks'\n",
    "lung_mask_files=glob(\"*.mhd\")\n",
    "%cd '/dsb/home/ubuntu/data/lungs'\n",
    "lung_files=glob(\"*.mhd\")\n",
    "%cd '/luna/home/ubuntu/data/lungs'\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_itk_image(filename):\n",
    "    itkimage = sitk.ReadImage(filename)\n",
    "    numpyImage = sitk.GetArrayFromImage(itkimage)\n",
    "    numpyOrigin = np.array(list(reversed(itkimage.GetOrigin())))\n",
    "    numpySpacing = np.array(list(reversed(itkimage.GetSpacing())))\n",
    "    return numpyImage, numpyOrigin, numpySpacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create sample dataset - lungs and masks\n",
    "\n",
    "CREATE_SAMPLE = False\n",
    "SAMPLE_SIZE = 0.4\n",
    "if CREATE_SAMPLE==True:\n",
    "    os.mkdir('masks-valid')\n",
    "    os.mkdir('lungs-valid')\n",
    "    shuffled_files = np.random.permutation(lung_files)\n",
    "    sample_size = int(round(len(lung_mask_files) * SAMPLE_SIZE,0))\n",
    "    for i in range(sample_size):\n",
    "        shuffled_files_raw = shuffled_files[i].replace(\"mhd\",\"zraw\")\n",
    "        shuffled_files_raw_lungs = shuffled_files[i].replace(\"mhd\",\"raw\")\n",
    "        os.rename('masks/' + shuffled_files[i], 'masks-valid/' + shuffled_files[i])\n",
    "        os.rename('masks/' + shuffled_files_raw, 'masks-valid/' + shuffled_files_raw) \n",
    "        os.rename('lungs/' + shuffled_files[i], 'lungs-valid/' + shuffled_files[i]) \n",
    "        os.rename('lungs/' + shuffled_files_raw_lungs, 'lungs-valid/' + shuffled_files_raw_lungs) \n",
    "# TBD LATER HANDLE 48 missing lungs in dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dsb/home/ubuntu/data/lungs\n",
      "/luna/home/ubuntu/data/lungs\n",
      "/dsb/home/ubuntu/data/lungs-valid\n",
      "/luna/home/ubuntu/data/lungs\n",
      "/luna/home/ubuntu/data\n"
     ]
    }
   ],
   "source": [
    "%cd '/dsb/home/ubuntu/data/lungs'\n",
    "lungs = glob('*.mhd')\n",
    "num_lungs = len(lungs)\n",
    "%cd '/luna/home/ubuntu/data/lungs'\n",
    "%cd '/dsb/home/ubuntu/data/lungs-valid'\n",
    "lungs_valid = glob('*.mhd')\n",
    "num_lungs_valid = len(lungs_valid)\n",
    "%cd '/luna/home/ubuntu/data/lungs'\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load data\n",
    "\n",
    "## load test lung\n",
    "img_path = \"/dsb/home/ubuntu/data/lungs/1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860.mhd\"\n",
    "numpyImage, numpyOrigin, numpySpacing = load_itk_image(img_path)\n",
    "print numpyImage.shape\n",
    "print numpyOrigin\n",
    "print numpySpacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(numpyImage[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## load test Mask\n",
    "img_path = \"masks/1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860.mhd\"\n",
    "numpyImage, numpyOrigin, numpySpacing = load_itk_image(img_path)\n",
    "print numpyImage.shape\n",
    "print numpyOrigin\n",
    "print numpySpacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(numpyImage[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = np.ndarray([512,512],dtype=np.int8)\n",
    "mask[:] = 0\n",
    "mask = numpyImage>0\n",
    "plt.imshow(mask[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Scale to all lungs, store in 20 datasets after resizing to 512 X 512 pixels\n",
    "\n",
    "FULL_LOAD = False\n",
    "\n",
    "if FULL_LOAD:\n",
    "    max_images = int(round(266 * num_lungs * 0.05,0))\n",
    "    final_images = np.ndarray([max_images,1,512,512],dtype=np.float32)\n",
    "    final_masks = np.ndarray([max_images,1,512,512],dtype=np.float32)\n",
    "    final_lungs = np.ndarray([max_images,2,3],dtype=np.float32)\n",
    "    final_slice_indices = np.ndarray([max_images,2],dtype=np.float32)\n",
    "\n",
    "    num_images = 0\n",
    "    lung_no = 0\n",
    "    fcount = 0\n",
    "    \n",
    "    for l in tqdm(lungs):\n",
    "        lung_no += 1\n",
    "        numpyImage, numpyOriginImage, numpySpacingImage = load_itk_image('/dsb/home/ubuntu/data/lungs/' + l)\n",
    "        numpyMask, numpyOriginMask, numpySpacingMask = load_itk_image('masks/' + l)\n",
    "        mask = np.ndarray([512,512],dtype=np.float32)\n",
    "        mask[:] = 0\n",
    "        mask = numpyMask>0\n",
    "        num_slices = numpyImage.shape[0]\n",
    "        for i in range(num_slices):\n",
    "            if uniform(0, 1) < 1.01:\n",
    "                if num_images < max_images:\n",
    "                    final_images[num_images,0] = numpyImage[i]\n",
    "                    final_masks[num_images,0] = mask[i]\n",
    "                    final_slice_indices[num_images,0] = lung_no\n",
    "                    final_slice_indices[num_images,1] = i\n",
    "                    final_lungs[num_images,0] = numpyOriginImage\n",
    "                    final_lungs[num_images,1] = numpySpacingImage\n",
    "                    num_images += 1\n",
    "                else:\n",
    "                    numImages = final_images.shape[0]\n",
    "                    final_images_160 = np.ndarray([numImages,1,512,512],dtype=np.float32)\n",
    "                    final_masks_160 = np.ndarray([numImages,1,512,512],dtype=np.float32)\n",
    "                    for i in range(numImages):\n",
    "                        final_images_160[i,0] = final_images[i,0]\n",
    "                        final_masks_160[i,0] = final_masks[i,0]\n",
    "                    final_images = final_images_160\n",
    "                    final_masks = final_masks_160\n",
    "                    save_array('/dsb/home/ubuntu/data/temp/final_images_%d.dat' % (fcount), final_images)\n",
    "                    save_array('final_masks_%d.dat' % (fcount), final_masks)\n",
    "                    save_array('final_slice_indices_%d.dat' % (fcount), final_slice_indices)\n",
    "                    save_array('final_lungs_%d.dat' % (fcount), final_lungs)\n",
    "                    final_images = np.ndarray([max_images,1,512,512],dtype=np.float32)\n",
    "                    final_masks = np.ndarray([max_images,1,512,512],dtype=np.float32)\n",
    "                    final_lungs = np.ndarray([max_images,2,3],dtype=np.float32)\n",
    "                    final_slice_indices = np.ndarray([max_images,2],dtype=np.float32)\n",
    "                    fcount += 1\n",
    "                    num_images = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if FULL_LOAD:\n",
    "    del(final_images)\n",
    "    del(final_masks)\n",
    "    del(final_slice_indices)\n",
    "    del(final_lungs)\n",
    "    del(final_images_160)\n",
    "    del(final_masks_160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Scale to all validation lungs, store in 20 datasets after resizing to 512 X 512 pixels\n",
    "\n",
    "FULL_LOAD = False\n",
    "\n",
    "if FULL_LOAD:\n",
    "    max_images = int(round(266 * num_lungs * 0.05,0))\n",
    "    final_images = np.ndarray([max_images,1,512,512],dtype=np.float32)\n",
    "    final_masks = np.ndarray([max_images,1,512,512],dtype=np.float32)\n",
    "    final_lungs = np.ndarray([max_images,2,3],dtype=np.float32)\n",
    "    final_slice_indices = np.ndarray([max_images,2],dtype=np.float32)\n",
    "\n",
    "    num_images = 0\n",
    "    lung_no = 0\n",
    "    fcount = 0\n",
    "    \n",
    "    for l in tqdm(lungs_valid):\n",
    "        lung_no += 1\n",
    "        numpyImage, numpyOriginImage, numpySpacingImage = load_itk_image('/dsb/home/ubuntu/data/lungs-valid/' + l)\n",
    "        numpyMask, numpyOriginMask, numpySpacingMask = load_itk_image('masks-valid/' + l)\n",
    "        mask = np.ndarray([512,512],dtype=np.float32)\n",
    "        mask[:] = 0\n",
    "        mask = numpyMask>0\n",
    "        num_slices = numpyImage.shape[0]\n",
    "        for i in range(num_slices):\n",
    "            if uniform(0, 1) < 1.01:\n",
    "                if num_images < max_images:\n",
    "                    final_images[num_images,0] = numpyImage[i]\n",
    "                    final_masks[num_images,0] = mask[i]\n",
    "                    final_slice_indices[num_images,0] = lung_no\n",
    "                    final_slice_indices[num_images,1] = i\n",
    "                    final_lungs[num_images,0] = numpyOriginImage\n",
    "                    final_lungs[num_images,1] = numpySpacingImage\n",
    "                    num_images += 1\n",
    "                else:\n",
    "                    numImages = final_images.shape[0]\n",
    "                    final_images_160 = np.ndarray([numImages,1,512,512],dtype=np.float32)\n",
    "                    final_masks_160 = np.ndarray([numImages,1,512,512],dtype=np.float32)\n",
    "                    for i in range(numImages):\n",
    "                        final_images_160[i,0] = final_images[i,0]\n",
    "                        final_masks_160[i,0] = final_masks[i,0]\n",
    "                    final_images = final_images_160\n",
    "                    final_masks = final_masks_160\n",
    "                    save_array('/dsb/home/ubuntu/data/temp/final_images_valid_%d.dat' % (fcount), final_images)\n",
    "                    save_array('final_masks_valid_%d.dat' % (fcount), final_masks)\n",
    "                    save_array('final_slice_indices_valid_%d.dat' % (fcount), final_slice_indices)\n",
    "                    save_array('final_lungs_valid_%d.dat' % (fcount), final_lungs)\n",
    "                    final_images = np.ndarray([max_images,1,512,512],dtype=np.float32)\n",
    "                    final_masks = np.ndarray([max_images,1,512,512],dtype=np.float32)\n",
    "                    final_lungs = np.ndarray([max_images,2,3],dtype=np.float32)\n",
    "                    final_slice_indices = np.ndarray([max_images,2],dtype=np.float32)\n",
    "                    fcount += 1\n",
    "                    num_images = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if FULL_LOAD:\n",
    "    del(final_images)\n",
    "    del(final_masks)\n",
    "    del(final_slice_indices)\n",
    "    del(final_lungs)\n",
    "    del(final_images_160)\n",
    "    del(final_masks_160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_node = pd.read_csv('/dsb/home/ubuntu/data/annotations_id.csv')\n",
    "df_node = df_node.dropna()\n",
    "df_node.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeNoduleMasks():\n",
    "    numImages = final_lungs.shape[0]\n",
    "    NoduleMasks = np.ndarray([numImages,1,512,512],dtype=np.float32)\n",
    "    masks = np.ndarray([512,512],dtype=np.float32)\n",
    "    NoduleMasks[:] = 0\n",
    "    for imgIndex in range(numImages):\n",
    "        mini_df = df_node[df_node[\"lungid\"]==float(final_slice_indices[imgIndex,0])] #get all nodules associate with file\n",
    "        masks[:] = 0\n",
    "        if mini_df.shape[0]>0:\n",
    "            origin = final_lungs[int(final_slice_indices[imgIndex,0]),0]         # x,y,z  Origin in world coordinates (mm)\n",
    "            spacing = final_lungs[int(final_slice_indices[imgIndex,0]),1]        # spacing of voxels in world coor. (mm)\n",
    "            for node_idx, cur_row in mini_df.iterrows():       \n",
    "                node_x = cur_row[\"coordX\"]\n",
    "                node_y = cur_row[\"coordY\"]\n",
    "                node_z = cur_row[\"coordZ\"]\n",
    "                diam = cur_row[\"diameter_mm\"]\n",
    "                center = np.array([node_z, node_y,node_x])                   # nodule center in world coor. (mm)\n",
    "                v_center = np.rint((center-origin)/spacing)                  # nodule center in voxel space \n",
    "                radius_z = int(np.round((diam/spacing[0])/2,0))              # nodule radius in voxel space \n",
    "                radius_y = int(np.round((diam/spacing[1])/2,0))              # nodule radius in voxel space \n",
    "                radius_x = int(np.round((diam/spacing[2])/2,0))              # nodule radius in voxel space \n",
    "                image_z = final_slice_indices[imgIndex,1]+1                  # Assumes that first slice starts at z=0 in voxel space\n",
    "                if (v_center[0]+radius_z>=image_z) & (v_center[0]-radius_z<=image_z):\n",
    "                    pixel_nodule_radius_projected_x = np.sqrt(max(0,np.square(radius_x) \n",
    "                                                                  - np.square(v_center[0]-image_z)))     # in voxel space \n",
    "                    pixel_nodule_radius_projected_y = np.sqrt(max(0,np.square(radius_y) \n",
    "                                                                  - np.square(v_center[0]-image_z)))     # in voxel space \n",
    "                    pixels_to_keep = range(512)\n",
    "                    for pixel_x in pixels_to_keep:\n",
    "                        for pixel_y in pixels_to_keep:\n",
    "                            pixel_x_adj = pixel_x * 1 + 0\n",
    "                            pixel_y_adj = pixel_y * 1 + 0\n",
    "                            distance_x = abs(pixel_x_adj - v_center[2])\n",
    "                            distance_y = abs(pixel_y_adj - v_center[1])\n",
    "                            if (distance_x <= pixel_nodule_radius_projected_x) & (distance_y <= pixel_nodule_radius_projected_y):\n",
    "                                   masks[pixel_y,pixel_x] = 1\n",
    "            NoduleMasks[imgIndex,0] = masks\n",
    "    return NoduleMasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create Nodule Masks for training data\n",
    "for i in range(19):  \n",
    "    final_masks = load_array('/luna/home/ubuntu/data/final_masks_%d.dat'%i)\n",
    "    final_lungs = load_array('/luna/home/ubuntu/data/final_lungs_%d.dat'%i)\n",
    "    final_slice_indices = load_array('/luna/home/ubuntu/data/final_slice_indices_%d.dat'%i)\n",
    "    save_array('/luna/home/ubuntu/data/final_nodule_masks_%d.dat'%i,makeNoduleMasks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeat for validation\n",
    "for i in range(12):  \n",
    "    final_masks = load_array('/luna/home/ubuntu/data/final_masks_valid_%d.dat'%i)\n",
    "    final_lungs = load_array('/luna/home/ubuntu/data/final_lungs_valid_%d.dat'%i)\n",
    "    final_slice_indices = load_array('/luna/home/ubuntu/data/final_slice_indices_valid_%d.dat'%i)\n",
    "    save_array('/luna/home/ubuntu/data/final_nodule_masks_valid_%d.dat'%i,makeNoduleMasks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert to the shape and scale that unet expects\n",
    "def standardizeImage(final_images_160):    \n",
    "    #Normalise data before training\n",
    "    mean = -771.49414                         \n",
    "    std = 959.30872\n",
    "    final_images_160 = final_images_160 - mean\n",
    "    final_images_160 = final_images_160 / std\n",
    "    minHU = np.percentile(final_images_160,1)\n",
    "    maxHU = np.percentile(final_images_160,99)\n",
    "    final_images_160 = np.clip(final_images_160,minHU,maxHU)\n",
    "    OldRange = (maxHU - minHU)  \n",
    "    NewRange = 1 - (-1)  \n",
    "    final_images_160 = ((((final_images_160 - minHU) / OldRange)) * NewRange) - 1\n",
    "    return final_images_160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unet():\n",
    "    inputs = Input((1,512,512))\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(pool2)\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(pool3)\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(pool4)\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(conv5)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(up6)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(up7)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(up8)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    model = Model(input=inputs, output=up9)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Precompute features till layer \"up9\"\n",
    "def preComputeFeatures(final_images_160,chunkIndex,valid):\n",
    "    final_nodule_masks = load_array('/luna/home/ubuntu/data/final_nodule_masks'+valid+'_%d.dat'%chunkIndex)\n",
    "    final_masks = load_array('/luna/home/ubuntu/data/final_masks'+valid+'_%d.dat'%chunkIndex)\n",
    "    final_lungs = load_array('/luna/home/ubuntu/data/final_lungs'+valid+'_%d.dat'%chunkIndex)\n",
    "    final_slice_indices = load_array('/luna/home/ubuntu/data/final_slice_indices'+valid+'_%d.dat'%chunkIndex)\n",
    "    numImages = final_images_160.shape[0]\n",
    "    tempImages = np.ndarray([numImages,1,512,512],dtype=np.float32)\n",
    "    tempNodules = np.ndarray([numImages,1,512,512],dtype=np.float32)\n",
    "    tempMasks = np.ndarray([numImages,1,512,512],dtype=np.float32)\n",
    "    tempLungs = np.ndarray([numImages,2,3],dtype=np.float32)\n",
    "    tempSliceIndices = np.ndarray([numImages,2],dtype=np.float32)\n",
    "    sampledImagesCount = 0\n",
    "    for i in range(numImages):\n",
    "        lung_pixel_count = np.sum(final_masks[i,0])\n",
    "        nodule_pixel_count = np.sum(final_nodule_masks[i,0])\n",
    "        if nodule_pixel_count>0:\n",
    "            random_seed = uniform(0,0.08)\n",
    "        else:\n",
    "            if lung_pixel_count > 0:\n",
    "                random_seed = uniform(0,1)\n",
    "            else:\n",
    "                random_seed = uniform(0,5)\n",
    "        if random_seed < 0.02:\n",
    "            tempImages[sampledImagesCount,0] = final_images_160[i,0]\n",
    "            tempMasks[sampledImagesCount,0] = final_masks[i,0]\n",
    "            tempNodules[sampledImagesCount,0] = final_nodule_masks[i,0]\n",
    "            tempLungs[sampledImagesCount] = final_lungs[i]\n",
    "            tempSliceIndices[sampledImagesCount] = final_slice_indices[i]\n",
    "            sampledImagesCount += 1\n",
    "\n",
    "    del(final_images_160)\n",
    "    del(final_masks)\n",
    "    del(final_nodule_masks)\n",
    "    del(final_lungs)\n",
    "    del(final_slice_indices)\n",
    "\n",
    "    final_images_features = np.ndarray([sampledImagesCount-1,96,512,512],dtype=np.float32)\n",
    "    if sampledImagesCount>0:\n",
    "        model = get_unet()\n",
    "        model.load_weights('../unet_initial.hdf5',by_name=True)\n",
    "        for img in range(sampledImagesCount-1):\n",
    "            final_images_features[img] = model.predict([tempImages[img:img+1]], verbose=0)[0]\n",
    "        save_array('/luna/home/ubuntu/data/temp/final_images_features'+valid+'_%d.dat' \n",
    "                          % (chunkIndex), final_images_features)  \n",
    "        save_array('/luna/home/ubuntu/data/temp/final_images_nodule_masks'+valid+'_%d.dat' \n",
    "                       % (chunkIndex), tempNodules[0:(sampledImagesCount-1)])  \n",
    "        save_array('/luna/home/ubuntu/data/temp/final_images_masks'+valid+'_%d.dat' \n",
    "                       % (chunkIndex), tempMasks[0:(sampledImagesCount-1)])  \n",
    "        save_array('/luna/home/ubuntu/data/temp/final_lungs'+valid+'_%d.dat' \n",
    "                       % (chunkIndex), tempLungs[0:(sampledImagesCount-1)])  \n",
    "        save_array('/luna/home/ubuntu/data/temp/final_slice_indices'+valid+'_%d.dat' \n",
    "                       % (chunkIndex), tempSliceIndices[0:(sampledImagesCount-1)])  \n",
    "\n",
    "        del(final_images_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precompute the features using a unet network for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#Precompute features for training dataset\n",
    "for f in range(2,17):\n",
    "    try:\n",
    "        preComputeFeatures(standardizeImage(load_array('/dsb/home/ubuntu/data/temp/final_images_%d.dat'%f)),f,\"\") \n",
    "    except:\n",
    "        print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeat for validation dataset\n",
    "for f in range(12):\n",
    "    try:\n",
    "        preComputeFeatures(standardizeImage(load_array('/dsb/home/ubuntu/data/temp/final_images_valid_%d.dat'%f)),f,\"_valid\") \n",
    "    except:\n",
    "        print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create categorical slice numbers\n",
    "rawFiles = glob(\"/luna/home/ubuntu/data/temp/final_slice_indices*.dat\")\n",
    "max_slice_global = 0\n",
    "for filename in rawFiles:\n",
    "    f=load_array(filename)\n",
    "    length = f.shape[0]\n",
    "    max_slice = np.max(f[range(length),])\n",
    "    if max_slice_global < max_slice: max_slice_global = max_slice\n",
    "#Repeat for validation set\n",
    "rawFiles = glob(\"/dsb/home/ubuntu/data/temp/final_slice_indices*.dat\")\n",
    "for filename in rawFiles:\n",
    "    f=load_array(filename)\n",
    "    length = f.shape[0]\n",
    "    max_slice = np.max(f[range(length),])\n",
    "    if max_slice_global < max_slice: max_slice_global = max_slice\n",
    "max_slice_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawFiles = glob(\"/luna/home/ubuntu/data/temp/final_slice_indices*.dat\")\n",
    "for filename in rawFiles:\n",
    "    f=load_array(filename)\n",
    "    length = f.shape[0]\n",
    "    a = u.to_categorical(f[range(length)][1],max_slice_global)\n",
    "    save_array(filename.replace(\"final_slice_indices\",\"final_slice_indices_cat\"),a)\n",
    "rawFiles = glob(\"/dsb/home/ubuntu/data/temp/final_slice_indices*.dat\")\n",
    "for filename in rawFiles:\n",
    "    f=load_array(filename)\n",
    "    length = f.shape[0]\n",
    "    a = u.to_categorical(f[range(length)][1],max_slice_global)\n",
    "    save_array(filename.replace(\"final_slice_indices\",\"final_slice_indices_cat\"),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_np(y_true,y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smooth_j = 1e-12\n",
    "\n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    # __author__ = Vladimir Iglovikov\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth_j) / (sum_ - intersection + smooth_j)\n",
    "    return K.mean(jac)\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    # __author__ = Vladimir Iglovikov\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth_j) / (sum_ - intersection + smooth_j)\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_loss(y_true, y_pred):\n",
    "    return -jaccard_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_last_layer(s):\n",
    "    inputs = Input(shape=s)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "    conv2 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)\n",
    "    conv3 = Convolution2D(1,1, 1, activation='sigmoid')(conv2)\n",
    "    model = Model(input=[inputs], output=conv3)    \n",
    "    return model  \n",
    "\n",
    "lrg_model = get_last_layer((96,512,512))   \n",
    "lrg_model.summary()\n",
    "lrg_model.compile(Adam(lr=0.001), loss=dice_coef_loss, metrics=[jaccard_coef_int,dice_coef])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches from training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_training_batches(batch_size,data_size):\n",
    "    for i in range(data_size):\n",
    "        featuresFiles = glob('/dsb/home/ubuntu/data/temp/final_images_features_%d.dat'%i)\n",
    "        featuresFiles = np.random.permutation(featuresFiles)\n",
    "        for f in featuresFiles:\n",
    "            features = load_array(f)\n",
    "            #masks = load_array(f.replace(\"images_features\",\"images_masks\"))\n",
    "            masks = load_array(f.replace(\"images_features\",\"images_nodule_masks\"))\n",
    "            numRows = features.shape[0]\n",
    "            count = 0\n",
    "            x = np.ndarray([batch_size,1,512,512],dtype=np.float32)\n",
    "            y = np.ndarray([batch_size,1,512,512],dtype=np.float32)\n",
    "            while (count < numRows) and (count+batch_size<numRows):\n",
    "                x = features[count:(count+batch_size)]\n",
    "                y = masks[count:(count+batch_size)]\n",
    "                count += batch_size\n",
    "                yield (x,y)\n",
    "\n",
    "def generate_validation_batches(batch_size,data_size):\n",
    "    for i in range(data_size):\n",
    "        featuresFiles = glob('/luna/home/ubuntu/data/temp/final_images_features_valid_%d.dat'%i)\n",
    "        for f in featuresFiles:\n",
    "            features = load_array(f)\n",
    "            #masks = load_array(f.replace(\"images_features\",\"images_masks\"))\n",
    "            masks = load_array(f.replace(\"images_features_valid\",\"images_nodule_masks_valid\"))\n",
    "            numRows = features.shape[0]\n",
    "            count = 0\n",
    "            while (count < numRows) and (count+batch_size<numRows):\n",
    "                x = features[count:(count+batch_size)]\n",
    "                y = masks[count:(count+batch_size)]\n",
    "                count += batch_size\n",
    "                yield (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainMyNw(nb_epoch):\n",
    "    best_loss_metric = 0\n",
    "    print \"training network using features from:\"\n",
    "    %cd '/luna/home/ubuntu/data/features'\n",
    "    for e in range(nb_epoch):\n",
    "        print 'epoch %d:'%e\n",
    "        start = time.clock()\n",
    "        f = open('/luna/home/ubuntu/log.txt', 'a')\n",
    "        for x,y in generate_training_batches(4,15):\n",
    "            lrg_model.fit(x, y, batch_size = 4, nb_epoch=1, verbose=0,shuffle=True)\n",
    "        for x,y in generate_validation_batches(4,13):\n",
    "            loss_metrics = lrg_model.evaluate(x, y, batch_size = 4, verbose=0)\n",
    "        print lrg_model.metrics_names,loss_metrics, \"time :\", time.clock() - start\n",
    "        f.write(str(lrg_model.metrics_names))\n",
    "        f.write(\" : \")\n",
    "        f.write(str(loss_metrics))\n",
    "        f.write(\" : time :\")\n",
    "        f.write(str(time.clock() - start))\n",
    "        f.write(\"\\n\")        \n",
    "        f.close() \n",
    "        if (loss_metrics[0]<best_loss_metric):\n",
    "            lrg_model.save_weights('/dsb/home/ubuntu/lung_mask_best.hdf5') \n",
    "            best_loss_metric = loss_metrics[0]\n",
    "    return loss_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.optimizer.lr = 1e-2\n",
    "loss_metrics=trainMyNw(1)\n",
    "lrg_model.optimizer.lr = 1e-3\n",
    "loss_metrics=trainMyNw(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.optimizer.lr = 1e-4\n",
    "loss_metrics=trainMyNw(10)        \n",
    "lrg_model.save_weights('/dsb/home/ubuntu/lung_nodule_mask.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Booz Allen Model 5 epochs with 1e-4 LR & 4 BS to predict Lung Masks                 :: dice=.025:.019:.015:NA:.007,jacc=.000286:.000867:.001197:NA:0.000711,time=1942 \n",
    "\n",
    "Train Booz Allen Model 5 epochs with 1e-4 LR & 4 BS to predict Lung Nodules               :: dice=,jacc=,time=\n",
    "\n",
    "Train Multi Input Model 5 epochs with 1e-4 LR & 4 BS to predict Lung Masks & Lung Nodules :: dice=,jacc=,time=\n",
    "\n",
    "Ideas for improvement:\n",
    "1. Try larger batch size\n",
    "2. Try multiple input\n",
    "3. Try Histogram Equalization\n",
    "4. Try Data Augmentation\n",
    "5. Try Regularization\n",
    "6. Shuffle generators"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
