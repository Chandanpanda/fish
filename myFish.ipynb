{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Perform initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "%matplotlib inline\n",
    "import utils\n",
    "reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function\n",
    "path = \"\"\n",
    "batch_size = 64\n",
    "validation_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a 20% validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preserve the class imbalances in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd \"train\"\n",
    "species = glob(\"*\")\n",
    "for d in species:\n",
    "    os.mkdir(\"..\\valid\\\"+d)\n",
    "    images = glob(d+\"\\*\")\n",
    "    shuf = np.random.permutation(images)\n",
    "    sample_size = int(round(len(images) * validation_size , 0))\n",
    "    for i in range(sample_size): os.rename(shuf[i], '../valid/' + shuf[i])\n",
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3022 images belonging to 8 classes.\n",
      "Found 755 images belonging to 8 classes.\n",
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels , trn_labels, val_filenames, trn_filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg640 = Vgg16BN(360,640).model\n",
    "vgg640.pop()\n",
    "vgg640.input_shape(), vgg640.output_shape()\n",
    "vgg640.compile(Adam(), 'categorical_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data in 360 X 640 format and augment it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce random variations in the training data including rotation, width shift, zoom etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, \n",
    "                               shear_range=0.15, zoom_range = 0.1 , channel_shift_range = 10. , horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3022 images belonging to 8 classes.\n",
      "Found 755 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', gen,shuffle=False, batch_size=1, class_mode=None, target_size=(360,640))\n",
    "trn = np.concatenate([batches.next() for i in range(batches.nb_sample)])\n",
    "\n",
    "batches = get_batches(path+'valid', shuffle=False, batch_size=1, class_mode=None, target_size=(360,640))\n",
    "val = np.concatenate([batches.next() for i in range(batches.nb_sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = vgg640.predict(trn, batch_size = 32, verbose =1)\n",
    "val_features = vgg640.predict(val, batch_size = 32, verbose =1)\n",
    "del trn, val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(path+'test', shuffle=False, batch_size=1, class_mode=None, target_size=(360,640))\n",
    "test = np.concatenate([batches.next() for i in range(batches.nb_sample)])\n",
    "test_features = vgg640.predict(test, batch_size = 32, verbose =1)\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy or load it from disk, if available \n",
    "\n",
    "################################################    RESUME FROM HERE   ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FIRST_TIME = False\n",
    "if(FIRST_TIME):\n",
    "    save_array(path+'val_features.dat', val_features)\n",
    "    save_array(path+'trn_features.dat', trn_features)\n",
    "    save_array(path+'test_features.dat', test_features)\n",
    "else:\n",
    "    val_features = load_array(path+'val_features.dat')\n",
    "    trn_features = load_array(path+'trn_features.dat')\n",
    "    test_features = load_array(path+'test_features.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the final dense layer making this is a fully convolutional net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_layers,_ = split_at(vgg640,Convolution_2D)\n",
    "nf=128, p = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add four convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_final_layers():\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((1,2)),\n",
    "        Convolution2D(8,3,3, border_mode='same'),\n",
    "        Dropout(p),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]\n",
    "myModel = Sequential(get_final_layers())\n",
    "myModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a couple of epochs with high learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myModel.compile(Adam(lr=0.001),loss='categorical_crossentropy' , metrics = ['accuracy'])\n",
    "myModel.fit(trn_features, trn_labels, batch_size = batch_size, nb_epoch = 2 , validation_data = (val_features, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train another 6 epochs with low learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myModel.optimizer.lr=1e-4\n",
    "myModel.fit(trn_features, trn_labels, batch_size = batch_size, nb_epoch = 6 , validation_data = (val_features, val_labels))\n",
    "myModel.save_weights(path+'models/myModel.h5')\n",
    "myModel.evaluate(val_features,val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/7, mx)\n",
    "predictions = myModel.predict(test_features, batch_size = batch_size)\n",
    "predictions_clipped = do_clip(predictions, 0.72)\n",
    "classes = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'image', raw_test_filenames)\n",
    "submission.head()\n",
    "subm_name = path+'submission.csv'\n",
    "submission.to_csv(subm_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
