{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SQUEEZENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, GlobalMaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from random import uniform\n",
    "import bcolz\n",
    "from time import *\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers import Input, Dense\n",
    "import math\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils as u\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "import resnet50; reload(resnet50)\n",
    "from resnet50 import Resnet50\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import myFunctionsv3; reload(myFunctionsv3)\n",
    "from myFunctionsv3 import *\n",
    "from  keras.applications.resnet50 import ResNet50\n",
    "from squeezenet import SqueezeNet\n",
    "\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf=128; p=0.01\n",
    "def get_lrg_layers():\n",
    "    \n",
    "    inp = Input((21, 39, 512))\n",
    "    x = BatchNormalization(axis=1)(inp)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Conv2D(nf,(1,1), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = Conv2D(nf,(3,3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = Conv2D(nf,(3,3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Conv2D(nf,(1,1), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = Conv2D(nf,(3,3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = Conv2D(nf,(3,3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Conv2D(nf,(1,1), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = Conv2D(nf,(3,3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    \n",
    "    x_15 = Conv2D(15,(3,3), activation='relu', padding='same')(x)\n",
    "    x_15 = BatchNormalization(axis=1)(x_15)\n",
    "    x_15 = GlobalMaxPooling2D()(x_15)\n",
    "    x_sig_15 = Activation('sigmoid')(x_15)\n",
    "    \n",
    "    x_4 = Conv2D(4,(3,3), activation='relu', padding='same')(x)\n",
    "    x_4 = BatchNormalization(axis=1)(x_4)\n",
    "    x_4 = GlobalMaxPooling2D()(x_4)\n",
    "    x_sof_4 = Activation('sigmoid')(x_4)\n",
    "    \n",
    "    x_2 = Conv2D(2,(3,3), activation='relu', padding='same')(x)\n",
    "    x_2 = BatchNormalization(axis=1)(x_2)\n",
    "    x_2 = GlobalMaxPooling2D()(x_2)\n",
    "    x_sof_2 = Activation('sigmoid')(x_2)\n",
    "    \n",
    "    model = Model([inp], [x_sig_15, x_sof_4, x_sof_2])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model = get_lrg_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 21, 39, 512)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 21, 39, 512)  84          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 10, 19, 512)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 10, 19, 128)  65664       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 10, 19, 128)  40          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 10, 19, 128)  147584      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 10, 19, 128)  40          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 10, 19, 128)  147584      batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 10, 19, 128)  40          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 5, 9, 128)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 5, 9, 128)    16512       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 5, 9, 128)    20          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 5, 9, 128)    147584      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 5, 9, 128)    20          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 5, 9, 128)    147584      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 5, 9, 128)    20          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 2, 4, 128)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 2, 4, 128)    16512       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 2, 4, 128)    8           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 2, 4, 128)    147584      batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 2, 4, 128)    8           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 2, 4, 15)     17295       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 2, 4, 4)      4612        batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 2, 4, 2)      2306        batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 2, 4, 15)     8           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 2, 4, 4)      8           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 2, 4, 2)      8           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 15)           0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 4)            0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalM (None, 2)            0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15)           0           global_max_pooling2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 4)            0           global_max_pooling2d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 2)            0           global_max_pooling2d_3[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 861,125\n",
      "Trainable params: 860,973\n",
      "Non-trainable params: 152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lrg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15183"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addZeroLabelCannula(y):\n",
    "    temp = sum(np.transpose(y)[[1,2,3,4],:]) \n",
    "    temp[temp>1]=1\n",
    "    temp = 1 - temp \n",
    "    final = np.hstack((y[:,[1,2,3,4]],np.expand_dims(temp,1)))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addZeroLabelForceps(y):\n",
    "    temp = sum(np.transpose(y)[[7,8],:]) \n",
    "    temp[temp>1]=1\n",
    "    temp = 1 - temp \n",
    "    final = np.hstack((y[:,[7,8]],np.expand_dims(temp,1)))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_generator(files):\n",
    "    while 1:\n",
    "        for i in range(len(files)):\n",
    "            conv_trn_feat = load_array('/cat/home/ubuntu/cat/out/features/train/'+files[i]+'_sqznt.dat')\n",
    "            label = load_array('/cat/home/ubuntu/cat/in/train/folds/'+files[i]+'_labels.dat')\n",
    "            totalFrames = label.shape[0]\n",
    "            batches = int(totalFrames/batch_size)\n",
    "            for j in range(batches):\n",
    "                x = conv_trn_feat[j*batch_size:(j+1)*batch_size,]\n",
    "                y = label[j*batch_size:(j+1)*batch_size,2:23]\n",
    "                \n",
    "                y1 = y[:,[0,5,6,9,10,11,12,13,14,15,16,17,18,19,20]] #remaining\n",
    "                \n",
    "                #y2 = addZeroLabelCannula(y)\n",
    "                y2 = y[:,[1,2,3,4]]\n",
    "                #Charleux canula, hydrodissection canula, Rycroft canula, viscoelastic cannula\n",
    "                \n",
    "                #y3 = addZeroLabelForceps(y)\n",
    "                y3 = y[:,[7,8]]\n",
    "                #Bonn forceps, capsulorhexis forceps\n",
    "                \n",
    "                yield (x, [y1,y2,y3])\n",
    "            del(conv_trn_feat,label)\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_generator_val(files):\n",
    "    while 1:\n",
    "        for i in range(len(files)):\n",
    "            conv_val_feat = load_array('/cat/home/ubuntu/cat/out/features/train/'+files[i]+'_sqznt.dat')\n",
    "            label_val = load_array('/cat/home/ubuntu/cat/in/train/labels/'+files[i]+'_labels.dat')\n",
    "            totalFrames = label_val.shape[0]\n",
    "            batches = int(totalFrames/batch_size)\n",
    "            for j in range(batches): \n",
    "                x = conv_val_feat[j*batch_size:(j+1)*batch_size,]\n",
    "                y = label_val[j*batch_size:(j+1)*batch_size,2:23]\n",
    "                \n",
    "                y1 = y[:,[0,5,6,9,10,11,12,13,14,15,16,17,18,19,20]] #remaining\n",
    "                \n",
    "                #y2 = addZeroLabelCannula(y)\n",
    "                y2 = y[:,[1,2,3,4]]\n",
    "                #Charleux canula, hydrodissection canula, Rycroft canula, viscoelastic cannula\n",
    "                \n",
    "                #y3 = addZeroLabelForceps(y)\n",
    "                y3 = y[:,[7,8]]\n",
    "                #Bonn forceps, capsulorhexis forceps\n",
    "                \n",
    "                yield (x, [y1,y2,y3])\n",
    "            del(conv_val_feat,label_val)\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107050\n"
     ]
    }
   ],
   "source": [
    "files = ['trainfold1','trainfold2','trainfold3','trainfold4','trainfold5',\n",
    "         'trainfold6','trainfold7','trainfold8','trainfold9','trainfold0']\n",
    "totalCount=0\n",
    "for i in range(len(files)):\n",
    "    labelFile = load_array('/cat/home/ubuntu/cat/in/train/folds/'+files[i]+'_labels.dat')\n",
    "    totalCount += labelFile.shape[0]\n",
    "    del(labelFile)\n",
    "print(totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30775\n"
     ]
    }
   ],
   "source": [
    "files_val = ['train03_val_minus_0p5','train04_val_minus_0p5','train10_val_minus_0p5','train18_val_minus_0p5',\n",
    "         'train21_val_minus_0p5','train14_val_minus_0p5','train19_val_minus_0p5','train24_val_minus_0p5']\n",
    "totalCountVal=0\n",
    "for i in range(len(files_val)):\n",
    "    labelFile = load_array('/cat/home/ubuntu/cat/in/train/labels/'+files_val[i]+'_labels.dat')\n",
    "    totalCountVal += labelFile.shape[0]\n",
    "    del(labelFile)\n",
    "print(totalCountVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.compile(Adam(lr=0.01), loss=['binary_crossentropy', 'binary_crossentropy','binary_crossentropy'], metrics=['accuracy'],\n",
    "             loss_weights=[0.45, 0.30,0.25])    \n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  app.launch_new_instance()\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., epochs=3, validation_data=<generator..., steps_per_epoch=1672, validation_steps=480)`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "lrg_model.fit_generator(feature_generator(files),steps_per_epoch=totalCount/batch_size,\n",
    "                                  nb_epoch=3,validation_data=feature_generator_val(files_val),\n",
    "                        validation_steps=totalCountVal/batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1,min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss',factor=0.1, patience=2, cooldown=2, verbose=1),\n",
    "                 ModelCheckpoint(filepath='../out/weights/sqznt_weights_folds_8_9_128_Iteration13.hdf5',\n",
    "                                 save_best_only=True,save_weights_only=True)]\n",
    "K.set_value(lrg_model.optimizer.lr, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Benchmark loss: 0.0677\n",
    "history = lrg_model.fit_generator(feature_generator(files),steps_per_epoch=totalCount/batch_size,\n",
    "                                  nb_epoch=50,callbacks=callbacks,\n",
    "                                  validation_data=feature_generator_val(files_val),validation_steps=totalCountVal/batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.save_weights('../out/weights/sqznt_weights_folds_8_9_128_Iteration13_final_epoch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take backup of weights before you rerun the above code!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['activation_1_acc'])\n",
    "plt.plot(history.history['val_activation_1_acc'])\n",
    "plt.title('model accuracy 15')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['activation_2_acc'])\n",
    "plt.plot(history.history['val_activation_2_acc'])\n",
    "plt.title('model accuracy 2')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['activation_3_acc'])\n",
    "plt.plot(history.history['val_activation_3_acc'])\n",
    "plt.title('model accuracy 3')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Temporary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from densenet import DenseNet, DenseNetImageNet121\n",
    "dense = DenseNetImageNet121(input_shape=(360,640,3),weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs = dense.input, outputs = dense.layers[-18].output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDenseNetFeatures(path,name):\n",
    "    rgb_hires = load_array(path + name + '_hires.dat')\n",
    "    conv_feat = model.predict(rgb_hires, batch_size=32, verbose=0)\n",
    "    save_array('/cat/home/ubuntu/cat/out/features/train/'+name+'_dnsnt.dat',conv_feat)\n",
    "    del(conv_feat,rgb_hires)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = ['trainfold1','trainfold2','trainfold3','trainfold4','trainfold5',\n",
    "         'trainfold6','trainfold7','trainfold8','trainfold9','trainfold0'] \n",
    "\n",
    "for i in tqdm(range(len(files))):\n",
    "    createDenseNetFeatures('/cat/home/ubuntu/cat/in/train/folds/',files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.load_weights('/cat/home/ubuntu/cat/out/weights/sqznt_weights_folds_8_9_128_Iteration10.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame=155\n",
    "pred = lrg_model.predict(np.expand_dims(conv_val_feat[frame],0))\n",
    "pred = np.hstack((pred[0],pred[1],pred[2]))\n",
    "pred.shape\n",
    "np.round(pred[0],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 0,5,6,9,10,11,12,13,14,15,16,17,18,19,20,1,2,3,4,7,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_temp = np.copy(labels_val[:,2:23])\n",
    "labels_temp[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]] = labels_temp[:,[0,5,6,9,10,11,12,13,14,15,16,17,18,19,20,1,2,3,4,7,8]]\n",
    "labels_temp[frame,0:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = ['train03_val_minus_0p5','train04_val_minus_0p5','train10_val_minus_0p5','train18_val_minus_0p5',\n",
    "         'train21_val_minus_0p5','train14_val_minus_0p5','train19_val_minus_0p5','train24_val_minus_0p5']\n",
    "FIRST_TIME=True\n",
    "for i in range(len(files)):\n",
    "    conv_val_feat = load_array('/cat/home/ubuntu/cat/out/features/train/'+files[i]+'_sqznt.dat')\n",
    "    labels_val = load_array('/cat/home/ubuntu/cat/in/train/labels/'+files[i]+'_labels.dat')\n",
    "    if FIRST_TIME:\n",
    "        predictions_all = lrg_model.predict(conv_val_feat) \n",
    "        predictions_all = np.hstack((predictions_all[0],predictions_all[1][:,0:4],predictions_all[2][:,0:2]))\n",
    "        labels_all = labels_val\n",
    "        FIRST_TIME = False\n",
    "    else:\n",
    "        temp = lrg_model.predict(conv_val_feat)\n",
    "        temp = np.hstack((temp[0],temp[1][:,0:4],temp[2][:,0:2]))\n",
    "        predictions_all = np.vstack((predictions_all,temp))\n",
    "        labels_all = np.vstack((labels_all,labels_val))\n",
    "    del(conv_val_feat,labels_val)\n",
    "    gc.collect()\n",
    "print predictions_all.shape, labels_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_all = labels_all[:,2:23]\n",
    "labels_all[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]] = labels_all[:,[0,5,6,9,10,11,12,13,14,15,16,17,18,19,20,1,2,3,4,7,8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write code to do Test Time Augmentation by taking 10 non-random patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computing the area under the ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from math import isnan\n",
    "\n",
    "totalScore=0\n",
    "count = 0\n",
    "\n",
    "predictions_temp = np.copy(predictions_all)\n",
    "truth_temp = np.copy(labels_all)\n",
    "\n",
    "for j in range(21):   \n",
    "    index = 0\n",
    "    #remove rows with label = 0.5\n",
    "    for i in range(labels_all.shape[0]):\n",
    "        if labels_all[i,j] != 0.5:\n",
    "            truth_temp[index,j]=labels_all[i,j]\n",
    "            predictions_temp[index,j]=predictions_all[i,j]\n",
    "            index += 1\n",
    "    fpr, tpr, _ = roc_curve(truth_temp[0:index,j], predictions_temp[0:index,j])\n",
    "    score = auc(fpr, tpr)\n",
    "    print j, score\n",
    "    if isnan(score):\n",
    "        score=0.0\n",
    "    else:\n",
    "        totalScore+=score\n",
    "        count +=1\n",
    "    \n",
    "print totalScore/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  train21 ; 0.957 \n",
    "\n",
    "  train22 ; 0.972\n",
    "\n",
    "  train23 ; 0.993\n",
    "\n",
    "  train24 ; 0.924\n",
    "\n",
    "  train25 ; 0.865\n",
    "\n",
    "train21-5 ; 0.899\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del(conv_val_feat)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_hires = load_array('/cataract/home/ubuntu/cataract/03_output/train21_rgb_hires.dat')\n",
    "rgb_hires.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = load_array('/cataract/home/ubuntu/cataract/03_output/train21_labels.dat')\n",
    "labels[500,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_trn_feat = load_array('/cataract4/home/ubuntu/cataract4/03_output/train21_fcf_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels.shape,conv_trn_feat.shape,rgb_hires.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load weights\n",
    "l = lrg_model.layers\n",
    "conv_fn = K.function([l[0].input, K.learning_phase()], l[-4].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame=500\n",
    "inp = np.expand_dims(conv_trn_feat[frame], 0)\n",
    "conv = conv_fn([inp,0])[0,:,:,0]\n",
    "#conv = conv_fn([inp,0])\n",
    "conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cm2(inp, label):\n",
    "    conv = conv_fn([inp,0])[0,:,:,label]\n",
    "    return scipy.misc.imresize(conv, (360,640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotImages(frame):\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    colors = ['red', 'brown', 'yellow', 'green', 'blue']\n",
    "    cmap = LinearSegmentedColormap.from_list('name', colors)\n",
    "    norm = plt.Normalize(0, 254)\n",
    "    inp = np.expand_dims(conv_trn_feat[frame], 0)\n",
    "    index_array = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "    f = plt.figure(figsize=(10, 40))\n",
    "    r = len(index_array)\n",
    "\n",
    "    A = np.transpose(rgb_hires,(0,2,3,1))[frame,]\n",
    "\n",
    "    for k, i in enumerate(index_array):\n",
    "        for j , M in enumerate([A,A,A]):\n",
    "            sp = f.add_subplot(r, 3, 3*k + j + 1)\n",
    "            sp.axis('Off')\n",
    "            if j==0:\n",
    "                plt.imshow(M)\n",
    "            else:\n",
    "                if j==1:\n",
    "                    plt.imshow(get_cm2(inp, i), cmap='cool', norm=norm, interpolation='none') \n",
    "                else:\n",
    "                    plt.imshow(M)\n",
    "                    plt.imshow(get_cm2(inp, i), cmap='cool', norm=norm, interpolation='none', alpha=0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "256 convolutions filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotImages(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotImages(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotImages(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotImages(3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotImages(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels[600,2:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotImages(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "128 convolution filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotImages(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(rgb_hires,(0,2,3,1))[1000,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = get_cm2(inp, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(cm, cmap=\"hot\") # binary cross entropy with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.transpose(rgb_hires,(0,2,3,1))[1000,])\n",
    "plt.imshow(cm, cmap=\"hot\", alpha=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = get_cm2(inp, 7)\n",
    "plt.imshow(cm, cmap=\"gray\") # binary cross entropy with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = [0.01,0.001,0.0001,0.00001]\n",
    "e =[1,2,2,2]\n",
    "p = [0.6]\n",
    "tuneMyNw('fold_4','03_output/features',lr,e,p,get_lrg_layers,batch_size,allTags=True,base_model=None,early_stop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features, labels = loadFeatures('03_output/features','fold_4')\n",
    "predictFolds('fold_4',get_bn_layers,'vgg',features,allTags=True,base_model='vgg')\n",
    "np.savetxt('03_output/results/labels_vgg_fold_4.csv', labels, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tags = ['all']\n",
    "\n",
    "learningSchedules = {}\n",
    "learningSchedules['1'] = [0.01,0.001,0.0001]\n",
    "learningSchedules['2'] = [0.01,0.001]\n",
    "learningSchedules['3'] = [0.01]\n",
    "\n",
    "epochSchedules = {}\n",
    "epochSchedules['1'] = [1,2,4]\n",
    "epochSchedules['2'] = [1,2]\n",
    "epochSchedules['3'] = [1,2,3]\n",
    "epochSchedules['4'] = [1,2,5]\n",
    "epochSchedules['5'] = [1]\n",
    "epochSchedules['6'] = [1,2,1]\n",
    "\n",
    "dropouts = [0.8]\n",
    "learningSchedule = ['1']\n",
    "epochSchedule = ['1']\n",
    "\n",
    "features, labels = loadFeatures('03_output/features','train')\n",
    "trainFolds('test',tags,dropouts,learningSchedule,learningSchedules,epochSchedule,epochSchedules,\n",
    "           get_bn_layers,'vgg',features, labels,batch_size,allTags=True,base_model='vgg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features, labels = loadFeatures('03_output/testfeatures', 'test')\n",
    "predictFolds('test',get_bn_layers,'vgg',features,allTags=True,base_model='vgg')\n",
    "np.savetxt('03_output/results/labels_vgg_test.csv', labels, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io,transform\n",
    "%matplotlib inline\n",
    "\n",
    "def data_aug(img = img):\n",
    "\tmu = 0\n",
    "\tsigma = 0.1\n",
    "\tfeature_vec=np.matrix(evecs_mat)\n",
    "\n",
    "\t# 3 x 1 scaled eigenvalue matrix\n",
    "\tse = np.zeros((3,1))\n",
    "\tse[0][0] = np.random.normal(mu, sigma)*evals[0]\n",
    "\tse[1][0] = np.random.normal(mu, sigma)*evals[1]\n",
    "\tse[2][0] = np.random.normal(mu, sigma)*evals[2]\n",
    "\tse = np.matrix(se)\n",
    "\tval = feature_vec*se\n",
    "\n",
    "\t# Parse through every pixel value.\n",
    "\tfor i in xrange(img.shape[0]):\n",
    "\t\tfor j in xrange(img.shape[1]):\n",
    "\t\t\t# Parse through every dimension.\n",
    "\t\t\tfor k in xrange(img.shape[2]):\n",
    "\t\t\t\timg[i,j,k] = float(img[i,j,k]) + float(val[k])\n",
    "\n",
    "imnames = ['n00.jpg','n01.jpg','n02.jpg','n03.jpg','n04.jpg','n05.jpg']\n",
    "#load list of images\n",
    "imlist = (io.imread_collection(imnames))\n",
    "\n",
    "res = np.zeros(shape=(1,3))\n",
    "for i in range(len(imlist)):\n",
    "\t# re-size all images to 256 x 256 x 3\n",
    "\tm=transform.resize(imlist[i],(256,256,3))\n",
    "\t# re-shape to make list of RGB vectors.\n",
    "\tarr=m.reshape((256*256),3)\n",
    "\t# consolidate RGB vectors of all images\n",
    "\tres = np.concatenate((res,arr),axis=0)\n",
    "res = np.delete(res, (0), axis=0)\n",
    "\n",
    "# subtracting the mean from each dimension\n",
    "m = res.mean(axis = 0)\n",
    "res = res - m\n",
    "\n",
    "R = np.cov(res, rowvar=False)\n",
    "print R\n",
    "\n",
    "from numpy import linalg as LA\n",
    "evals, evecs = LA.eigh(R)\n",
    "\n",
    "idx = np.argsort(evals)[::-1]\n",
    "evecs = evecs[:,idx]\n",
    "# sort eigenvectors according to same index\n",
    "\n",
    "evals = evals[idx]\n",
    "# select the first 3 eigenvectors (3 is desired dimension\n",
    "# of rescaled data array)\n",
    "\n",
    "evecs = evecs[:, :3]\n",
    "# carry out the transformation on the data using eigenvectors\n",
    "# and return the re-scaled data, eigenvalues, and eigenvectors\n",
    "m = np.dot(evecs.T, res.T).T\n",
    "\n",
    "# perturbing color in image[0]\n",
    "# re-scaling from 0-1\n",
    "img = imlist[0]/255.0\n",
    "data_aug(img)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelFile = load_array('/cataract/home/ubuntu/cataract/03_output/train01_labels.dat')\n",
    "features = load_array('/cataract2/home/ubuntu/cataract2/03_output/train01_fcf_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.fit(conv_trn_feat, labels[:,2:23], batch_size=batch_size, nb_epoch=2, \n",
    "             validation_data=(conv_val_feat, labels_val[:,2:23])) #binary cross entropy with sigmoid n = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc_tool(truth_directory, prediction_directory, tool):\n",
    "    \"\"\"Computes the area under the ROC curve for one tool.\n",
    "    \"\"\"\n",
    "    filename = ''\n",
    "    try:\n",
    "    truth = []\n",
    "    predictions = []\n",
    "    # loop on (truth, predictions) file pairs\n",
    "    for file in range(1, num_files + 1):\n",
    "\n",
    "    # getting the filenames\n",
    "    if (file < 10):\n",
    "    filename = file_prefix + '0{}.csv'.format(file)\n",
    "    else:\n",
    "    filename = file_prefix + '{}.csv'.format(file)\n",
    "    truth_filename = join(truth_directory, filename)\n",
    "    prediction_filename = join(prediction_directory, filename)\n",
    "    # parsing the right column for the current tool\n",
    "    truth_data = read_csv(truth_filename, header = 0, skipinitialspace = True,\n",
    "    usecols = [tool], squeeze = True, dtype = 'float32').tolist()\n",
    "    prediction_data = read_csv(prediction_filename, header = None, skipinitialspace = True,\n",
    "    usecols = [tool], squeeze = True, dtype = 'float32').tolist()\n",
    "    if len(truth_data) != len(prediction_data):\n",
    "    raise ValueError('Files {} and {} have different row counts'.\n",
    "    format(truth_filename, prediction_filename))\n",
    "\n",
    "    # appending rows with consensual ground truth\n",
    "    indices = [index for index, value in enumerate(truth_data) if value != 0.5]\n",
    "    truth += [truth_data[index] for index in indices]\n",
    "    predictions += [prediction_data[index] for index in indices]\n",
    "\n",
    "    # computing the area under the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(truth, predictions)\n",
    "    score = auc(fpr, tpr)\n",
    "    return 0. if isnan(score) else score\n",
    "    except Exception as e:\n",
    "    print('Error: missing column in {} for tool number {}!'.format(filename, tool)\n",
    "    if 'Usecols' in str(e) else 'Error: {}!'.format(e))\n",
    "    return 0.\n",
    "\n",
    "\n",
    "    def main():\n",
    "    \"\"\"Main function.\n",
    "    \"\"\"\n",
    "\n",
    "    # parsing the command line\n",
    "    parser = ArgumentParser(description = 'Evaluator for the CATARACTS challenge.')\n",
    "    parser.add_argument('-t', '--truth', required = True, help = 'directory containing ground truth files')\n",
    "    parser.add_argument('-p', '--predictions', required = True, help = 'directory containing automatic predictions')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # computing tool-specific scores\n",
    "    scores = []\n",
    "    for tool in range(1, num_tools + 1):\n",
    "    score = auc_tool(args.truth, args.predictions, tool)\n",
    "    print('Score tool {0}: {1:.4f}'.format(tool, score))\n",
    "    scores.append(score)\n",
    "\n",
    "    # computing the average score\n",
    "    print('Average: {0:.4f}'.format(sum(scores) / float(len(scores))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path='../03_output/'\n",
    "name='train01'\n",
    "rgb_hires = load_array(path + name + '_rgb_hires.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_hires.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_feat = vgg640.predict(rgb_hires[1:100], batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_hires = load_array('03_output/train01_rgb_hires.dat')\n",
    "labels = load_array('03_output/train01_labels.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_hires_val = load_array('03_output/train25_rgb_hires.dat')\n",
    "labels_val = load_array('03_output/train25_labels.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_hires_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_hires = np.reshape(np.transpose(rgb_hires,(1,0)),(2460,360,640,3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_hires_val = np.reshape(np.transpose(rgb_hires_val,(1,0)),(6990,360,640,3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_hires.shape, labels.shape,rgb_hires_val.shape, labels_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rgb_hires = np.transpose(rgb_hires,(0,3,1,2)) \n",
    "rgb_hires_val = np.transpose(rgb_hires_val,(0,3,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.transpose(labels,(1,0))\n",
    "labels_val = np.transpose(labels_val,(1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_hires.shape\n",
    "plt.imshow(rgb_hires[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_trn_feat = vgg640.predict(rgb_hires_val, batch_size=32, verbose=1)\n",
    "conv_val_feat = vgg640.predict(rgb_hires, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array('03_output/train25_fcn_feat.dat',conv_trn_feat)\n",
    "save_array('03_output/train01_fcn_feat.dat',conv_val_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_layers,_ = split_at(vgg640, Convolution2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf=128; p=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf=256; p=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf=256; p=0.2\n",
    "def get_lrg_layers():\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(21,3,3, border_mode='same'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lrg_layers():\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(21,3,3, border_mode='same'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('sigmoid')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model = Sequential(get_lrg_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.fit(conv_trn_feat, labels_val[:,1:22], batch_size=batch_size, nb_epoch=2, \n",
    "             validation_data=(conv_val_feat, labels[:,1:22])) #binary cross entropy with sigmoid n = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.fit(conv_trn_feat, labels_val[:,1:22], batch_size=batch_size, nb_epoch=2, \n",
    "             validation_data=(conv_val_feat, labels[:,1:22])) #binary cross entropy with softmax n = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.fit(conv_trn_feat, labels_val[:,1:22], batch_size=batch_size, nb_epoch=2, \n",
    "             validation_data=(conv_val_feat, labels[:,1:22])) #binary cross entropy with sigmoid n = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.optimizer.lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.fit(conv_trn_feat, labels_val[:,1:22], batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, labels[:,1:22])) #categorical cross entropy with softmax n = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.fit(conv_trn_feat, labels_val[:,1:22], batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, labels[:,1:22])) #binary cross entropy with softmax n = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.fit(conv_trn_feat, labels_val[:,1:22], batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, labels[:,1:22])) #binary cross entropy with sigmoid  n = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrg_model.fit(conv_trn_feat, labels_val[:,1:22], batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, labels[:,1:22])) #binary cross entropy with softmax n = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = lrg_model.layers\n",
    "conv_fn = K.function([l[0].input, K.learning_phase()], l[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cm2(inp, label):\n",
    "    conv = conv_fn([inp,0])[0, label]\n",
    "    return scipy.misc.imresize(conv, (360,640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = np.expand_dims(conv_val_feat[400], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(to_plot(rgb_hires[400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = get_cm2(inp, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(cm, cmap=\"cool\") # binary cross entropy with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plot(rgb_hires[400])\n",
    "plt.imshow(cm, cmap=\"cool\", alpha=0.5) #plt.imshow(cm, cmap=\"cool\") # binary cross entropy with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(cm, cmap=\"cool\") # binary cross entropy with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plot(rgb_hires[400])\n",
    "plt.imshow(cm, cmap=\"cool\", alpha=0.5) #plt.imshow(cm, cmap=\"cool\") # binary cross entropy with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = get_cm2(inp, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(cm, cmap=\"gray\") # binary cross entropy with softmax n = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plot(rgb_hires[400])\n",
    "plt.imshow(cm, cmap=\"gray\", alpha=0.5) #plt.imshow(cm, cmap=\"cool\") # binary cross entropy with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = ['train02','train03','train04','train05','train06','train07','train08','train09','train10','train11',\n",
    "         'train12','train13','train14','train15','train16','train17','train18','train19','train20','train21','train22',\n",
    "         'train23','train24','train25'] \n",
    "\n",
    "labels = load_array('../03_output/train01_labels.dat')\n",
    "for i in tqdm(range(len(files))):\n",
    "    lbl = load_array('../03_output/'+files[i]+'_labels.dat')\n",
    "    labels = np.vstack((labels,lbl))\n",
    "    del(lbl)\n",
    "    gc.collect()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anyLabel = sum(np.transpose(labels)[3:23,:])\n",
    "unique, counts = np.unique(anyLabel, return_counts=True)\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels[labels==0.5]=999\n",
    "anyLabel = sum(np.transpose(labels)[3:23,:])\n",
    "unique, counts = np.unique(anyLabel, return_counts=True)\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'0',36060/82479.0,'1',13838/82479.0,'2',30591/82479.0,'0.5 or 1.5 or 2.5 or 3', (1595+334+27+34)/82479.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anyLabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "\n",
    "lagggedLabels = shift(anyLabel, -15, cval=1)\n",
    "lagggedLabels[lagggedLabels<0.5]=0\n",
    "for i in range(30):\n",
    "    temp = shift(anyLabel, -14+i, cval=1)\n",
    "    temp[temp<0.5]=0\n",
    "    lagggedLabels = np.vstack((lagggedLabels,temp))\n",
    "\n",
    "lagggedLabels.shape\n",
    "anyLabelLag15 = sum(lagggedLabels)\n",
    "anyLabelLag15.shape\n",
    "anyLabelLag15[anyLabelLag15>0]=1\n",
    "unique, counts = np.unique(anyLabelLag15, return_counts=True)\n",
    "unique, counts\n",
    "keepOrNot = np.vstack((anyLabel,anyLabelLag15)).T\n",
    "keepOrNot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(keepOrNot[:,0], return_counts=True)\n",
    "unique, counts"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
